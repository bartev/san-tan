{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description\n",
    "\n",
    "learn pyspark basics from a blog\n",
    "\n",
    "Follow this blogpost https://towardsdatascience.com/the-hitchhikers-guide-to-handle-big-data-using-spark-90b9be0fe89a\n",
    "\n",
    "\n",
    "* RDD Programming Guide http://spark.apache.org/docs/latest/rdd-programming-guide.html#actions\n",
    "\n",
    "Follow up\n",
    "\n",
    "See coursera courses\n",
    "* Big Data Essentials: HDFS, MapReduce and Spark RDD\n",
    "    * https://www.coursera.org/learn/big-data-analysis?ranMID=40328&ranEAID=lVarvwc5BD0&ranSiteID=lVarvwc5BD0-Y2ZYRU0eP2qngfq6ffGH2Q&siteID=lVarvwc5BD0-Y2ZYRU0eP2qngfq6ffGH2Q&utm_content=2&utm_medium=partners&utm_source=linkshare&utm_campaign=lVarvwc5BD0\n",
    "* Big Data Analysis: Hive, Spark SQL, DataFrames and GraphFrames\n",
    "    * https://www.coursera.org/learn/big-data-analysis?ranMID=40328&ranEAID=lVarvwc5BD0&ranSiteID=lVarvwc5BD0-Y2ZYRU0eP2qngfq6ffGH2Q&siteID=lVarvwc5BD0-Y2ZYRU0eP2qngfq6ffGH2Q&utm_content=2&utm_medium=partners&utm_source=linkshare&utm_campaign=lVarvwc5BD0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Setup spark context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sc = pyspark.SparkContext('local[*]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.17:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=pyspark-shell>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PYTHONHASHSEED': '0'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'local-1590729753223'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.applicationId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pyspark-shell'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.appName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.5'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.7'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.pythonVer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sc.sparkHome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1590729747186"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.startTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1590729747186"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.startTime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Read in Macbeth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[36mData-ML-100k--master\u001b[m\u001b[m/ Untitled.ipynb        requirements.txt\r\n",
      "README.md             Untitled.py\r\n",
      "\u001b[1m\u001b[36mShakespearePlaysPlus\u001b[m\u001b[m/ Untitled.txt\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "lines = sc.textFile('ShakespearePlaysPlus/tragedies/Macbeth.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9879\n"
     ]
    }
   ],
   "source": [
    "print(lines.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Create a list with all words\n",
    "# create tuple (word, 1)\n",
    "# reduce by key (i.e. the word)\n",
    "\n",
    "counts = (lines.flatMap(lambda x: x.replace('\\x00', '').split(' '))\n",
    "           .map(lambda x: (x, 1))\n",
    "           .reduceByKey(lambda x, y : x + y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[22] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('��<', 1),\n",
       " ('Shakespeare', 1),\n",
       " ('>', 6),\n",
       " ('', 5719),\n",
       " ('of', 319),\n",
       " ('Liberty', 1),\n",
       " ('(http://oll.libertyfund.org)', 1),\n",
       " ('Unicode', 1),\n",
       " ('version', 1),\n",
       " ('Scott', 1)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the output on local\n",
    "output = counts.take(10)\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "��<: 1\n",
      "Shakespeare: 1\n",
      ">: 6\n",
      ": 5719\n",
      "of: 319\n",
      "Liberty: 1\n",
      "(http://oll.libertyfund.org): 1\n",
      "Unicode: 1\n",
      "version: 1\n",
      "Scott: 1\n"
     ]
    }
   ],
   "source": [
    "# print output\n",
    "for word, count in output:\n",
    "  print(f'{word}: {count:d}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 9, 16, 25, 36, 49, 64, 81, 100]\n"
     ]
    }
   ],
   "source": [
    "my_list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "\n",
    "# Square every term in my_list\n",
    "squared_list = map(lambda x: x**2, my_list)\n",
    "print(list(squared_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4, 6, 8, 10]\n"
     ]
    }
   ],
   "source": [
    "# keep only even numbers\n",
    "\n",
    "filtered_list = filter(lambda x: x%2 == 0, my_list)\n",
    "print(list(filtered_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import functools\n",
    "my_list = [1,2,3,4,5]\n",
    "\n",
    "# sum all elements in my_list\n",
    "sum_list = functools.reduce(lambda x,y: x + y, my_list)\n",
    "sum_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## parallelize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[25] at parallelize at PythonRDD.scala:195"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [1,2,3,4,5,6,7,8,9,10]\n",
    "\n",
    "new_rdd = sc.parallelize(data, 4)\n",
    "new_rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Two operations - Transformation and Action\n",
    "\n",
    "* Transformation - create new dataset from existing RDD\n",
    "*  Action - mechanism to get results out of sparkj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.PipelinedRDD"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Transformations\n",
    "\n",
    "http://spark.apache.org/docs/latest/rdd-programming-guide.html#transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 4, 9, 16, 25, 36, 49, 64, 81, 100]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [1,2,3,4,5,6,7,8,9,10]\n",
    "rdd = sc.parallelize(data, 4)\n",
    "squared_rdd = rdd.map(lambda x: x ** 2)\n",
    "\n",
    "squared_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[33] at collect at <ipython-input-58-426c84408c9f>:5"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squared_rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Filter\n",
    "\n",
    "Return those elements that fulfill the condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4, 6, 8, 10]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [1,2,3,4,5,6,7,8,9,10]\n",
    "rdd = sc.parallelize(data, 4)\n",
    "filtered_rdd = rdd.filter(lambda x: x % 2 == 0)\n",
    "filtered_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Distinct\n",
    "\n",
    "return only distinct elemens of an RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 8, 1, 5, 9, 2, 6, 10, 3, 7]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [1,2,2,2,2,3,3,3,3,4,5,6,7,7,7,8,8,8,9,10]\n",
    "rdd = sc.parallelize(data, 4)\n",
    "distinct_rdd = rdd.distinct()\n",
    "distinct_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## flatmap\n",
    "\n",
    "Similar to map, but each input item can be mapped to 0 or more output items.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 2, 8, 3, 27, 4, 64]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [1,2,3,4]\n",
    "rdd = sc.parallelize(data, 4)\n",
    "flat_rdd = rdd.flatMap(lambda x: [x, x**3])\n",
    "flat_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Reduce By Key\n",
    "\n",
    "The parallel to the reduce in Hadoop MapReduce.\n",
    "\n",
    "Now Spark cannot provide the value if it just worked with Lists.\n",
    "\n",
    "In Spark, there is a concept of pair RDDs that makes it a lot more flexible. Let's assume we have a data in which we have a product, its category, and its selling price. We can still parallelize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data = [('Apple','Fruit',200),\n",
    "        ('Banana','Fruit',24),\n",
    "        ('Tomato','Fruit',56),\n",
    "        ('Potato','Vegetable',103),\n",
    "        ('Carrot','Vegetable',34)]\n",
    "\n",
    "rdd = sc.parallelize(data, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Right now our RDD rdd holds tuples.\n",
    "\n",
    "Now we want to find out the total sum of revenue that we got from each category.\n",
    "\n",
    "To do that we have to transform our rdd to a pair rdd so that it only contains key-value pairs/tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Fruit', 200),\n",
       " ('Fruit', 24),\n",
       " ('Fruit', 56),\n",
       " ('Vegetable', 103),\n",
       " ('Vegetable', 34)]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_price_rdd = rdd.map(lambda x: (x[1], x[2]))\n",
    "category_price_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Here we used the map function to get it in the format we wanted. When working with textfile, the RDD that gets formed has got a lot of strings. We use map to convert it into a format that we want.\n",
    "\n",
    "So now our category_price_rdd contains the product category and the price at which the product sold.\n",
    "\n",
    "Now we want to reduce on the key category and sum the prices. We can do this by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Fruit', 280), ('Vegetable', 137)]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_total_price_rdd = category_price_rdd.reduceByKey(lambda x, y: x + y)\n",
    "category_total_price_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Group By Key\n",
    "\n",
    "Similar to reduceByKey but does not reduces just puts all the elements in an iterator. For example, if we wanted to keep as key the category and as the value all the products we would use this function.\n",
    "\n",
    "Let us again use map to get data in the required form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Fruit', 'Apple'),\n",
       " ('Fruit', 'Banana'),\n",
       " ('Fruit', 'Tomato'),\n",
       " ('Vegetable', 'Potato'),\n",
       " ('Vegetable', 'Carrot')]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ata = [('Apple','Fruit',200),\n",
    "       ('Banana','Fruit',24),\n",
    "       ('Tomato','Fruit',56),\n",
    "       ('Potato','Vegetable',103),\n",
    "       ('Carrot','Vegetable',34)]\n",
    "rdd = sc.parallelize(data, 4)\n",
    "category_product_rdd = rdd.map(lambda x: (x[1], x[0]))\n",
    "category_product_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We then use groupByKey as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fruit ['Apple', 'Banana', 'Tomato']\n",
      "Vegetable ['Potato', 'Carrot']\n"
     ]
    }
   ],
   "source": [
    "grouped_products_by_category_rdd = category_product_rdd.groupByKey()\n",
    "findata = grouped_products_by_category_rdd.collect()\n",
    "for data in findata:\n",
    "    print(data[0], list(data[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Fruit', <pyspark.resultiterable.ResultIterable at 0x1210206a0>),\n",
       " ('Vegetable', <pyspark.resultiterable.ResultIterable at 0x121020710>)]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "findata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Here the groupByKey function worked and it returned the category and the list of products in that category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Action Basics\n",
    "\n",
    "http://spark.apache.org/docs/latest/rdd-programming-guide.html#actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "You have filtered your data, mapped some functions on it. Done your computation.\n",
    "\n",
    "Now you want to get the data on your local machine or save it to a file or show the results in the form of some graphs in excel or any visualization tool.\n",
    "\n",
    "You will need actions for that. A comprehensive list of actions is provided here http://spark.apache.org/docs/latest/rdd-programming-guide.html#actions.\n",
    "\n",
    "Some of the most common actions that I tend to use are:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## collect\n",
    "We have already used this action many times. It takes the whole RDD and brings it back to the driver program."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## reduce\n",
    "\n",
    "Aggregate the elements of the dataset using a function func (which takes two arguments and returns one). The function should be commutative and associative so that it can be computed correctly in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([1,2,3,4,5])\n",
    "rdd.reduce(lambda x,y : x+y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## take\n",
    "Sometimes you will need to see what your RDD contains without getting all the elements in memory itself. take returns a list with the first n elements of the RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([1,2,3,4,5])\n",
    "rdd.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## takeOrdered\n",
    "takeOrdered returns the first n elements of the RDD using either their natural order or a custom comparator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[23, 12, 5]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([5,3,12,23])\n",
    "# descending order\n",
    "rdd.takeOrdered(3, lambda s: -1*s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(12, 344), (3, 34), (23, 29)]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([(5,23),(3,34),(12,344),(23,29)])\n",
    "# descending order\n",
    "rdd.takeOrdered(3, lambda s: -1*s[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Understanding The WordCount Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now we sort of understand the transformations and the actions provided to us by Spark.\n",
    "\n",
    "It should not be difficult to understand the wordcount program now. Let us go through the program line by line.\n",
    "\n",
    "The first line creates an RDD and distributes it to the workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "lines = sc.textFile('ShakespearePlaysPlus/tragedies/Macbeth.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This RDD lines contains a list of sentences in the file. You can see the rdd content using take"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['��<\\x00 \\x00S\\x00h\\x00a\\x00k\\x00e\\x00s\\x00p\\x00e\\x00a\\x00r\\x00e\\x00 \\x00-\\x00-\\x00 \\x00M\\x00A\\x00C\\x00B\\x00E\\x00T\\x00H\\x00 \\x00>\\x00',\n",
       " '\\x00',\n",
       " '\\x00<\\x00 \\x00f\\x00r\\x00o\\x00m\\x00 \\x00O\\x00n\\x00l\\x00i\\x00n\\x00e\\x00 \\x00L\\x00i\\x00b\\x00r\\x00a\\x00r\\x00y\\x00 \\x00o\\x00f\\x00 \\x00L\\x00i\\x00b\\x00e\\x00r\\x00t\\x00y\\x00 \\x00(\\x00h\\x00t\\x00t\\x00p\\x00:\\x00/\\x00/\\x00o\\x00l\\x00l\\x00.\\x00l\\x00i\\x00b\\x00e\\x00r\\x00t\\x00y\\x00f\\x00u\\x00n\\x00d\\x00.\\x00o\\x00r\\x00g\\x00)\\x00 \\x00>\\x00',\n",
       " '\\x00',\n",
       " '\\x00<\\x00 \\x00U\\x00n\\x00i\\x00c\\x00o\\x00d\\x00e\\x00 \\x00.\\x00t\\x00x\\x00t\\x00 \\x00v\\x00e\\x00r\\x00s\\x00i\\x00o\\x00n\\x00 \\x00b\\x00y\\x00 \\x00M\\x00i\\x00k\\x00e\\x00 \\x00S\\x00c\\x00o\\x00t\\x00t\\x00 \\x00(\\x00h\\x00t\\x00t\\x00p\\x00:\\x00/\\x00/\\x00w\\x00w\\x00w\\x00.\\x00l\\x00e\\x00x\\x00i\\x00c\\x00a\\x00l\\x00l\\x00y\\x00.\\x00n\\x00e\\x00t\\x00)\\x00 \\x00>\\x00']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This next line is actually the workhorse function in the whole script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "counts = (lines.flatMap(lambda x: x.replace('\\x00', '').split(' '))\n",
    "                  .map(lambda x: (x, 1))\n",
    "                  .reduceByKey(lambda x, y : x + y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "It contains a series of transformations that we do to the lines RDD. First of all, we do a flatmap transformation.\n",
    "\n",
    "The flatmap transformation takes as input the lines and gives words as output. So after the flatmap transformation, the RDD is of the form:\n",
    "\n",
    "`['word1','word2','word3','word4','word3','word2']`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Next, we do a map transformation on the flatmap output which converts the RDD to :\n",
    "\n",
    "`[('word1',1),('word2',1),('word3',1),('word4',1),('word3',1),('word2',1)]`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Finally, we do a reduceByKey transformation which counts the number of time each word appeared.\n",
    "\n",
    "After which the RDD approaches the final desirable form.\n",
    "\n",
    "`[('word1',1),('word2',2),('word3',2),('word4',1)]`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This next line is an action that takes the first 10 elements of the resulting RDD locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "output = counts.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This line just prints the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "��<: 1\n",
      "Shakespeare: 1\n",
      ">: 6\n",
      ": 5719\n",
      "of: 319\n",
      "Liberty: 1\n",
      "(http://oll.libertyfund.org): 1\n",
      "Unicode: 1\n",
      "version: 1\n",
      "Scott: 1\n"
     ]
    }
   ],
   "source": [
    "# print output\n",
    "for word, count in output:\n",
    "  print(f'{word}: {count:d}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "And that is it for the wordcount program. Hope you understand it now.\n",
    "\n",
    "So till now, we talked about the Wordcount example and the basic transformations and actions that you could use in Spark. But we don’t do wordcount in real life.\n",
    "\n",
    "We have to work on bigger problems which are much more complex. Worry not! Whatever we have learned till now will let us do that and more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Spark in Action with Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let us work with a concrete example which takes care of some usual transformations.\n",
    "\n",
    "We will work on Movielens ml-100k.zip dataset which is a stable benchmark dataset. 100,000 ratings from 1000 users on 1700 movies. Released 4/1998.\n",
    "\n",
    "The Movielens dataset contains a lot of files but we are going to be working with 3 files only:\n",
    "\n",
    "1) Users: This file name is kept as “u.user”, The columns in this file are:\n",
    "\n",
    "    ['user_id', 'age', 'sex', 'occupation', 'zip_code']\n",
    "\n",
    "2) Ratings: This file name is kept as “u.data”, The columns in this file are:\n",
    "\n",
    "    ['user_id', 'movie_id', 'rating', 'unix_timestamp']\n",
    "\n",
    "3) Movies: This file name is kept as “u.item”, The columns in this file are:\n",
    "\n",
    "    ['movie_id', 'title', 'release_date', 'video_release_date', 'imdb_url', and 18 more columns.....]\n",
    "\n",
    "Let us start by importing these 3 files into our spark instance using ‘Import and Explore Data’ on the home tab.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "import the files u.user, u.data, u.item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Find 25 most rated movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Our business partner now comes to us and asks us to find out the 25 most rated movie titles from this data. How many times a movie has been rated?\n",
    "\n",
    "Let us load the data in different RDDs and see what the data contains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[36mData-ML-100k--master\u001b[m\u001b[m/   Untitled.py             pyspark-tutorial.py\r\n",
      "README.md               Untitled.txt            requirements.txt\r\n",
      "\u001b[1m\u001b[36mShakespearePlaysPlus\u001b[m\u001b[m/   pyspark-tutorial.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "userRDD: ['1|24|M|technician|85711']\n",
      "ratingRDD: ['196\\t242\\t3\\t881250949']\n",
      "movieRDD: ['1|Toy Story (1995)|01-Jan-1995||http://us.imdb.com/M/title-exact?Toy%20Story%20(1995)|0|0|0|1|1|1|0|0|0|0|0|0|0|0|0|0|0|0|0']\n"
     ]
    }
   ],
   "source": [
    "userRDD = sc.textFile('Data-ML-100k--master/ml-100k/u.user') \n",
    "ratingRDD = sc.textFile('Data-ML-100k--master/ml-100k/u.data') \n",
    "movieRDD = sc.textFile('Data-ML-100k--master/ml-100k/u.item') \n",
    "\n",
    "print(\"userRDD:\", userRDD.take(1))\n",
    "print(\"ratingRDD:\", ratingRDD.take(1))\n",
    "print(\"movieRDD:\", movieRDD.take(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We note that to answer this question we will need to use the ratingRDD. But the ratingRDD does not have the movie name.\n",
    "\n",
    "So we would have to merge movieRDD and ratingRDD using movie_id.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDD_movie_rating: [('242', '3'), ('302', '3'), ('377', '1'), ('51', '2')]\n"
     ]
    }
   ],
   "source": [
    "# Create a RDD from RatingRDD that only contains the two columns of interest i.e. movie_id,rating.\n",
    "\n",
    "RDD_movie_rating = ratingRDD.map(lambda x: (x.split('\\t')[1], x.split('\\t')[2]))\n",
    "print(f'RDD_movie_rating: {RDD_movie_rating.take(4)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDD_movie_title: [('1', 'Toy Story (1995)'), ('2', 'GoldenEye (1995)')]\n"
     ]
    }
   ],
   "source": [
    "# Create a RDD from MovieRDD that only contains the two columns of interest i.e. movie_id,title.\n",
    "RDD_movie_title = movieRDD.map(lambda x: (x.split('|')[0], x.split('|')[1]))\n",
    "print(f'RDD_movie_title: {RDD_movie_title.take(2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rdd_movie_title_rating: [('346', ('1', 'Jackie Brown (1997)'))]\n"
     ]
    }
   ],
   "source": [
    "# merge these two pair RDDs based on movie_id. For this we will use the transformation leftOuterJoin(). See the transformation document.\n",
    "rdd_movie_title_rating = RDD_movie_rating.leftOuterJoin(RDD_movie_title)\n",
    "print(f'rdd_movie_title_rating: {rdd_movie_title_rating.take(1)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rdd_title_rating: [('Jackie Brown (1997)', 1)]\n"
     ]
    }
   ],
   "source": [
    "# use the RDD in previous step to create (movie,1) tuple pair RDD\n",
    "rdd_title_rating = rdd_movie_title_rating.map(lambda x: (x[1][1], 1))\n",
    "print(f'rdd_title_rating: {rdd_title_rating.take(1)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rdd_title_rating_cnt: [('Jackie Brown (1997)', 126), ('Jungle Book, The (1994)', 85)]\n"
     ]
    }
   ],
   "source": [
    "# Use the reduceByKey transformation to reduce on the basis of movie_title\n",
    "rdd_title_rating_cnt = rdd_title_rating.reduceByKey(lambda x, y: x + y)\n",
    "print(f'rdd_title_rating_cnt: {rdd_title_rating_cnt.take(2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################\n",
      "25 most rated movies\n",
      "583 Star Wars (1977)\n",
      "509 Contact (1997)\n",
      "508 Fargo (1996)\n",
      "507 Return of the Jedi (1983)\n",
      "485 Liar Liar (1997)\n",
      "481 English Patient, The (1996)\n",
      "478 Scream (1996)\n",
      "452 Toy Story (1995)\n",
      "431 Air Force One (1997)\n",
      "429 Independence Day (ID4) (1996)\n",
      "420 Raiders of the Lost Ark (1981)\n",
      "413 Godfather, The (1972)\n",
      "394 Pulp Fiction (1994)\n",
      "392 Twelve Monkeys (1995)\n",
      "390 Silence of the Lambs, The (1991)\n",
      "384 Jerry Maguire (1996)\n",
      "379 Chasing Amy (1997)\n",
      "378 Rock, The (1996)\n",
      "367 Empire Strikes Back, The (1980)\n",
      "365 Star Trek: First Contact (1996)\n",
      "350 Back to the Future (1985)\n",
      "350 Titanic (1997)\n",
      "344 Mission: Impossible (1996)\n",
      "336 Fugitive, The (1993)\n",
      "331 Indiana Jones and the Last Crusade (1989)\n",
      "#########################\n"
     ]
    }
   ],
   "source": [
    "# Get the final answer by using takeOrdered Transformation\n",
    "print('#' * 25)\n",
    "print('25 most rated movies')\n",
    "[print(x[1], x[0]) for x in rdd_title_rating_cnt.takeOrdered(25, lambda x: -x[1])]\n",
    "print('#' * 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Star Wars is the most rated movie in the Movielens Dataset.\n",
    "\n",
    "Now we could have done all this in a single command using the below command but the code is a little messy now.\n",
    "\n",
    "I did this to show that you can use chaining functions with Spark and you could bypass the process of variable creation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Rewrite as chained function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Star Wars (1977)', 583), ('Contact (1997)', 509), ('Fargo (1996)', 508), ('Return of the Jedi (1983)', 507), ('Liar Liar (1997)', 485), ('English Patient, The (1996)', 481), ('Scream (1996)', 478), ('Toy Story (1995)', 452), ('Air Force One (1997)', 431), ('Independence Day (ID4) (1996)', 429), ('Raiders of the Lost Ark (1981)', 420), ('Godfather, The (1972)', 413), ('Pulp Fiction (1994)', 394), ('Twelve Monkeys (1995)', 392), ('Silence of the Lambs, The (1991)', 390), ('Jerry Maguire (1996)', 384), ('Chasing Amy (1997)', 379), ('Rock, The (1996)', 378), ('Empire Strikes Back, The (1980)', 367), ('Star Trek: First Contact (1996)', 365), ('Back to the Future (1985)', 350), ('Titanic (1997)', 350), ('Mission: Impossible (1996)', 344), ('Fugitive, The (1993)', 336), ('Indiana Jones and the Last Crusade (1989)', 331)]\n"
     ]
    }
   ],
   "source": [
    "print((ratingRDD.map(lambda x : (x.split(\"\\t\")[1],x.split(\"\\t\")[2])))\n",
    "       .leftOuterJoin(movieRDD.map(lambda x : (x.split(\"|\")[0],x.split(\"|\")[1])))\n",
    "       .map(lambda x: (x[1][1],1))\n",
    "       .reduceByKey(lambda x,y: x+y)\n",
    "      .takeOrdered(25,lambda x:-x[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Find 25 most highly rated movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let us do one more. For practice:\n",
    "\n",
    "Now we want to find the most highly rated 25 movies using the same dataset. We actually want only those movies which have been rated at least 100 times.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('346', ('1', 'Jackie Brown (1997)'))"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_movie_title_rating.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rdd_title_ratings_sum: [('Jackie Brown (1997)', 459), ('Jungle Book, The (1994)', 303)]\n"
     ]
    }
   ],
   "source": [
    "# We create an RDD that contains sum of all the ratings for a particular movie\n",
    "rdd_title_ratings_sum = (\n",
    "    rdd_movie_title_rating\n",
    "    .map(lambda x: (x[1][1], int(x[1][0])))\n",
    "    .reduceByKey(lambda x, y: x + y))\n",
    "print(f'rdd_title_ratings_sum: {rdd_title_ratings_sum.take(2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rdd_title_ratings_mean_rating_count: [('Jackie Brown (1997)', 3.642857142857143, 126)]\n"
     ]
    }
   ],
   "source": [
    "# Merge this data with the RDD rdd_title_ratingcnt we created in the last step\n",
    "# And use Map function to divide ratingsum by rating count.\n",
    "\n",
    "rdd_title_ratings_mean_rating_count = (\n",
    "    rdd_title_ratings_sum\n",
    "    .leftOuterJoin(rdd_title_rating_cnt)\n",
    "    .map(lambda x: (x[0], x[1][0] / x[1][1], x[1][1]))\n",
    ")\n",
    "\n",
    "print(f'rdd_title_ratings_mean_rating_count: {rdd_title_ratings_mean_rating_count.take(1)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rdd_title_rating_rating_ct_gt_100: [('Jackie Brown (1997)', 3.642857142857143, 126)]\n"
     ]
    }
   ],
   "source": [
    "# We could use take ordered here only but we want to only get the movies which have count\n",
    "# of ratings more than or equal to 100 so lets filter the data RDD.\n",
    "rdd_title_rating_rating_ct_gt_100 = (rdd_title_ratings_mean_rating_count\n",
    "    .filter(lambda x: x[2] >= 100))\n",
    "\n",
    "print(f'rdd_title_rating_rating_ct_gt_100: {rdd_title_rating_rating_ct_gt_100.take(1)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################\n",
      "25 highly rated movies\n",
      "4.491071428571429 Close Shave, A (1995)\n",
      "4.466442953020135 Schindler's List (1993)\n",
      "4.466101694915254 Wrong Trousers, The (1993)\n",
      "4.45679012345679 Casablanca (1942)\n",
      "4.445229681978798 Shawshank Redemption, The (1994)\n",
      "4.3875598086124405 Rear Window (1954)\n",
      "4.385767790262173 Usual Suspects, The (1995)\n",
      "4.3584905660377355 Star Wars (1977)\n",
      "4.344 12 Angry Men (1957)\n",
      "4.292929292929293 Citizen Kane (1941)\n",
      "4.292237442922374 To Kill a Mockingbird (1962)\n",
      "4.291666666666667 One Flew Over the Cuckoo's Nest (1975)\n",
      "4.28974358974359 Silence of the Lambs, The (1991)\n",
      "4.284916201117318 North by Northwest (1959)\n",
      "4.283292978208232 Godfather, The (1972)\n",
      "4.265432098765432 Secrets & Lies (1996)\n",
      "4.262626262626263 Good Will Hunting (1997)\n",
      "4.259541984732825 Manchurian Candidate, The (1962)\n",
      "4.252577319587629 Dr. Strangelove or: How I Learned to Stop Worrying and Love the Bomb (1963)\n",
      "4.252380952380952 Raiders of the Lost Ark (1981)\n",
      "4.251396648044692 Vertigo (1958)\n",
      "4.2457142857142856 Titanic (1997)\n",
      "4.23121387283237 Lawrence of Arabia (1962)\n",
      "4.2101449275362315 Maltese Falcon, The (1941)\n",
      "4.204359673024523 Empire Strikes Back, The (1980)\n",
      "#########################\n"
     ]
    }
   ],
   "source": [
    "# Get the final answer by using takeOrdered Transformation\n",
    "print('#' * 25)\n",
    "print('25 highly rated movies')\n",
    "[print(x[1], x[0]) for x in rdd_title_rating_rating_ct_gt_100.takeOrdered(25, lambda x: -x[1])]\n",
    "print('#' * 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We have talked about RDDs till now as they are very powerful.\n",
    "\n",
    "You can use RDDs to work with non-relational databases too.\n",
    "\n",
    "They let you do a lot of things that you couldn’t do with SparkSQL?\n",
    "\n",
    "Yes, you can use SQL with Spark too which I am going to talk about now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark DataFrames\n",
    "\n",
    "https://docs.databricks.com/spark/latest/dataframes-datasets/introduction-to-dataframes-python.html#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark has provided DataFrame API for us Data Scientists to work with relational data. Here is the documentation for the adventurous folks.\n",
    "\n",
    "Remember that in the background it still is all RDDs and that is why the starting part of this post focussed on RDDs.\n",
    "\n",
    "I will start with some common functionalities you will need to work with Spark DataFrames. Would look a lot like Pandas with some syntax changes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup `SparkSession`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (SparkSession\n",
    "        .builder\n",
    "        .appName('Python Spark SQL basic example')\n",
    "        .config('spark.some.config.option', 'some-value')\n",
    "        .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.17:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x120f6c7f0>"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = spark.read.load('Data-ML-100k--master/ml-100k/u.data', \n",
    "                          format='csv',\n",
    "                          sep='\\t',\n",
    "                          inferSchema='true',\n",
    "                          header='false')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---+---------+\n",
      "|_c0| _c1|_c2|      _c3|\n",
      "+---+----+---+---------+\n",
      "|196| 242|  3|881250949|\n",
      "|186| 302|  3|891717742|\n",
      "| 22| 377|  1|878887116|\n",
      "|244|  51|  2|880606923|\n",
      "|166| 346|  1|886397596|\n",
      "|298| 474|  4|884182806|\n",
      "|115| 265|  2|881171488|\n",
      "|253| 465|  5|891628467|\n",
      "|305| 451|  3|886324817|\n",
      "|  6|  86|  3|883603013|\n",
      "| 62| 257|  2|879372434|\n",
      "|286|1014|  5|879781125|\n",
      "|200| 222|  5|876042340|\n",
      "|210|  40|  3|891035994|\n",
      "|224|  29|  3|888104457|\n",
      "|303| 785|  3|879485318|\n",
      "|122| 387|  5|879270459|\n",
      "|194| 274|  2|879539794|\n",
      "|291|1042|  4|874834944|\n",
      "|234|1184|  2|892079237|\n",
      "+---+----+---+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ratings.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_c0: int, _c1: int, _c2: int, _c3: int]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(ratings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = ratings.toDF??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[user_id: int, movie_id: int, rating: int, unix_timestamp: int]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ratings = ratings.toDF(*['user_id', 'movie_id', 'rating', 'unix_timestamp'])\n",
    "display(ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+------+--------------+\n",
      "|user_id|movie_id|rating|unix_timestamp|\n",
      "+-------+--------+------+--------------+\n",
      "|    196|     242|     3|     881250949|\n",
      "|    186|     302|     3|     891717742|\n",
      "|     22|     377|     1|     878887116|\n",
      "|    244|      51|     2|     880606923|\n",
      "|    166|     346|     1|     886397596|\n",
      "|    298|     474|     4|     884182806|\n",
      "|    115|     265|     2|     881171488|\n",
      "|    253|     465|     5|     891628467|\n",
      "|    305|     451|     3|     886324817|\n",
      "|      6|      86|     3|     883603013|\n",
      "|     62|     257|     2|     879372434|\n",
      "|    286|    1014|     5|     879781125|\n",
      "|    200|     222|     5|     876042340|\n",
      "|    210|      40|     3|     891035994|\n",
      "|    224|      29|     3|     888104457|\n",
      "|    303|     785|     3|     879485318|\n",
      "|    122|     387|     5|     879270459|\n",
      "|    194|     274|     2|     879539794|\n",
      "|    291|    1042|     4|     874834944|\n",
      "|    234|    1184|     2|     892079237|\n",
      "+-------+--------+------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ratings.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some basic stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row count:    100000\n",
      "column count: 4\n"
     ]
    }
   ],
   "source": [
    "print(f'row count:    {ratings.count()}') \n",
    "print(f'column count: {len(ratings.columns)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+-----------------+\n",
      "|summary|           user_id|          movie_id|            rating|   unix_timestamp|\n",
      "+-------+------------------+------------------+------------------+-----------------+\n",
      "|  count|            100000|            100000|            100000|           100000|\n",
      "|   mean|         462.48475|         425.53013|           3.52986|8.8352885148862E8|\n",
      "| stddev|266.61442012750905|330.79835632558473|1.1256735991443214|5343856.189502848|\n",
      "|    min|                 1|                 1|                 1|        874724710|\n",
      "|    max|               943|              1682|                 5|        893286638|\n",
      "+-------+------------------+------------------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ratings.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary</th>\n",
       "      <th>user_id</th>\n",
       "      <th>movie_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>unix_timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>count</td>\n",
       "      <td>100000</td>\n",
       "      <td>100000</td>\n",
       "      <td>100000</td>\n",
       "      <td>100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mean</td>\n",
       "      <td>462.48475</td>\n",
       "      <td>425.53013</td>\n",
       "      <td>3.52986</td>\n",
       "      <td>8.8352885148862E8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stddev</td>\n",
       "      <td>266.61442012750905</td>\n",
       "      <td>330.79835632558473</td>\n",
       "      <td>1.1256735991443214</td>\n",
       "      <td>5343856.189502848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>min</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>874724710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>max</td>\n",
       "      <td>943</td>\n",
       "      <td>1682</td>\n",
       "      <td>5</td>\n",
       "      <td>893286638</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  summary             user_id            movie_id              rating  \\\n",
       "0   count              100000              100000              100000   \n",
       "1    mean           462.48475           425.53013             3.52986   \n",
       "2  stddev  266.61442012750905  330.79835632558473  1.1256735991443214   \n",
       "3     min                   1                   1                   1   \n",
       "4     max                 943                1682                   5   \n",
       "\n",
       "      unix_timestamp  \n",
       "0             100000  \n",
       "1  8.8352885148862E8  \n",
       "2  5343856.189502848  \n",
       "3          874724710  \n",
       "4          893286638  "
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings.describe().toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select a few columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+\n",
      "|user_id|movie_id|\n",
      "+-------+--------+\n",
      "|    196|     242|\n",
      "|    186|     302|\n",
      "|     22|     377|\n",
      "|    244|      51|\n",
      "|    166|     346|\n",
      "|    298|     474|\n",
      "|    115|     265|\n",
      "|    253|     465|\n",
      "|    305|     451|\n",
      "|      6|      86|\n",
      "|     62|     257|\n",
      "|    286|    1014|\n",
      "|    200|     222|\n",
      "|    210|      40|\n",
      "|    224|      29|\n",
      "|    303|     785|\n",
      "|    122|     387|\n",
      "|    194|     274|\n",
      "|    291|    1042|\n",
      "|    234|    1184|\n",
      "+-------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ratings.select('user_id', 'movie_id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+------+--------------+\n",
      "|user_id|movie_id|rating|unix_timestamp|\n",
      "+-------+--------+------+--------------+\n",
      "|    253|     465|     5|     891628467|\n",
      "|    253|     510|     5|     891628416|\n",
      "|    253|     183|     5|     891628341|\n",
      "|    253|     483|     5|     891628122|\n",
      "|    253|     198|     5|     891628392|\n",
      "|    253|     127|     5|     891628060|\n",
      "|    253|     173|     5|     891628483|\n",
      "|    253|     527|     5|     891628518|\n",
      "|    253|     117|     5|     891628535|\n",
      "|    253|      87|     5|     891628278|\n",
      "|    253|     705|     5|     891628598|\n",
      "|    253|      64|     5|     891628252|\n",
      "|    253|     496|     5|     891628278|\n",
      "|    253|      79|     5|     891628518|\n",
      "|    253|      98|     5|     891628295|\n",
      "|    253|     588|     5|     891628416|\n",
      "|    253|      22|     5|     891628435|\n",
      "|    253|     494|     5|     891628341|\n",
      "|    253|      12|     5|     891628159|\n",
      "|    253|     202|     5|     891628392|\n",
      "+-------+--------+------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ratings.filter((ratings.rating==5) & (ratings.user_id==253)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Groupby"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use groupby function with a spark dataframe too. Pretty much same as a pandas groupby with the exception that you will need to import `pyspark.sql.functions`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have found the count of ratings and average rating from each user_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+------------------+\n",
      "|user_id|count(user_id)|       avg(rating)|\n",
      "+-------+--------------+------------------+\n",
      "|    148|            65|               4.0|\n",
      "|    463|           133|2.8646616541353382|\n",
      "|    471|            31|3.3870967741935485|\n",
      "|    496|           129|3.0310077519379846|\n",
      "|    833|           267| 3.056179775280899|\n",
      "|    243|            81|3.6419753086419755|\n",
      "|    392|           111| 4.045045045045045|\n",
      "|    540|            63|3.7142857142857144|\n",
      "|    623|            45|3.7333333333333334|\n",
      "|    737|            33|3.9696969696969697|\n",
      "|    858|            21|3.4285714285714284|\n",
      "|    897|           185| 3.962162162162162|\n",
      "|     31|            36|3.9166666666666665|\n",
      "|    516|            21| 4.095238095238095|\n",
      "|    251|            77| 3.792207792207792|\n",
      "|     85|           288|3.5381944444444446|\n",
      "|    137|            47| 4.319148936170213|\n",
      "|    451|            98|2.7346938775510203|\n",
      "|    580|            47|3.5531914893617023|\n",
      "|    808|            23| 4.130434782608695|\n",
      "+-------+--------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "ratings.groupBy('user_id').agg(F.count('user_id'), F.mean('rating')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+------+--------------+\n",
      "|user_id|movie_id|rating|unix_timestamp|\n",
      "+-------+--------+------+--------------+\n",
      "|      1|     113|     5|     878542738|\n",
      "|      1|     227|     4|     876892946|\n",
      "|      1|      17|     3|     875073198|\n",
      "|      1|      61|     4|     878542420|\n",
      "|      1|      90|     4|     878542300|\n",
      "|      1|      33|     4|     878542699|\n",
      "|      1|      64|     5|     875072404|\n",
      "|      1|     202|     5|     875072442|\n",
      "|      1|      92|     3|     876892425|\n",
      "|      1|     265|     4|     878542441|\n",
      "|      1|     228|     5|     878543541|\n",
      "|      1|     155|     2|     878542201|\n",
      "|      1|     266|     1|     885345728|\n",
      "|      1|      47|     4|     875072125|\n",
      "|      1|     121|     4|     875071823|\n",
      "|      1|      20|     4|     887431883|\n",
      "|      1|     114|     5|     875072173|\n",
      "|      1|     189|     3|     888732928|\n",
      "|      1|     132|     4|     878542889|\n",
      "|      1|     171|     5|     889751711|\n",
      "+-------+--------+------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ratings.sort('user_id').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descending sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+------+--------------+\n",
      "|user_id|movie_id|rating|unix_timestamp|\n",
      "+-------+--------+------+--------------+\n",
      "|    943|       2|     5|     888639953|\n",
      "|    943|      12|     5|     888639093|\n",
      "|    943|      42|     5|     888639042|\n",
      "|    943|      55|     5|     888639118|\n",
      "|    943|      56|     5|     888639269|\n",
      "|    943|      64|     5|     875409939|\n",
      "|    943|      69|     5|     888639427|\n",
      "|    943|      79|     5|     888639019|\n",
      "|    943|      92|     5|     888639660|\n",
      "|    943|      98|     5|     888638980|\n",
      "|    943|     100|     5|     875501725|\n",
      "|    943|     127|     5|     875501774|\n",
      "|    943|     173|     5|     888638960|\n",
      "|    943|     182|     5|     888639066|\n",
      "|    943|     184|     5|     888639247|\n",
      "|    943|     186|     5|     888639478|\n",
      "|    943|     187|     5|     888639147|\n",
      "|    943|     194|     5|     888639192|\n",
      "|    943|     196|     5|     888639192|\n",
      "|    943|     201|     5|     888639351|\n",
      "+-------+--------+------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "ratings.sort(F.desc('user_id'), F.desc('rating'), 'movie_id').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joins/Merging with Spark Dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I was not able to find a pandas equivalent of merge with Spark DataFrames but we can use SQL with dataframes and thus we can merge dataframes using SQL.\n",
    "\n",
    "Let us try to run some SQL on Ratings.\n",
    "\n",
    "We first register the ratings df to a temporary table ratings_table on which we can run sql operations.\n",
    "\n",
    "As you can see the result of the SQL select statement is again a Spark Dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define `sqlContext`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+------+--------------+\n",
      "|user_id|movie_id|rating|unix_timestamp|\n",
      "+-------+--------+------+--------------+\n",
      "|    253|     465|     5|     891628467|\n",
      "|    286|    1014|     5|     879781125|\n",
      "|    200|     222|     5|     876042340|\n",
      "|    122|     387|     5|     879270459|\n",
      "|     38|      95|     5|     892430094|\n",
      "|    160|     234|     5|     876861185|\n",
      "|    278|     603|     5|     891295330|\n",
      "|    287|     327|     5|     875333916|\n",
      "|    246|     201|     5|     884921594|\n",
      "|    242|    1137|     5|     879741196|\n",
      "|    249|     241|     5|     879641194|\n",
      "|     99|       4|     5|     886519097|\n",
      "|     25|     181|     5|     885853415|\n",
      "|     59|     196|     5|     888205088|\n",
      "|    290|     143|     5|     880474293|\n",
      "|     42|     423|     5|     881107687|\n",
      "|    138|      26|     5|     879024232|\n",
      "|     60|     427|     5|     883326620|\n",
      "|     57|     304|     5|     883698581|\n",
      "|    127|     229|     5|     884364867|\n",
      "+-------+--------+------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ratings.registerTempTable('ratings_table')\n",
    "newDf = sqlContext.sql('select * from ratings_table where rating > 4')\n",
    "newDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- movie_id: integer (nullable = true)\n",
      " |-- rating: integer (nullable = true)\n",
      " |-- unix_timestamp: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newDf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now add one more Spark Dataframe to the mix to see if we can use join using the SQL queries:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-----------+----+--------------------+---+---+---+---+---+----+----+----+----+----+----+----+----+----+----+----+----+----+----+\n",
      "|_c0|                 _c1|        _c2| _c3|                 _c4|_c5|_c6|_c7|_c8|_c9|_c10|_c11|_c12|_c13|_c14|_c15|_c16|_c17|_c18|_c19|_c20|_c21|_c22|_c23|\n",
      "+---+--------------------+-----------+----+--------------------+---+---+---+---+---+----+----+----+----+----+----+----+----+----+----+----+----+----+----+\n",
      "|  1|    Toy Story (1995)|01-Jan-1995|null|http://us.imdb.co...|  0|  0|  0|  1|  1|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|\n",
      "|  2|    GoldenEye (1995)|01-Jan-1995|null|http://us.imdb.co...|  0|  1|  1|  0|  0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   1|   0|   0|\n",
      "|  3|   Four Rooms (1995)|01-Jan-1995|null|http://us.imdb.co...|  0|  0|  0|  0|  0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   1|   0|   0|\n",
      "|  4|   Get Shorty (1995)|01-Jan-1995|null|http://us.imdb.co...|  0|  1|  0|  0|  0|   1|   0|   0|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|\n",
      "|  5|      Copycat (1995)|01-Jan-1995|null|http://us.imdb.co...|  0|  0|  0|  0|  0|   0|   1|   0|   1|   0|   0|   0|   0|   0|   0|   0|   1|   0|   0|\n",
      "|  6|Shanghai Triad (Y...|01-Jan-1995|null|http://us.imdb.co...|  0|  0|  0|  0|  0|   0|   0|   0|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|\n",
      "|  7|Twelve Monkeys (1...|01-Jan-1995|null|http://us.imdb.co...|  0|  0|  0|  0|  0|   0|   0|   0|   1|   0|   0|   0|   0|   0|   0|   1|   0|   0|   0|\n",
      "|  8|         Babe (1995)|01-Jan-1995|null|http://us.imdb.co...|  0|  0|  0|  0|  1|   1|   0|   0|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|\n",
      "|  9|Dead Man Walking ...|01-Jan-1995|null|http://us.imdb.co...|  0|  0|  0|  0|  0|   0|   0|   0|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|\n",
      "| 10|  Richard III (1995)|22-Jan-1996|null|http://us.imdb.co...|  0|  0|  0|  0|  0|   0|   0|   0|   1|   0|   0|   0|   0|   0|   0|   0|   0|   1|   0|\n",
      "| 11|Seven (Se7en) (1995)|01-Jan-1995|null|http://us.imdb.co...|  0|  0|  0|  0|  0|   0|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|   1|   0|   0|\n",
      "| 12|Usual Suspects, T...|14-Aug-1995|null|http://us.imdb.co...|  0|  0|  0|  0|  0|   0|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|   1|   0|   0|\n",
      "| 13|Mighty Aphrodite ...|30-Oct-1995|null|http://us.imdb.co...|  0|  0|  0|  0|  0|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|\n",
      "| 14|  Postino, Il (1994)|01-Jan-1994|null|http://us.imdb.co...|  0|  0|  0|  0|  0|   0|   0|   0|   1|   0|   0|   0|   0|   0|   1|   0|   0|   0|   0|\n",
      "| 15|Mr. Holland's Opu...|29-Jan-1996|null|http://us.imdb.co...|  0|  0|  0|  0|  0|   0|   0|   0|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|\n",
      "| 16|French Twist (Gaz...|01-Jan-1995|null|http://us.imdb.co...|  0|  0|  0|  0|  0|   1|   0|   0|   0|   0|   0|   0|   0|   0|   1|   0|   0|   0|   0|\n",
      "| 17|From Dusk Till Da...|05-Feb-1996|null|http://us.imdb.co...|  0|  1|  0|  0|  0|   1|   1|   0|   0|   0|   0|   1|   0|   0|   0|   0|   1|   0|   0|\n",
      "| 18|White Balloon, Th...|01-Jan-1995|null|http://us.imdb.co...|  0|  0|  0|  0|  0|   0|   0|   0|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|\n",
      "| 19|Antonia's Line (1...|01-Jan-1995|null|http://us.imdb.co...|  0|  0|  0|  0|  0|   0|   0|   0|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|\n",
      "| 20|Angels and Insect...|01-Jan-1995|null|http://us.imdb.co...|  0|  0|  0|  0|  0|   0|   0|   0|   1|   0|   0|   0|   0|   0|   1|   0|   0|   0|   0|\n",
      "+---+--------------------+-----------+----+--------------------+---+---+---+---+---+----+----+----+----+----+----+----+----+----+----+----+----+----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get one more df to join\n",
    "movies = spark.read.load('Data-ML-100k--master/ml-100k/u.item', \n",
    "                        format='csv', sep='|', inferSchema='true', header='false')\n",
    "movies.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change column names\n",
    "movies = movies.toDF(*['movie_id', 'movie_title',\n",
    "                       'release_date',\n",
    "                       'video_release_date','IMDb_URL','unknown','Action','Adventure',\n",
    "                       'Animation','Children','Comedy','Crime','Documentary','Drama','Fantasy',\n",
    "                       'Film_Noir','Horror','Musical','Mystery','Romance','Sci_Fi','Thriller','War','Western'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie_id</th>\n",
       "      <th>movie_title</th>\n",
       "      <th>release_date</th>\n",
       "      <th>video_release_date</th>\n",
       "      <th>IMDb_URL</th>\n",
       "      <th>unknown</th>\n",
       "      <th>Action</th>\n",
       "      <th>Adventure</th>\n",
       "      <th>Animation</th>\n",
       "      <th>Children</th>\n",
       "      <th>...</th>\n",
       "      <th>Fantasy</th>\n",
       "      <th>Film_Noir</th>\n",
       "      <th>Horror</th>\n",
       "      <th>Musical</th>\n",
       "      <th>Mystery</th>\n",
       "      <th>Romance</th>\n",
       "      <th>Sci_Fi</th>\n",
       "      <th>Thriller</th>\n",
       "      <th>War</th>\n",
       "      <th>Western</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>01-Jan-1995</td>\n",
       "      <td>None</td>\n",
       "      <td>http://us.imdb.com/M/title-exact?Toy%20Story%2...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>GoldenEye (1995)</td>\n",
       "      <td>01-Jan-1995</td>\n",
       "      <td>None</td>\n",
       "      <td>http://us.imdb.com/M/title-exact?GoldenEye%20(...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Four Rooms (1995)</td>\n",
       "      <td>01-Jan-1995</td>\n",
       "      <td>None</td>\n",
       "      <td>http://us.imdb.com/M/title-exact?Four%20Rooms%...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Get Shorty (1995)</td>\n",
       "      <td>01-Jan-1995</td>\n",
       "      <td>None</td>\n",
       "      <td>http://us.imdb.com/M/title-exact?Get%20Shorty%...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Copycat (1995)</td>\n",
       "      <td>01-Jan-1995</td>\n",
       "      <td>None</td>\n",
       "      <td>http://us.imdb.com/M/title-exact?Copycat%20(1995)</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1677</th>\n",
       "      <td>1678</td>\n",
       "      <td>Mat' i syn (1997)</td>\n",
       "      <td>06-Feb-1998</td>\n",
       "      <td>None</td>\n",
       "      <td>http://us.imdb.com/M/title-exact?Mat%27+i+syn+...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1678</th>\n",
       "      <td>1679</td>\n",
       "      <td>B. Monkey (1998)</td>\n",
       "      <td>06-Feb-1998</td>\n",
       "      <td>None</td>\n",
       "      <td>http://us.imdb.com/M/title-exact?B%2E+Monkey+(...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1679</th>\n",
       "      <td>1680</td>\n",
       "      <td>Sliding Doors (1998)</td>\n",
       "      <td>01-Jan-1998</td>\n",
       "      <td>None</td>\n",
       "      <td>http://us.imdb.com/Title?Sliding+Doors+(1998)</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1680</th>\n",
       "      <td>1681</td>\n",
       "      <td>You So Crazy (1994)</td>\n",
       "      <td>01-Jan-1994</td>\n",
       "      <td>None</td>\n",
       "      <td>http://us.imdb.com/M/title-exact?You%20So%20Cr...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1681</th>\n",
       "      <td>1682</td>\n",
       "      <td>Scream of Stone (Schrei aus Stein) (1991)</td>\n",
       "      <td>08-Mar-1996</td>\n",
       "      <td>None</td>\n",
       "      <td>http://us.imdb.com/M/title-exact?Schrei%20aus%...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1682 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      movie_id                                movie_title release_date  \\\n",
       "0            1                           Toy Story (1995)  01-Jan-1995   \n",
       "1            2                           GoldenEye (1995)  01-Jan-1995   \n",
       "2            3                          Four Rooms (1995)  01-Jan-1995   \n",
       "3            4                          Get Shorty (1995)  01-Jan-1995   \n",
       "4            5                             Copycat (1995)  01-Jan-1995   \n",
       "...        ...                                        ...          ...   \n",
       "1677      1678                          Mat' i syn (1997)  06-Feb-1998   \n",
       "1678      1679                           B. Monkey (1998)  06-Feb-1998   \n",
       "1679      1680                       Sliding Doors (1998)  01-Jan-1998   \n",
       "1680      1681                        You So Crazy (1994)  01-Jan-1994   \n",
       "1681      1682  Scream of Stone (Schrei aus Stein) (1991)  08-Mar-1996   \n",
       "\n",
       "     video_release_date                                           IMDb_URL  \\\n",
       "0                  None  http://us.imdb.com/M/title-exact?Toy%20Story%2...   \n",
       "1                  None  http://us.imdb.com/M/title-exact?GoldenEye%20(...   \n",
       "2                  None  http://us.imdb.com/M/title-exact?Four%20Rooms%...   \n",
       "3                  None  http://us.imdb.com/M/title-exact?Get%20Shorty%...   \n",
       "4                  None  http://us.imdb.com/M/title-exact?Copycat%20(1995)   \n",
       "...                 ...                                                ...   \n",
       "1677               None  http://us.imdb.com/M/title-exact?Mat%27+i+syn+...   \n",
       "1678               None  http://us.imdb.com/M/title-exact?B%2E+Monkey+(...   \n",
       "1679               None      http://us.imdb.com/Title?Sliding+Doors+(1998)   \n",
       "1680               None  http://us.imdb.com/M/title-exact?You%20So%20Cr...   \n",
       "1681               None  http://us.imdb.com/M/title-exact?Schrei%20aus%...   \n",
       "\n",
       "      unknown  Action  Adventure  Animation  Children  ...  Fantasy  \\\n",
       "0           0       0          0          1         1  ...        0   \n",
       "1           0       1          1          0         0  ...        0   \n",
       "2           0       0          0          0         0  ...        0   \n",
       "3           0       1          0          0         0  ...        0   \n",
       "4           0       0          0          0         0  ...        0   \n",
       "...       ...     ...        ...        ...       ...  ...      ...   \n",
       "1677        0       0          0          0         0  ...        0   \n",
       "1678        0       0          0          0         0  ...        0   \n",
       "1679        0       0          0          0         0  ...        0   \n",
       "1680        0       0          0          0         0  ...        0   \n",
       "1681        0       0          0          0         0  ...        0   \n",
       "\n",
       "      Film_Noir  Horror  Musical  Mystery  Romance  Sci_Fi  Thriller  War  \\\n",
       "0             0       0        0        0        0       0         0    0   \n",
       "1             0       0        0        0        0       0         1    0   \n",
       "2             0       0        0        0        0       0         1    0   \n",
       "3             0       0        0        0        0       0         0    0   \n",
       "4             0       0        0        0        0       0         1    0   \n",
       "...         ...     ...      ...      ...      ...     ...       ...  ...   \n",
       "1677          0       0        0        0        0       0         0    0   \n",
       "1678          0       0        0        0        1       0         1    0   \n",
       "1679          0       0        0        0        1       0         0    0   \n",
       "1680          0       0        0        0        0       0         0    0   \n",
       "1681          0       0        0        0        0       0         0    0   \n",
       "\n",
       "      Western  \n",
       "0           0  \n",
       "1           0  \n",
       "2           0  \n",
       "3           0  \n",
       "4           0  \n",
       "...       ...  \n",
       "1677        0  \n",
       "1678        0  \n",
       "1679        0  \n",
       "1680        0  \n",
       "1681        0  \n",
       "\n",
       "[1682 rows x 24 columns]"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+------+--------------+--------------------+\n",
      "|user_id|movie_id|rating|unix_timestamp|         movie_title|\n",
      "+-------+--------+------+--------------+--------------------+\n",
      "|    196|     242|     3|     881250949|        Kolya (1996)|\n",
      "|    186|     302|     3|     891717742|L.A. Confidential...|\n",
      "|     22|     377|     1|     878887116| Heavyweights (1994)|\n",
      "|    244|      51|     2|     880606923|Legends of the Fa...|\n",
      "|    166|     346|     1|     886397596| Jackie Brown (1997)|\n",
      "|    298|     474|     4|     884182806|Dr. Strangelove o...|\n",
      "|    115|     265|     2|     881171488|Hunt for Red Octo...|\n",
      "|    253|     465|     5|     891628467|Jungle Book, The ...|\n",
      "|    305|     451|     3|     886324817|       Grease (1978)|\n",
      "|      6|      86|     3|     883603013|Remains of the Da...|\n",
      "|     62|     257|     2|     879372434| Men in Black (1997)|\n",
      "|    286|    1014|     5|     879781125|Romy and Michele'...|\n",
      "|    200|     222|     5|     876042340|Star Trek: First ...|\n",
      "|    210|      40|     3|     891035994|To Wong Foo, Than...|\n",
      "|    224|      29|     3|     888104457|Batman Forever (1...|\n",
      "|    303|     785|     3|     879485318|     Only You (1994)|\n",
      "|    122|     387|     5|     879270459|Age of Innocence,...|\n",
      "|    194|     274|     2|     879539794|      Sabrina (1995)|\n",
      "|    291|    1042|     4|     874834944|   Just Cause (1995)|\n",
      "|    234|    1184|     2|     892079237|Endless Summer 2,...|\n",
      "+-------+--------+------+--------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movies.registerTempTable('movies_table')\n",
    "\n",
    "q = \"\"\"\n",
    "select ratings_table.*,\n",
    "    movies_table.movie_title\n",
    "from ratings_table\n",
    "left join movies_table on movies_table.movie_id = ratings_table.movie_id\"\"\"\n",
    "sqlContext.sql(q).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try to do what we were doing earlier with the RDDs. Finding the top 25 most rated movies:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----------+\n",
      "|movie_id|         movie_title|num_ratings|\n",
      "+--------+--------------------+-----------+\n",
      "|      50|    Star Wars (1977)|        583|\n",
      "|     258|      Contact (1997)|        509|\n",
      "|     100|        Fargo (1996)|        508|\n",
      "|     181|Return of the Jed...|        507|\n",
      "|     294|    Liar Liar (1997)|        485|\n",
      "|     286|English Patient, ...|        481|\n",
      "|     288|       Scream (1996)|        478|\n",
      "|       1|    Toy Story (1995)|        452|\n",
      "|     300|Air Force One (1997)|        431|\n",
      "|     121|Independence Day ...|        429|\n",
      "|     174|Raiders of the Lo...|        420|\n",
      "|     127|Godfather, The (1...|        413|\n",
      "|      56| Pulp Fiction (1994)|        394|\n",
      "|       7|Twelve Monkeys (1...|        392|\n",
      "|      98|Silence of the La...|        390|\n",
      "|     237|Jerry Maguire (1996)|        384|\n",
      "|     117|    Rock, The (1996)|        378|\n",
      "|     172|Empire Strikes Ba...|        367|\n",
      "|     222|Star Trek: First ...|        365|\n",
      "|     204|Back to the Futur...|        350|\n",
      "+--------+--------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q = \"\"\"\n",
    "select movie_id,\n",
    "    movie_title,\n",
    "    count(user_id) as num_ratings\n",
    "from (select r.*, m.movie_title\n",
    "    from ratings_table r\n",
    "    left join movies_table m on m.movie_id = r.movie_id) A\n",
    "group by movie_id, movie_title\n",
    "order by num_ratings desc\n",
    "\"\"\"\n",
    "\n",
    "sqlContext.sql(q).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finding the top 25 highest rated movies having more than 100 votes:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+------------------+-----------+\n",
      "|movie_id|         movie_title|        avg_rating|num_ratings|\n",
      "+--------+--------------------+------------------+-----------+\n",
      "|     408|Close Shave, A (1...| 4.491071428571429|        112|\n",
      "|     318|Schindler's List ...| 4.466442953020135|        298|\n",
      "|     169|Wrong Trousers, T...| 4.466101694915254|        118|\n",
      "|     483|   Casablanca (1942)|  4.45679012345679|        243|\n",
      "|      64|Shawshank Redempt...| 4.445229681978798|        283|\n",
      "|     603|  Rear Window (1954)|4.3875598086124405|        209|\n",
      "|      12|Usual Suspects, T...| 4.385767790262173|        267|\n",
      "|      50|    Star Wars (1977)|4.3584905660377355|        583|\n",
      "|     178| 12 Angry Men (1957)|             4.344|        125|\n",
      "|     134| Citizen Kane (1941)| 4.292929292929293|        198|\n",
      "|     427|To Kill a Mocking...| 4.292237442922374|        219|\n",
      "|     357|One Flew Over the...| 4.291666666666667|        264|\n",
      "|      98|Silence of the La...|  4.28974358974359|        390|\n",
      "|     480|North by Northwes...| 4.284916201117318|        179|\n",
      "|     127|Godfather, The (1...| 4.283292978208232|        413|\n",
      "|     285|Secrets & Lies (1...| 4.265432098765432|        162|\n",
      "|     272|Good Will Hunting...| 4.262626262626263|        198|\n",
      "|     657|Manchurian Candid...| 4.259541984732825|        131|\n",
      "|     474|Dr. Strangelove o...| 4.252577319587629|        194|\n",
      "|     174|Raiders of the Lo...| 4.252380952380952|        420|\n",
      "+--------+--------------------+------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q = \"\"\"\n",
    "select movie_id,\n",
    "    movie_title,\n",
    "    avg(rating) as avg_rating,\n",
    "    count(movie_id) as num_ratings\n",
    "from (select r.*, m.movie_title\n",
    "    from ratings_table r\n",
    "    left join movies_table m on m.movie_id = r.movie_id) A\n",
    "group by movie_id, movie_title\n",
    "having num_ratings > 100\n",
    "\n",
    "order by avg_rating desc\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "high_rated_ddf = sqlContext.sql(q)\n",
    "high_rated_ddf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do I get this to work in my notebook?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting from Spark Dataframe to RDD and vice versa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes you may want to convert to RDD from a spark Dataframe or vice versa so that you can have the best of both worlds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To convert from DF to RDD, you can simply do :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(movie_id=408, movie_title='Close Shave, A (1995)', avg_rating=4.491071428571429, num_ratings=112),\n",
       " Row(movie_id=318, movie_title=\"Schindler's List (1993)\", avg_rating=4.466442953020135, num_ratings=298)]"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "high_rated_ddf.rdd.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(movie_id=408, movie_title='Close Shave, A (1995)', avg_rating=4.491071428571429, num_ratings=112),\n",
       " Row(movie_id=318, movie_title=\"Schindler's List (1993)\", avg_rating=4.466442953020135, num_ratings=298)]"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "high_rated_ddf.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To go from an RDD to a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "# create an RDD first\n",
    "data = [('A',1),('B',2),('C',3),('D',4)]\n",
    "rdd = sc.parallelize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[498] at parallelize at PythonRDD.scala:195"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map the schema using Row\n",
    "rdd_new = rdd.map(lambda x: Row(key=x[0], value=int(x[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|key|value|\n",
      "+---+-----+\n",
      "|  A|    1|\n",
      "|  B|    2|\n",
      "|  C|    3|\n",
      "|  D|    4|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# convert the rdd to a DataFrame\n",
    "rdd_as_df = sqlContext.createDataFrame(rdd_new)\n",
    "rdd_as_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
