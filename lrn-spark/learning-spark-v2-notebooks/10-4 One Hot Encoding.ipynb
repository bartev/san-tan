{"cells":[{"cell_type":"markdown","source":["d\n# One-Hot Encoding\n\nIn this notebook we will be adding additional features to our model, as well as discuss how to handle categorical features."],"metadata":{}},{"cell_type":"code","source":["filePath = \"/databricks-datasets/learning-spark-v2/sf-airbnb/sf-airbnb-clean.parquet\"\nairbnbDF = spark.read.parquet(filePath)"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":["## Train/Test Split\n\nLet's use the same 80/20 split with the same seed as the previous notebook so we can compare our results apples to apples (unless you changed the cluster config!)"],"metadata":{}},{"cell_type":"code","source":["trainDF, testDF = airbnbDF.randomSplit([.8, .2], seed=42)"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":["## Option 1: StringIndexer, OneHotEncoder, and VectorAssembler\n\nHere, we are going to One Hot Encode (OHE) our categorical variables. The first approach we are going to use will combine StringIndexer, OneHotEncoder, and VectorAssembler.\n\nFirst we need to use `StringIndexer` to map a string column of labels to an ML column of label indices [Python](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.StringIndexer)/[Scala](https://spark.apache.org/docs/latest/api/scala/#org.apache.spark.ml.feature.StringIndexer).\n\nThen, we can apply the `OneHotEncoder` to the output of the StringIndexer [Python](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.OneHotEncoder)/[Scala](https://spark.apache.org/docs/latest/api/scala/#org.apache.spark.ml.feature.OneHotEncoder)."],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.feature import OneHotEncoder, StringIndexer\n\ncategoricalCols = [field for (field, dataType) in trainDF.dtypes \n                   if dataType == \"string\"]\nindexOutputCols = [x + \"Index\" for x in categoricalCols]\noheOutputCols = [x + \"OHE\" for x in categoricalCols]\n\nstringIndexer = StringIndexer(inputCols=categoricalCols, \n                              outputCols=indexOutputCols, \n                              handleInvalid=\"skip\")\noheEncoder = OneHotEncoder(inputCols=indexOutputCols, \n                           outputCols=oheOutputCols)"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":["Now we can combine our OHE categorical features with our numeric features."],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.feature import VectorAssembler\n\nnumericCols = [field for (field, dataType) in trainDF.dtypes \n               if ((dataType == \"double\") & (field != \"price\"))]\nassemblerInputs = oheOutputCols + numericCols\nvecAssembler = VectorAssembler(inputCols=assemblerInputs, \n                               outputCol=\"features\")"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":["## Option 2: RFormula\nInstead of manually specifying which columns are categorical to the StringIndexer and OneHotEncoder, RFormula can do that automatically for you [Python](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.RFormula)/[Scala](https://spark.apache.org/docs/latest/api/scala/#org.apache.spark.ml.feature.RFormula).\n\nWith RFormula, if you have any columns of type String, it treats it as a categorical feature and string indexes & one hot encodes it for us. Otherwise, it leaves as it is. Then it combines all of one-hot encoded features and numeric features into a single vector, called `features`."],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.feature import RFormula\n\nrFormula = RFormula(formula=\"price ~ .\", featuresCol=\"features\", labelCol=\"price\", handleInvalid=\"skip\")"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["## Linear Regression\n\nNow that we have all of our features, let's build a linear regression model."],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.regression import LinearRegression\n\nlr = LinearRegression(labelCol=\"price\", featuresCol=\"features\")"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["## Pipeline\n\nLet's put all these stages in a Pipeline. A `Pipeline` is a way of organizing all of our transformers and estimators [Python](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.Pipeline)/[Scala](https://spark.apache.org/docs/latest/api/scala/#org.apache.spark.ml.Pipeline).\n\nVerify you get the same results with Option 1 (StringIndexer, OneHotEncoderEstimator, and VectorAssembler) and Option 2 (RFormula)"],"metadata":{}},{"cell_type":"code","source":["# Option 1: StringIndexer + OHE + VectorAssembler\nfrom pyspark.ml import Pipeline\n\nstages = [stringIndexer, oheEncoder, vecAssembler, lr]\npipeline = Pipeline(stages=stages)\n\npipelineModel = pipeline.fit(trainDF)\npredDF = pipelineModel.transform(testDF)\npredDF.select(\"features\", \"price\", \"prediction\").show(5)"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["# Option 2: RFormula\nfrom pyspark.ml import Pipeline\n\npipeline = Pipeline(stages = [rFormula, lr])\n\npipelineModel = pipeline.fit(trainDF)\npredDF = pipelineModel.transform(testDF)\npredDF.select(\"features\", \"price\", \"prediction\").show(5)"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":["## Evaluate Model: RMSE"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.evaluation import RegressionEvaluator\n\nregressionEvaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"price\", metricName=\"rmse\")\n\nrmse = round(regressionEvaluator.evaluate(predDF), 2)\nprint(f\"RMSE is {rmse}\")"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":["## R2\n\n![](https://files.training.databricks.com/images/r2d2.jpg) How is our R2 doing?"],"metadata":{}},{"cell_type":"code","source":["r2 = round(regressionEvaluator.setMetricName(\"r2\").evaluate(predDF), 2)\nprint(f\"R2 is {r2}\")"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["pipelinePath = \"/tmp/sf-airbnb/lr-pipeline-model\"\npipelineModel.write().overwrite().save(pipelinePath)"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":["## Loading models\n\nWhen you load in models, you need to know the type of model you are loading back in (was it a linear regression or logistic regression model?).\n\nFor this reason, we recommend you always put your transformers/estimators into a Pipeline, so you can always load the generic PipelineModel back in."],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml import PipelineModel\n\nsavedPipelineModel = PipelineModel.load(pipelinePath)"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":["## Distributed Setting\n\nIf you are interested in learning how linear regression is implemented in the distributed setting and bottlenecks, check out these lecture slides:\n* [distributed-linear-regression-1](https://files.training.databricks.com/static/docs/distributed-linear-regression-1.pdf)\n* [distributed-linear-regression-2](https://files.training.databricks.com/static/docs/distributed-linear-regression-2.pdf)"],"metadata":{}}],"metadata":{"name":"10-4 One Hot Encoding","notebookId":3114109954854167},"nbformat":4,"nbformat_minor":0}
