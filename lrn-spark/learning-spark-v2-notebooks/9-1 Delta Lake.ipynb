{"cells":[{"cell_type":"markdown","source":["d\n# Chapter 9: Building Reliable Data Lakes with Delta Lake and Apache Spark™\n\nDelta Lake: An open-source storage format that brings ACID transactions to Apache Spark™ and big data workloads.\n\n<img src=\"https://docs.delta.io/latest/_static/delta-lake-logo.png\" width=300/>\n\n\n* **Open format**: Stored as Parquet format in blob storage.\n* **ACID Transactions**: Ensures data integrity and read consistency with complex, concurrent data pipelines.\n* **Schema Enforcement and Evolution**: Ensures data cleanliness by blocking writes with unexpected.\n* **Audit History**: History of all the operations that happened in the table.\n* **Time Travel**: Query previous versions of the table by time or version number.\n* **Deletes and upserts**: Supports deleting and upserting into tables with programmatic APIs.\n* **Scalable Metadata management**: Able to handle millions of files are scaling the metadata operations with Spark.\n* **Unified Batch and Streaming Source and Sink**: A table in Delta Lake is both a batch table, as well as a streaming source and sink. Streaming data ingest, batch historic backfill, and interactive queries all just work out of the box. \n\n### Source:\nThis notebook is a modified version of the [SAIS EU 2019 Delta Lake Tutorial](https://github.com/delta-io/delta/tree/master/examples/tutorials/saiseu19). The data used is a modified version of the public data from [Lending Club](https://www.kaggle.com/wendykan/lending-club-loan-data). It includes all funded loans from 2012 to 2017. Each loan includes applicant information provided by the applicant as well as the current loan status (Current, Late, Fully Paid, etc.) and latest payment information. For a full view of the data please view the data dictionary available [here](https://resources.lendingclub.com/LCDataDictionary.xlsx)."],"metadata":{}},{"cell_type":"markdown","source":["## ![Delta Lake Tiny Logo](https://pages.databricks.com/rs/094-YMS-629/images/delta-lake-tiny-logo.png) Loading data in Delta Lake table\n\nFirst let’s, read this data and save it as a Delta Lake table."],"metadata":{}},{"cell_type":"code","source":["spark.sql(\"set spark.sql.shuffle.partitions = 1\")\n\nsourcePath = \"/databricks-datasets/learning-spark-v2/loans/loan-risks.snappy.parquet\"\n\n# Configure Delta Lake Path\ndeltaPath = \"/tmp/loans_delta\"\n\n# Remove folder if it exists\ndbutils.fs.rm(deltaPath, recurse=True)\n\n# Create the Delta table with the same loans data\n(spark.read.format(\"parquet\").load(sourcePath) \n  .write.format(\"delta\").save(deltaPath))\n\nspark.read.format(\"delta\").load(deltaPath).createOrReplaceTempView(\"loans_delta\")\nprint(\"Defined view 'loans_delta'\")"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":["Let's explore the data."],"metadata":{}},{"cell_type":"code","source":["spark.sql(\"SELECT count(*) FROM loans_delta\").show()"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["spark.sql(\"SELECT * FROM loans_delta LIMIT 5\").show()"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":["## ![Delta Lake Tiny Logo](https://pages.databricks.com/rs/094-YMS-629/images/delta-lake-tiny-logo.png) Loading data streams into Delta Lake table\n\nWe will generate a stream of data from with randomly generated loan ids and amounts. \nIn addition, we are going to define a few useful utility functions."],"metadata":{}},{"cell_type":"code","source":["import random\nimport os\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.types import *\n\ndef random_checkpoint_dir(): \n  return \"/tmp/chkpt/%s\" % str(random.randint(0, 10000))\n\n\n# User-defined function to generate random state\n\nstates = [\"CA\", \"TX\", \"NY\", \"WA\"]\n\n@udf(returnType=StringType())\ndef random_state():\n  return str(random.choice(states))\n\n\n# Function to start a streaming query with a stream of randomly generated data and append to the parquet table\ndef generate_and_append_data_stream():\n\n  newLoanStreamDF = (spark.readStream.format(\"rate\").option(\"rowsPerSecond\", 5).load() \n    .withColumn(\"loan_id\", 10000 + col(\"value\")) \n    .withColumn(\"funded_amnt\", (rand() * 5000 + 5000).cast(\"integer\")) \n    .withColumn(\"paid_amnt\", col(\"funded_amnt\") - (rand() * 2000)) \n    .withColumn(\"addr_state\", random_state())\n    .select(\"loan_id\", \"funded_amnt\", \"paid_amnt\", \"addr_state\"))\n    \n  checkpointDir = random_checkpoint_dir()\n\n  streamingQuery = (newLoanStreamDF.writeStream \n    .format(\"delta\") \n    .option(\"checkpointLocation\", random_checkpoint_dir()) \n    .trigger(processingTime = \"10 seconds\") \n    .start(deltaPath))\n\n  return streamingQuery\n\n# Function to stop all streaming queries \ndef stop_all_streams():\n  # Stop all the streams\n  print(\"Stopping all streams\")\n  for s in spark.streams.active:\n    s.stop()\n  print(\"Stopped all streams\")\n  print(\"Deleting checkpoints\")  \n  dbutils.fs.rm(\"/tmp/chkpt/\", True)\n  print(\"Deleted checkpoints\")"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["streamingQuery = generate_and_append_data_stream()"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["You can see that the streaming query is adding data to the table by counting the number of records in the table. Run the following cell multiple times."],"metadata":{}},{"cell_type":"code","source":["spark.sql(\"SELECT count(*) FROM loans_delta\").show()"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":["**Remember to stop all the streaming queries.**"],"metadata":{}},{"cell_type":"code","source":["stop_all_streams()"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["##  ![Delta Lake Tiny Logo](https://pages.databricks.com/rs/094-YMS-629/images/delta-lake-tiny-logo.png) Enforcing schema on write to prevent data corruption\n\n Let’s test this by trying to write some data with an additional column `closed` that signifies whether the loan has been terminated. Note that this column does not exist in the table."],"metadata":{}},{"cell_type":"code","source":["cols = ['loan_id', 'funded_amnt', 'paid_amnt', 'addr_state', 'closed']\n\nitems = [\n  (1111111, 1000, 1000.0, 'TX', True), \n  (2222222, 2000, 0.0, 'CA', False)\n]\n\nfrom pyspark.sql.functions import *\n\nloanUpdates = (spark\n                .createDataFrame(items, cols)\n                .withColumn(\"funded_amnt\", col(\"funded_amnt\").cast(\"int\")))"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["# Uncomment the line below and it will error\n# loanUpdates.write.format(\"delta\").mode(\"append\").save(deltaPath)"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":["##  ![Delta Lake Tiny Logo](https://pages.databricks.com/rs/094-YMS-629/images/delta-lake-tiny-logo.png) Evolving schema to accommodate changing data"],"metadata":{}},{"cell_type":"code","source":["(loanUpdates.write.format(\"delta\").mode(\"append\")\n  .option(\"mergeSchema\", \"true\")\n  .save(deltaPath))"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":["Let's query the table once again to see the schema."],"metadata":{}},{"cell_type":"code","source":["spark.read.format(\"delta\").load(deltaPath).filter(\"loan_id = 1111111\").show()"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":["For existing rows are read, the value of the new column is considered as NULL."],"metadata":{}},{"cell_type":"code","source":["spark.read.format(\"delta\").load(deltaPath).show()"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":["##  ![Delta Lake Tiny Logo](https://pages.databricks.com/rs/094-YMS-629/images/delta-lake-tiny-logo.png) Transforming existing data\n\n\nLet's look into how we can transform existing data. But first, let's refine the view on the table because the schema has changed and the view needs to redefined to pick up the new schema."],"metadata":{}},{"cell_type":"code","source":["spark.read.format(\"delta\").load(deltaPath).createOrReplaceTempView(\"loans_delta\")\nprint(\"Defined view 'loans_delta'\")"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":["#### Updating loan data to fix errors\n* Upon reviewing the data, we realized that all of the loans assigned to `addr_state = 'OR'` should have been assigned to `addr_state = 'WA'`.\n* In Parquet, to do an `update`, you would need to \n  * Copy all of the rows that are not affected into a new table\n  * Copy all of the rows that are affected into a DataFrame, perform the data modification\n  * Insert the previously noted DataFrame's rows into the new table\n  * Remove the old table\n  * Rename the new table to the old table"],"metadata":{}},{"cell_type":"code","source":["spark.sql(\"\"\"SELECT addr_state, count(1) FROM loans_delta WHERE addr_state IN ('OR', 'WA', 'CA', 'TX', 'NY') GROUP BY addr_state\"\"\").show()"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":["Let's fix the data."],"metadata":{}},{"cell_type":"code","source":["from delta.tables import *\n\ndeltaTable = DeltaTable.forPath(spark, deltaPath)\ndeltaTable.update(\"addr_state = 'OR'\",  {\"addr_state\": \"'WA'\"})"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"markdown","source":["Let's see the data once again."],"metadata":{}},{"cell_type":"code","source":["spark.sql(\"\"\"SELECT addr_state, count(1) FROM loans_delta WHERE addr_state IN ('OR', 'WA', 'CA', 'TX', 'NY') GROUP BY addr_state\"\"\").show()"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"markdown","source":["#### Deleting user-related data from a table General Data Protection Regulation (GDPR)\n\nYou can remove data that matches a predicate from a Delta Lake table. Let's say we want to remove all the fully paid loans. Let's first see how many are there."],"metadata":{}},{"cell_type":"code","source":["spark.sql(\"SELECT COUNT(*) FROM loans_delta WHERE funded_amnt = paid_amnt\").show()"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"markdown","source":["Now let's delete them."],"metadata":{}},{"cell_type":"code","source":["from delta.tables import *\n\ndeltaTable = DeltaTable.forPath(spark, deltaPath)\ndeltaTable.delete(\"funded_amnt = paid_amnt\")"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"markdown","source":["Let's check the number of fully paid loans once again."],"metadata":{}},{"cell_type":"code","source":["spark.sql(\"SELECT COUNT(*) FROM loans_delta WHERE funded_amnt = paid_amnt\").show()"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"markdown","source":["### Upserting change data to a table using merge\nA common use cases is Change Data Capture (CDC), where you have to replicate row changes made in an OLTP table to another table for OLAP workloads. To continue with our loan data example, say we have another table of new loan information, some of which are new loans and others are updates to existing loans. In addition, let’s say this changes table has the same schema as the loan_delta table. You can upsert these changes into the table using the DeltaTable.merge() operation which is based on the MERGE SQL command."],"metadata":{}},{"cell_type":"code","source":["spark.sql(\"select * from loans_delta where addr_state = 'NY' and loan_id < 30\").show()"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"markdown","source":["Let's say we have some changes to this data, one loan has been paid off, and another new loan has been added."],"metadata":{}},{"cell_type":"code","source":["cols = ['loan_id', 'funded_amnt', 'paid_amnt', 'addr_state', 'closed']\n\nitems = [\n  (11, 1000, 1000.0, 'NY', True),   # loan paid off\n  (12, 1000, 0.0, 'NY', False)      # new loan\n]\n\nloanUpdates = spark.createDataFrame(items, cols)"],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"markdown","source":["Now, let's update the table with the change data using the `merge` operation."],"metadata":{}},{"cell_type":"code","source":["from delta.tables import *\n\ndeltaTable = DeltaTable.forPath(spark, deltaPath)\n\n(deltaTable\n  .alias(\"t\")\n  .merge(loanUpdates.alias(\"s\"), \"t.loan_id = s.loan_id\") \n  .whenMatchedUpdateAll() \n  .whenNotMatchedInsertAll() \n  .execute())"],"metadata":{},"outputs":[],"execution_count":42},{"cell_type":"markdown","source":["Let's see whether the table has been updated."],"metadata":{}},{"cell_type":"code","source":["spark.sql(\"select * from loans_delta where addr_state = 'NY' and loan_id < 30\").show()"],"metadata":{},"outputs":[],"execution_count":44},{"cell_type":"markdown","source":["### Deduplicating data while inserting using insert-only merge\n\nThe merge operation in Delta Lake supports an extended syntax beyond that specified by the ANSI standard. It supports advanced features like the following. \n- Delete actions: For example, `MERGE … WHEN MATCHED THEN DELETE`\n- Clause conditions: For example, `MERGE … WHEN MATCHED AND <condition> THEN ...``\n- Optional actions: All the MATCHED and NOT MATCHED clauses are optional.\n- Star syntax: For example, `UPDATE *` and `INSERT *` to update/insert all the columns in the target table with matching columns from the source dataset. The equivalent API in DeltaTable is `updateAll()` and `insertAll()`, which we have already seen.\n\nThis allows you to express many more complex use cases with little code. For example, say you want to backfill the loan_delta table with historical data of past loans. But some of the historical data may already have been inserted in the table and you don't want to update them (since their emails may already have been updated). You can deduplicate by the loan_id while inserting by running the following merge operation with only the INSERT action (since the UPDATE action is optional)."],"metadata":{}},{"cell_type":"code","source":["spark.sql(\"select * from loans_delta where addr_state = 'NY' and loan_id < 30\").show()"],"metadata":{},"outputs":[],"execution_count":46},{"cell_type":"markdown","source":["Let's say we have some historical data that we want to merge with this table. One of the historical loan exists in the current table but the historical table has old values, therefore it should not update the current value present in the table. And another historical does not exist in the current table, therefore it should be inserted into the table."],"metadata":{}},{"cell_type":"code","source":["cols = ['loan_id', 'funded_amnt', 'paid_amnt', 'addr_state', 'closed']\n\nitems = [\n  (11, 1000, 0.0, 'NY', False),\n  (-100, 1000, 10.0, 'NY', False)\n]\n\nhistoricalUpdates = spark.createDataFrame(items, cols)"],"metadata":{},"outputs":[],"execution_count":48},{"cell_type":"markdown","source":["Let's do the merge."],"metadata":{}},{"cell_type":"code","source":["from delta.tables import *\n\ndeltaTable = DeltaTable.forPath(spark, deltaPath)\n\n(deltaTable\n  .alias(\"t\")\n  .merge(historicalUpdates.alias(\"s\"), \"t.loan_id = s.loan_id\") \n  .whenNotMatchedInsertAll() \n  .execute())"],"metadata":{},"outputs":[],"execution_count":50},{"cell_type":"markdown","source":["Let's see whether the table has been updated."],"metadata":{}},{"cell_type":"code","source":["spark.sql(\"select * from loans_delta where addr_state = 'NY' and loan_id < 30\").show()"],"metadata":{},"outputs":[],"execution_count":52},{"cell_type":"markdown","source":["Notice that the only change in the table is that insert of new loan, and existing loans were not updated to old values."],"metadata":{}},{"cell_type":"markdown","source":["## ![Delta Lake Tiny Logo](https://pages.databricks.com/rs/094-YMS-629/images/delta-lake-tiny-logo.png) Auditing data changes with operation history\n\nAll changes to the Delta table are recorded as commits in the table's transaction log. As you write into a Delta table or directory, every operation is automatically versioned. You can use the HISTORY command to view the table's history."],"metadata":{}},{"cell_type":"code","source":["from delta.tables import *\n\ndeltaTable = DeltaTable.forPath(spark, deltaPath)\ndeltaTable.history().show()"],"metadata":{},"outputs":[],"execution_count":55},{"cell_type":"code","source":["deltaTable.history(4).select(\"version\", \"timestamp\", \"operation\", \"operationParameters\").show(truncate=False)"],"metadata":{},"outputs":[],"execution_count":56},{"cell_type":"markdown","source":["## ![Delta Lake Tiny Logo](https://pages.databricks.com/rs/094-YMS-629/images/delta-lake-tiny-logo.png) Querying previous snapshots of the table with time travel\n\nDelta Lake’s time travel feature allows you to access previous versions of the table. Here are some possible uses of this feature:\n\n* Auditing Data Changes\n* Reproducing experiments & reports\n* Rollbacks\n\nYou can query by using either a timestamp or a version number using Python, Scala, and/or SQL syntax. For this examples we will query a specific version using the Python syntax.  \n\nFor more information, refer to [Introducing Delta Time Travel for Large Scale Data Lakes](https://databricks.com/blog/2019/02/04/introducing-delta-time-travel-for-large-scale-data-lakes.html) and the [docs](https://docs.delta.io/latest/delta-batch.html#deltatimetravel).\n\n**Let's query the table's state before we deleted the data, which still contains the fully paid loans.**"],"metadata":{}},{"cell_type":"code","source":["previousVersion = deltaTable.history(1).select(\"version\").first()[0] - 3\n\n(spark.read.format(\"delta\")\n  .option(\"versionAsOf\", previousVersion)\n  .load(deltaPath)\n  .createOrReplaceTempView(\"loans_delta_pre_delete\"))\n\nspark.sql(\"SELECT COUNT(*) FROM loans_delta_pre_delete WHERE funded_amnt = paid_amnt\").show()"],"metadata":{},"outputs":[],"execution_count":58},{"cell_type":"markdown","source":["We see the same number of fully paid loans that we had seen before delete."],"metadata":{}}],"metadata":{"name":"9-1 Delta Lake","notebookId":3114109954854557},"nbformat":4,"nbformat_minor":0}
