{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create an RDD of tuples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## setup `SparkContext`"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext('local', 'First App')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sc():\n",
    "    \"\"\"get a SparkContext (don't recreate)\"\"\"\n",
    "    import pyspark\n",
    "\n",
    "    # don't try to redefine 'sc'\n",
    "    if not globals().get('sc', False):\n",
    "        sc = pyspark.SparkContext('local', 'ch3 notebook')\n",
    "    else:\n",
    "        print('not redefining sc')\n",
    "        sc = globals()['sc']\n",
    "    return sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = get_sc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_names_ages = [('Brooke', 20),\n",
    "                   ('Denny', 31),\n",
    "                   ('Jules', 30),\n",
    "                   ('TD', 35),\n",
    "                   ('Brooke', 25)]\n",
    "\n",
    "dataRDD = sc.parallelize(data_names_ages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use map and reduceByKey transformations with their \n",
    "# lambda expressions to aggregate and then compute average\n",
    "\n",
    "agesRDD = (dataRDD.map(lambda x, y: (x, (y, 1)))\n",
    "           .reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    "           .map(lambda x, y, z: (x, y/z))\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|  name|avg(age)|\n",
      "+------+--------+\n",
      "|Brooke|    22.5|\n",
      "| Jules|    30.0|\n",
      "|    TD|    35.0|\n",
      "| Denny|    31.0|\n",
      "+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Same thing, but with a dataframe\n",
    "\n",
    "from pyspark.sql.functions import avg\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# create a SparkSession\n",
    "spark = (SparkSession\n",
    "    .builder\n",
    "    .appName(\"example\")\n",
    "    .getOrCreate())\n",
    "\n",
    "data_df = spark.createDataFrame(data_names_ages, ['name', 'age'])\n",
    "\n",
    "# group names, aggregate age, get avg\n",
    "avg_df = data_df.groupBy('name').agg(avg('age'))\n",
    "\n",
    "\n",
    "avg_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://172.20.10.2:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>ch3 notebook</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x107665c88>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create DataFrame with schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "see file `ex3-6-define-schema.py`\n",
    "\n",
    "run it using either\n",
    "\n",
    "```\n",
    "spark-submit ex3-6-define-schema.py\n",
    "```\n",
    "\n",
    "or \n",
    "\n",
    "```\n",
    "python ex3-6-define-schema.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Python \n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# define schema for our data using DDL \n",
    "schema = \"`Id` INT,`First` STRING,`Last` STRING,`Url` STRING,`Published` STRING,`Hits` INT,`Campaigns` ARRAY<STRING>\"\n",
    "# create our static data\n",
    "data = [\n",
    "    [1, \"Jules\", \"Damji\", \"https://tinyurl.1\", \"1/4/2016\", 4535, [\"twitter\", \"LinkedIn\"]],\n",
    "    [2, \"Brooke\",\"Wenig\",\"https://tinyurl.2\", \"5/5/2018\", 8908, [\"twitter\", \"LinkedIn\"]],\n",
    "    [3, \"Denny\", \"Lee\", \"https://tinyurl.3\",\"6/7/2019\",7659, [\"web\", \"twitter\", \"FB\", \"LinkedIn\"]],\n",
    "    [4, \"Tathagata\", \"Das\",\"https://tinyurl.4\", \"5/12/2018\", 10568, [\"twitter\", \"FB\"]],\n",
    "    [5, \"Matei\",\"Zaharia\", \"https://tinyurl.5\", \"5/14/2014\", 40578, [\"web\", \"twitter\", \"FB\", \"LinkedIn\"]],\n",
    "    [6, \"Reynold\", \"Xin\", \"https://tinyurl.6\", \"3/2/2015\", 25568, [\"twitter\", \"LinkedIn\"]]\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a SparkSession\n",
    "spark = (SparkSession\n",
    "    .builder\n",
    "    .appName(\"Example-3_6\")\n",
    "    .getOrCreate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning Spark, 2nd ed ch 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a DataFrame using the schema defined above\n",
    "blogs_df = spark.createDataFrame(data, schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(Id,IntegerType,true),StructField(First,StringType,true),StructField(Last,StringType,true),StructField(Url,StringType,true),StructField(Published,StringType,true),StructField(Hits,IntegerType,true),StructField(Campaigns,ArrayType(StringType,true),true)))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blogs_df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these types are defined in spark.sql.types\n",
    "scm = StructType(\n",
    "    [StructField('Id', IntegerType(), True),\n",
    "         StructField('First', StringType(), True),\n",
    "         StructField('Last', StringType(), True),\n",
    "         StructField('Url', StringType(), True),\n",
    "         StructField('Published', StringType(), True),\n",
    "         StructField('Hits', IntegerType(), True),\n",
    "         StructField('Campaigns', ArrayType(StringType(), True), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "| Id|    First|   Last|              Url|Published| Hits|           Campaigns|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "|  1|    Jules|  Damji|https://tinyurl.1| 1/4/2016| 4535| [twitter, LinkedIn]|\n",
      "|  2|   Brooke|  Wenig|https://tinyurl.2| 5/5/2018| 8908| [twitter, LinkedIn]|\n",
      "|  3|    Denny|    Lee|https://tinyurl.3| 6/7/2019| 7659|[web, twitter, FB...|\n",
      "|  4|Tathagata|    Das|https://tinyurl.4|5/12/2018|10568|       [twitter, FB]|\n",
      "|  5|    Matei|Zaharia|https://tinyurl.5|5/14/2014|40578|[web, twitter, FB...|\n",
      "|  6|  Reynold|    Xin|https://tinyurl.6| 3/2/2015|25568| [twitter, LinkedIn]|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "blogs2_df = spark.createDataFrame(data, scm)\n",
    "blogs2_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Id', 'First', 'Last', 'Url', 'Published', 'Hits', 'Campaigns']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blogs_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|double| Hits|\n",
      "+------+-----+\n",
      "|  9070| 4535|\n",
      "| 17816| 8908|\n",
      "| 15318| 7659|\n",
      "| 21136|10568|\n",
      "| 81156|40578|\n",
      "| 51136|25568|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "blogs_df.selectExpr('Hits * 2 as double','Hits').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+----------+\n",
      "|(Hits * 2)| Hits|(Hits - 1)|\n",
      "+----------+-----+----------+\n",
      "|      9070| 4535|      4534|\n",
      "|     17816| 8908|      8907|\n",
      "|     15318| 7659|      7658|\n",
      "|     21136|10568|     10567|\n",
      "|     81156|40578|     40577|\n",
      "|     51136|25568|     25567|\n",
      "+----------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "blogs2_df.select(F.col('Hits') * 2, 'Hits', F.col('Hits') - 1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-------+-----------------+---------+-----+--------------------+-----------+\n",
      "| Id|    First|   Last|              Url|Published| Hits|           Campaigns|Big Hitters|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+-----------+\n",
      "|  1|    Jules|  Damji|https://tinyurl.1| 1/4/2016| 4535| [twitter, LinkedIn]|      false|\n",
      "|  2|   Brooke|  Wenig|https://tinyurl.2| 5/5/2018| 8908| [twitter, LinkedIn]|      false|\n",
      "|  3|    Denny|    Lee|https://tinyurl.3| 6/7/2019| 7659|[web, twitter, FB...|      false|\n",
      "|  4|Tathagata|    Das|https://tinyurl.4|5/12/2018|10568|       [twitter, FB]|       true|\n",
      "|  5|    Matei|Zaharia|https://tinyurl.5|5/14/2014|40578|[web, twitter, FB...|       true|\n",
      "|  6|  Reynold|    Xin|https://tinyurl.6| 3/2/2015|25568| [twitter, LinkedIn]|       true|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "blogs2_df.withColumn('Big Hitters', (F.expr('Hits > 10000'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|    AuthorsId|\n",
      "+-------------+\n",
      "|  JulesDamji1|\n",
      "| BrookeWenig2|\n",
      "|    DennyLee3|\n",
      "|TathagataDas4|\n",
      "+-------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(blogs2_df\n",
    " .withColumn('AuthorsId', \n",
    "             (F.concat(F.expr('First'),\n",
    "                       F.expr('Last'), \n",
    "                       F.expr('Id'))))\n",
    " .select('AuthorsId')\n",
    " .show(n=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using `expr`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use `expr` to compute a value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|(Hits * 2)|\n",
      "+----------+\n",
      "|      9070|\n",
      "|     17816|\n",
      "+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "blogs_df.select(expr('Hits * 2')).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### or use `col` to compute value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|(Hits * 2)|\n",
      "+----------+\n",
      "|      9070|\n",
      "|     17816|\n",
      "+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "blogs_df.select(col('Hits') * 2).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add new columns using `withColumn` and `expr`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>First</th>\n",
       "      <th>Last</th>\n",
       "      <th>Url</th>\n",
       "      <th>Published</th>\n",
       "      <th>Hits</th>\n",
       "      <th>Campaigns</th>\n",
       "      <th>Big Hitters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Jules</td>\n",
       "      <td>Damji</td>\n",
       "      <td>https://tinyurl.1</td>\n",
       "      <td>1/4/2016</td>\n",
       "      <td>4535</td>\n",
       "      <td>[twitter, LinkedIn]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Brooke</td>\n",
       "      <td>Wenig</td>\n",
       "      <td>https://tinyurl.2</td>\n",
       "      <td>5/5/2018</td>\n",
       "      <td>8908</td>\n",
       "      <td>[twitter, LinkedIn]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Denny</td>\n",
       "      <td>Lee</td>\n",
       "      <td>https://tinyurl.3</td>\n",
       "      <td>6/7/2019</td>\n",
       "      <td>7659</td>\n",
       "      <td>[web, twitter, FB, LinkedIn]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Tathagata</td>\n",
       "      <td>Das</td>\n",
       "      <td>https://tinyurl.4</td>\n",
       "      <td>5/12/2018</td>\n",
       "      <td>10568</td>\n",
       "      <td>[twitter, FB]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Matei</td>\n",
       "      <td>Zaharia</td>\n",
       "      <td>https://tinyurl.5</td>\n",
       "      <td>5/14/2014</td>\n",
       "      <td>40578</td>\n",
       "      <td>[web, twitter, FB, LinkedIn]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>Reynold</td>\n",
       "      <td>Xin</td>\n",
       "      <td>https://tinyurl.6</td>\n",
       "      <td>3/2/2015</td>\n",
       "      <td>25568</td>\n",
       "      <td>[twitter, LinkedIn]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id      First     Last                Url  Published   Hits  \\\n",
       "0   1      Jules    Damji  https://tinyurl.1   1/4/2016   4535   \n",
       "1   2     Brooke    Wenig  https://tinyurl.2   5/5/2018   8908   \n",
       "2   3      Denny      Lee  https://tinyurl.3   6/7/2019   7659   \n",
       "3   4  Tathagata      Das  https://tinyurl.4  5/12/2018  10568   \n",
       "4   5      Matei  Zaharia  https://tinyurl.5  5/14/2014  40578   \n",
       "5   6    Reynold      Xin  https://tinyurl.6   3/2/2015  25568   \n",
       "\n",
       "                      Campaigns  Big Hitters  \n",
       "0           [twitter, LinkedIn]        False  \n",
       "1           [twitter, LinkedIn]        False  \n",
       "2  [web, twitter, FB, LinkedIn]        False  \n",
       "3                 [twitter, FB]         True  \n",
       "4  [web, twitter, FB, LinkedIn]         True  \n",
       "5           [twitter, LinkedIn]         True  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(blogs_df\n",
    " .withColumn('Big Hitters', expr('Hits > 10000'))\n",
    " .toPandas()\n",
    "#  .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use `expr` to concatenate columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|    AuthorsId|\n",
      "+-------------+\n",
      "|  JulesDamji1|\n",
      "| BrookeWenig2|\n",
      "|    DennyLee3|\n",
      "|TathagataDas4|\n",
      "|MateiZaharia5|\n",
      "|  ReynoldXin6|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import concat\n",
    "\n",
    "(blogs_df\n",
    " .withColumn('AuthorsId', \n",
    "             concat(expr('First'), expr('Last'), expr('Id')))\n",
    "#  .select(expr('AuthorsId'))\n",
    " .select('AuthorsId')\n",
    " .show() \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 ways to do the same thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "| Id|    First|   Last|              Url|Published| Hits|           Campaigns|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "|  1|    Jules|  Damji|https://tinyurl.1| 1/4/2016| 4535| [twitter, LinkedIn]|\n",
      "|  2|   Brooke|  Wenig|https://tinyurl.2| 5/5/2018| 8908| [twitter, LinkedIn]|\n",
      "|  3|    Denny|    Lee|https://tinyurl.3| 6/7/2019| 7659|[web, twitter, FB...|\n",
      "|  4|Tathagata|    Das|https://tinyurl.4|5/12/2018|10568|       [twitter, FB]|\n",
      "|  5|    Matei|Zaharia|https://tinyurl.5|5/14/2014|40578|[web, twitter, FB...|\n",
      "|  6|  Reynold|    Xin|https://tinyurl.6| 3/2/2015|25568| [twitter, LinkedIn]|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "blogs_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "| Id|    First|   Last|              Url|Published| Hits|           Campaigns|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "|  1|    Jules|  Damji|https://tinyurl.1| 1/4/2016| 4535| [twitter, LinkedIn]|\n",
      "|  2|   Brooke|  Wenig|https://tinyurl.2| 5/5/2018| 8908| [twitter, LinkedIn]|\n",
      "|  3|    Denny|    Lee|https://tinyurl.3| 6/7/2019| 7659|[web, twitter, FB...|\n",
      "|  4|Tathagata|    Das|https://tinyurl.4|5/12/2018|10568|       [twitter, FB]|\n",
      "|  5|    Matei|Zaharia|https://tinyurl.5|5/14/2014|40578|[web, twitter, FB...|\n",
      "|  6|  Reynold|    Xin|https://tinyurl.6| 3/2/2015|25568| [twitter, LinkedIn]|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "blogs2_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|Hits|\n",
      "+----+\n",
      "|4535|\n",
      "|8908|\n",
      "+----+\n",
      "only showing top 2 rows\n",
      "\n",
      "+----+\n",
      "|Hits|\n",
      "+----+\n",
      "|4535|\n",
      "|8908|\n",
      "+----+\n",
      "only showing top 2 rows\n",
      "\n",
      "\"col\" is short for \"column\"\n",
      "+----+\n",
      "|Hits|\n",
      "+----+\n",
      "|4535|\n",
      "|8908|\n",
      "+----+\n",
      "only showing top 2 rows\n",
      "\n",
      "+----+\n",
      "|Hits|\n",
      "+----+\n",
      "|4535|\n",
      "|8908|\n",
      "+----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "blogs_df.select('Hits').show(2)\n",
    "blogs_df.select(expr('Hits')).show(2)\n",
    "\n",
    "print('\"col\" is short for \"column\"')\n",
    "blogs_df.select(col('Hits')).show(2)\n",
    "blogs_df.select(column('Hits')).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sort by `Id`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "| Id|    First|   Last|              Url|Published| Hits|           Campaigns|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "|  6|  Reynold|    Xin|https://tinyurl.6| 3/2/2015|25568| [twitter, LinkedIn]|\n",
      "|  5|    Matei|Zaharia|https://tinyurl.5|5/14/2014|40578|[web, twitter, FB...|\n",
      "|  4|Tathagata|    Das|https://tinyurl.4|5/12/2018|10568|       [twitter, FB]|\n",
      "|  3|    Denny|    Lee|https://tinyurl.3| 6/7/2019| 7659|[web, twitter, FB...|\n",
      "|  2|   Brooke|  Wenig|https://tinyurl.2| 5/5/2018| 8908| [twitter, LinkedIn]|\n",
      "|  1|    Jules|  Damji|https://tinyurl.1| 1/4/2016| 4535| [twitter, LinkedIn]|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "blogs_df.sort(col('Id').desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-------+-----------------+---------+-----+--------------------+--------+\n",
      "| Id|    First|   Last|              Url|Published| Hits|           Campaigns|     new|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+--------+\n",
      "|  1|    Jules|  Damji|https://tinyurl.1| 1/4/2016| 4535| [twitter, LinkedIn]|   Jules|\n",
      "|  2|   Brooke|  Wenig|https://tinyurl.2| 5/5/2018| 8908| [twitter, LinkedIn]|********|\n",
      "|  3|    Denny|    Lee|https://tinyurl.3| 6/7/2019| 7659|[web, twitter, FB...|   Denny|\n",
      "|  4|Tathagata|    Das|https://tinyurl.4|5/12/2018|10568|       [twitter, FB]|********|\n",
      "|  5|    Matei|Zaharia|https://tinyurl.5|5/14/2014|40578|[web, twitter, FB...|   Matei|\n",
      "|  6|  Reynold|    Xin|https://tinyurl.6| 3/2/2015|25568| [twitter, LinkedIn]|********|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(blogs_df\n",
    "    .withColumn('new',\n",
    "               when(blogs_df['Id'].isin([1,3,5]),\n",
    "                    col('First'))\n",
    "               .otherwise('*' * 8))\n",
    ".show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: Ch 3 says that `$` is a spark function that converts something to a column. However, it doesn't work here"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "blogs_df.sort($'Id'.desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "| Id|    First|   Last|              Url|Published| Hits|           Campaigns|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "|  5|    Matei|Zaharia|https://tinyurl.5|5/14/2014|40578|[web, twitter, FB...|\n",
      "|  4|Tathagata|    Das|https://tinyurl.4|5/12/2018|10568|       [twitter, FB]|\n",
      "|  3|    Denny|    Lee|https://tinyurl.3| 6/7/2019| 7659|[web, twitter, FB...|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(blogs2_df\n",
    " .filter('`Id` < 6')\n",
    " .filter('Id > 2')\n",
    " .sort(col('Id').desc())\n",
    " .show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `where` and `filter` are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-------+-----------------+---------+-----+--------------------+\n",
      "| Id|First|   Last|              Url|Published| Hits|           Campaigns|\n",
      "+---+-----+-------+-----------------+---------+-----+--------------------+\n",
      "|  5|Matei|Zaharia|https://tinyurl.5|5/14/2014|40578|[web, twitter, FB...|\n",
      "+---+-----+-------+-----------------+---------+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use plain SQL in `filter`\n",
    "(blogs2_df\n",
    " .filter('`Id` < 6')\n",
    " .filter('Id > 2')\n",
    " .filter(\"Last like 'Z%'\")\n",
    " .sort(col('Id').desc())\n",
    " .show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-------+-----------------+---------+-----+--------------------+\n",
      "| Id|First|   Last|              Url|Published| Hits|           Campaigns|\n",
      "+---+-----+-------+-----------------+---------+-----+--------------------+\n",
      "|  5|Matei|Zaharia|https://tinyurl.5|5/14/2014|40578|[web, twitter, FB...|\n",
      "+---+-----+-------+-----------------+---------+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use plain SQL in `filter`\n",
    "(blogs2_df\n",
    " .filter('`Id` < 6')\n",
    " .filter('Id > 2')\n",
    " .where(\"Last like 'Z%'\")\n",
    " .sort(col('Id').desc())\n",
    " .show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate a row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Python\n",
    "from pyspark.sql import Row\n",
    "\n",
    "blog_row = Row(6, \"Reynold\", \"Xin\", \"https://tinyurl.6\", 255568, \n",
    "               \"3/2/2015\", [\"twitter\", \"LinkedIn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index 0: 6\n",
      "index 1: Reynold\n",
      "\n",
      "6\n",
      "Reynold\n",
      "Xin\n",
      "https://tinyurl.6\n",
      "255568\n",
      "3/2/2015\n",
      "['twitter', 'LinkedIn']\n"
     ]
    }
   ],
   "source": [
    "# access using index for individual items\n",
    "print(f\"index 0: {blog_row[0]}\")\n",
    "print(f\"index 1: {blog_row[1]}\")\n",
    "\n",
    "print()\n",
    "\n",
    "for item in blog_row:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(blog_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Row objects can be used to create DataFrames if you need them for quick interactivity and exploration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Python \n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using DDL String to define a schema\n",
    "schema = \"`Author` STRING, `State` STRING\"\n",
    "rows = [Row(\"Matei Zaharia\", \"CA\"), Row(\"Reynold Xin\", \"CA\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|       Author|State|\n",
      "+-------------+-----+\n",
      "|Matei Zaharia|   CA|\n",
      "|  Reynold Xin|   CA|\n",
      "+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "authors_df = spark.createDataFrame(rows, schema)\n",
    "authors_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|       author|state|\n",
      "+-------------+-----+\n",
      "|Matei Zaharia|   CA|\n",
      "|  Reynold Xin|   CA|\n",
      "+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema2 = \"\"\"\n",
    "    author string, \n",
    "    state string\n",
    "    \"\"\"\n",
    "spark.createDataFrame(rows, schema2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|       author|state|\n",
      "+-------------+-----+\n",
      "|Matei Zaharia|   CA|\n",
      "|  Reynold Xin|   CA|\n",
      "+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.createDataFrame(rows, ['author', 'state']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## drop a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_df.drop('State').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common DataFrame Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_dir = '/Users/bartev/dev/spark-3.0.0-preview2-bin-hadoop2.7'\n",
    "people_file_relative = 'examples/src/main/resources/people.csv'\n",
    "people_file = os.path.join(spark_dir, people_file_relative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/bartev/dev/spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/resources/people.csv'"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programmatic way to define a schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "people_schema = StructType([StructField('name', StringType(), True),\n",
    "                           StructField('age', IntegerType(), True),\n",
    "                           StructField('job', StringType(), True)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## infer schema from a smaller sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|      name;age;job|\n",
      "+------------------+\n",
      "|Jorge;30;Developer|\n",
      "|  Bob;32;Developer|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(spark\n",
    " .read\n",
    " .option('samplingRatio', 0.5)\n",
    " .option('header', 'true')\n",
    " .csv(people_file)\n",
    " .show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read the file using DataFrameReader using format csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "people_df = spark.read.csv(people_file, \n",
    "                           header=True, \n",
    "                           schema=people_schema, \n",
    "                           sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can separate out options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+---------+\n",
      "| name|age|      job|\n",
      "+-----+---+---------+\n",
      "|Jorge| 30|Developer|\n",
      "|  Bob| 32|Developer|\n",
      "+-----+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(spark\n",
    " .read\n",
    " .option('header', 'true')\n",
    " .option('schema', people_schema)\n",
    " .option('sep', ';')\n",
    " .csv(people_file)\n",
    " .show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+---------+\n",
      "| name|age|      job|\n",
      "+-----+---+---------+\n",
      "|Jorge| 30|Developer|\n",
      "|  Bob| 32|Developer|\n",
      "+-----+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people_tbl = people_df.write.format('parquet').save('people.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.parquet('people.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_table = 'people_tbl'\n",
    "(people_df.write\n",
    "    .format('parquet')\n",
    "    .saveAsTable(parquet_table))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projections and filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people_df = spark.read.csv(people_file, header=True, schema=people_schema, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(people_df.select('age')\n",
    "     .where('age > 30')\n",
    "    .show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_fname = '/Users/bartev/dev/github-bv/san-tan/lrn-spark/Data-ML-100k--master/ml-100k/u.item'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df = spark.read.csv(movie_fname, header=False, sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns\n",
    "\n",
    "(movies_df\n",
    " .select('_c0', '_c1', '_c2', '_c4')\n",
    " .withColumnRenamed('_c0', 'id')\n",
    " .withColumnRenamed('_c1', 'title')\n",
    " .withColumnRenamed('_c2', 'date')\n",
    " .withColumnRenamed('_c4', 'url')\n",
    " .where('date > \"1996-01-01\"')\n",
    " .where('id > 30')\n",
    "#  .schema\n",
    " .show(5)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(movies_df\n",
    " .select('_c0', '_c1', '_c2', '_c4')\n",
    " .withColumnRenamed('_c0', 'id')\n",
    " .withColumnRenamed('_c1', 'title')\n",
    " .withColumnRenamed('_c2', 'date')\n",
    " .withColumnRenamed('_c4', 'url')\n",
    " .where('date > \"1996-01-01\"')\n",
    " .where('id > 30')\n",
    "#  .schema\n",
    " .select('date')\n",
    " .distinct()\n",
    " .count()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how do I convert column 'id' to an int?\n",
    "\n",
    "(movies_df\n",
    " .select('_c0', '_c1', '_c2', '_c4')\n",
    " .withColumnRenamed('_c0', 'id')\n",
    " .withColumnRenamed('_c1', 'title')\n",
    " .withColumnRenamed('_c2', 'date')\n",
    " .withColumnRenamed('_c4', 'url')\n",
    "#  .where('date > \"1996-01-01\"')\n",
    " .where('id > 30')\n",
    " .where(F.col('id') < \"38\")\n",
    "#  .schema\n",
    " .select('id', 'date')\n",
    "#  .distinct()\n",
    " .show(10, False)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df2 = (movies_df\n",
    " .select('_c0', '_c1', '_c2', '_c4')\n",
    " .withColumnRenamed('_c0', 'id')\n",
    " .withColumnRenamed('_c1', 'title')\n",
    " .withColumnRenamed('_c2', 'date')\n",
    " .withColumnRenamed('_c4', 'url')\n",
    " .where('id > 30')\n",
    " .select('id', 'date')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df2.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df2.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df2.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(movies_df2\n",
    " .withColumn('new_date', F.to_date(F.col('date'), 'dd-MMM-yyyy'))\n",
    " .withColumn('new_ts', F.to_timestamp(F.col('date'), 'dd-MMM-yyyy'))\n",
    " .where(F.col('new_date') < '1990-01-01')\n",
    " .show())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### order by year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(movies_df2\n",
    " .withColumn('new_date', F.to_date(F.col('date'), 'dd-MMM-yyyy'))\n",
    " .withColumn('new_ts', F.to_timestamp(F.col('date'), 'dd-MMM-yyyy'))\n",
    " .where(F.col('new_date') < '1990-01-01')\n",
    " .orderBy(F.year('new_date'))\n",
    " .withColumn('year', F.year('new_date'))\n",
    " .withColumn('month', F.month('new_date'))\n",
    " .where(F.col('month') != 1)\n",
    " .show())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## write to csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With `repartition`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(movies_df2\n",
    "    .withColumn('year', F.year(F.to_date(F.col('date'), 'dd-MMM-yyyy')))\n",
    "     .select('year')\n",
    "     .distinct()\n",
    "     .orderBy('year')\n",
    "     .where('date != \"null\"')\n",
    "     .repartition(1)\n",
    "    .write\n",
    " .format('csv')\n",
    " .option('header', 'true')\n",
    " .save('movie_dates.csv')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### with `coalesce`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(movies_df2\n",
    "    .withColumn('year', F.year(F.to_date(F.col('date'), 'dd-MMM-yyyy')))\n",
    "     .select('year')\n",
    "     .distinct()\n",
    "     .orderBy('year')\n",
    "     .where('date != \"null\"')\n",
    "     .coalesce(1)\n",
    "    .write\n",
    " .format('csv')\n",
    " .option('header', 'true')\n",
    " .save('movie_dates_coalesce.csv')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### with `pandas`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(movies_df2\n",
    "    .withColumn('year', F.year(F.to_date(F.col('date'), 'dd-MMM-yyyy')))\n",
    "     .select('year')\n",
    "     .distinct()\n",
    "     .orderBy('year')\n",
    "     .where('date != \"null\"')\n",
    "    .toPandas()\n",
    " .to_csv('movie_dates_pandas.csv', header=True, index=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df = (spark.read\n",
    "             .csv(movie_fname, header=False, sep='|')\n",
    "             .select('_c0', '_c1', '_c2', '_c4')\n",
    "             .withColumnRenamed('_c0', 'id')\n",
    "             .withColumnRenamed('_c1', 'title')\n",
    "             .withColumnRenamed('_c2', 'date')\n",
    "             .withColumnRenamed('_c4', 'url')\n",
    "             .where('id > 30')\n",
    "             .select('id', 'date', 'title', 'url')\n",
    ")\n",
    "movies_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(movies_df\n",
    " .where(F.col('date').isNotNull())\n",
    " .withColumn('new_date', F.to_date('date', 'dd-MMM-yyyy'))\n",
    " .withColumn('year', F.year(F.to_date(F.col('date'), 'dd-MMM-yyyy')))\n",
    " .groupBy('year')\n",
    " .count()\n",
    " .orderBy('count', ascending=False)\n",
    " .select(F.sum('count'), F.avg('count'), F.stddev('count'), F.min('count'), F.max('count'))\n",
    " .show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(movies_df\n",
    " .where(F.col('date').isNotNull())\n",
    " .withColumn('new_date', F.to_date('date', 'dd-MMM-yyyy'))\n",
    " .withColumn('year', F.year(F.to_date(F.col('date'), 'dd-MMM-yyyy')))\n",
    " .groupBy('year')\n",
    " .count()\n",
    " .orderBy('count', ascending=False)\n",
    " .select(F.sum('count'), F.avg('count'), F.stddev('count'), F.min('count'), F.max('count'))\n",
    " .printSchema())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "row = Row(350, True, 'Learning Spark 2E', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "row = Row(350, True, \"Learning Spark 2E\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[r for r in row]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
