{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create DataFrame with schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Python \n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SparkSession\n",
    "# define schema for our data using DDL \n",
    "schema = \"`Id` INT,`First` STRING,`Last` STRING,`Url` STRING,`Published` STRING,`Hits` INT,`Campaigns` ARRAY<STRING>\"\n",
    "# create our static data\n",
    "data = [\n",
    "    [1, \"Jules\", \"Damji\", \"https://tinyurl.1\", \"1/4/2016\", 4535, [\"twitter\", \"LinkedIn\"]],\n",
    "    [2, \"Brooke\",\"Wenig\",\"https://tinyurl.2\", \"5/5/2018\", 8908, [\"twitter\", \"LinkedIn\"]],\n",
    "    [3, \"Denny\", \"Lee\", \"https://tinyurl.3\",\"6/7/2019\",7659, [\"web\", \"twitter\", \"FB\", \"LinkedIn\"]],\n",
    "    [4, \"Tathagata\", \"Das\",\"https://tinyurl.4\", \"5/12/2018\", 10568, [\"twitter\", \"FB\"]],\n",
    "    [5, \"Matei\",\"Zaharia\", \"https://tinyurl.5\", \"5/14/2014\", 40578, [\"web\", \"twitter\", \"FB\", \"LinkedIn\"]],\n",
    "    [6, \"Reynold\", \"Xin\", \"https://tinyurl.6\", \"3/2/2015\", 25568, [\"twitter\", \"LinkedIn\"]]\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a SparkSession\n",
    "spark = (SparkSession\n",
    "    .builder\n",
    "    .appName(\"Example-3_6\")\n",
    "    .getOrCreate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning Spark, 2nd ed ch 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a DataFrame using the schema defined above\n",
    "blogs_df = spark.createDataFrame(data, schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(Id,IntegerType,true),StructField(First,StringType,true),StructField(Last,StringType,true),StructField(Url,StringType,true),StructField(Published,StringType,true),StructField(Hits,IntegerType,true),StructField(Campaigns,ArrayType(StringType,true),true)))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blogs_df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.types as pst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "scm = StructType(\n",
    "    [StructField('Id', IntegerType(), True),\n",
    "         StructField('First', StringType(), True),\n",
    "         StructField('Last', StringType(), True),\n",
    "         StructField('Url', StringType(), True),\n",
    "         StructField('Published', StringType(), True),\n",
    "         StructField('Hits', IntegerType(), True),\n",
    "         StructField('Campaigns', ArrayType(StringType(), True), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "| Id|    First|   Last|              Url|Published| Hits|           Campaigns|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "|  1|    Jules|  Damji|https://tinyurl.1| 1/4/2016| 4535| [twitter, LinkedIn]|\n",
      "|  2|   Brooke|  Wenig|https://tinyurl.2| 5/5/2018| 8908| [twitter, LinkedIn]|\n",
      "|  3|    Denny|    Lee|https://tinyurl.3| 6/7/2019| 7659|[web, twitter, FB...|\n",
      "|  4|Tathagata|    Das|https://tinyurl.4|5/12/2018|10568|       [twitter, FB]|\n",
      "|  5|    Matei|Zaharia|https://tinyurl.5|5/14/2014|40578|[web, twitter, FB...|\n",
      "|  6|  Reynold|    Xin|https://tinyurl.6| 3/2/2015|25568| [twitter, LinkedIn]|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "blogs2_df = spark.createDataFrame(data, scm)\n",
    "blogs2_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Id', 'First', 'Last', 'Url', 'Published', 'Hits', 'Campaigns']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blogs2_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|double| Hits|\n",
      "+------+-----+\n",
      "|  9070| 4535|\n",
      "| 17816| 8908|\n",
      "| 15318| 7659|\n",
      "| 21136|10568|\n",
      "| 81156|40578|\n",
      "| 51136|25568|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "blogs2_df.selectExpr('Hits * 2 as double','Hits').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|(Hits * 2)|\n",
      "+----------+\n",
      "|      9070|\n",
      "|     17816|\n",
      "|     15318|\n",
      "|     21136|\n",
      "|     81156|\n",
      "|     51136|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "blogs2_df.select(F.col('Hits') * 2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-------+-----------------+---------+-----+--------------------+-----------+\n",
      "| Id|    First|   Last|              Url|Published| Hits|           Campaigns|Big Hitters|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+-----------+\n",
      "|  1|    Jules|  Damji|https://tinyurl.1| 1/4/2016| 4535| [twitter, LinkedIn]|      false|\n",
      "|  2|   Brooke|  Wenig|https://tinyurl.2| 5/5/2018| 8908| [twitter, LinkedIn]|      false|\n",
      "|  3|    Denny|    Lee|https://tinyurl.3| 6/7/2019| 7659|[web, twitter, FB...|      false|\n",
      "|  4|Tathagata|    Das|https://tinyurl.4|5/12/2018|10568|       [twitter, FB]|       true|\n",
      "|  5|    Matei|Zaharia|https://tinyurl.5|5/14/2014|40578|[web, twitter, FB...|       true|\n",
      "|  6|  Reynold|    Xin|https://tinyurl.6| 3/2/2015|25568| [twitter, LinkedIn]|       true|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "blogs2_df.withColumn('Big Hitters', (F.expr('Hits > 10000'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|    AuthorsId|\n",
      "+-------------+\n",
      "|  JulesDamji1|\n",
      "| BrookeWenig2|\n",
      "|    DennyLee3|\n",
      "|TathagataDas4|\n",
      "+-------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(blogs2_df\n",
    " .withColumn('AuthorsId', \n",
    "             (F.concat(F.expr('First'),\n",
    "                       F.expr('Last'), \n",
    "                       F.expr('Id'))))\n",
    " .select('AuthorsId')\n",
    " .show(n=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 ways to do the same thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|Hits|\n",
      "+----+\n",
      "|4535|\n",
      "|8908|\n",
      "+----+\n",
      "only showing top 2 rows\n",
      "\n",
      "+----+\n",
      "|Hits|\n",
      "+----+\n",
      "|4535|\n",
      "|8908|\n",
      "+----+\n",
      "only showing top 2 rows\n",
      "\n",
      "\"col\" is short for \"column\"\n",
      "+----+\n",
      "|Hits|\n",
      "+----+\n",
      "|4535|\n",
      "|8908|\n",
      "+----+\n",
      "only showing top 2 rows\n",
      "\n",
      "+----+\n",
      "|Hits|\n",
      "+----+\n",
      "|4535|\n",
      "|8908|\n",
      "+----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "blogs2_df.select('Hits').show(2)\n",
    "blogs2_df.select(F.expr('Hits')).show(2)\n",
    "print('\"col\" is short for \"column\"')\n",
    "blogs2_df.select(F.col('Hits')).show(2)\n",
    "blogs2_df.select(F.column('Hits')).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sort by `Id`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "| Id|    First|   Last|              Url|Published| Hits|           Campaigns|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "|  6|  Reynold|    Xin|https://tinyurl.6| 3/2/2015|25568| [twitter, LinkedIn]|\n",
      "|  5|    Matei|Zaharia|https://tinyurl.5|5/14/2014|40578|[web, twitter, FB...|\n",
      "|  4|Tathagata|    Das|https://tinyurl.4|5/12/2018|10568|       [twitter, FB]|\n",
      "|  3|    Denny|    Lee|https://tinyurl.3| 6/7/2019| 7659|[web, twitter, FB...|\n",
      "|  2|   Brooke|  Wenig|https://tinyurl.2| 5/5/2018| 8908| [twitter, LinkedIn]|\n",
      "|  1|    Jules|  Damji|https://tinyurl.1| 1/4/2016| 4535| [twitter, LinkedIn]|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "blogs2_df.sort(F.col('Id').desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm. `$` doesn't work to convert to a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-63-63099fbf8b4a>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-63-63099fbf8b4a>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    blogs_df.sort($'Id').show()\u001b[0m\n\u001b[0m                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "blogs_df.sort($'Id').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate a row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Python\n",
    "from pyspark.sql import Row\n",
    "\n",
    "blog_row = Row(6, \"Reynold\", \"Xin\", \"https://tinyurl.6\", 255568, \n",
    "               \"3/2/2015\", [\"twitter\", \"LinkedIn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Reynold'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# access using index for individual items\n",
    "\n",
    "blog_row[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Row objects can be used to create DataFrames if you need them for quick interactivity and exploration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Python \n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using DDL String to define a schema\n",
    "schema = \"`Author` STRING, `State` STRING\"\n",
    "rows = [Row(\"Matei Zaharia\", \"CA\"), Row(\"Reynold Xin\", \"CA\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|       Author|State|\n",
      "+-------------+-----+\n",
      "|Matei Zaharia|   CA|\n",
      "|  Reynold Xin|   CA|\n",
      "+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "authors_df = spark.createDataFrame(rows, schema)\n",
    "authors_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## drop a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|       Author|\n",
      "+-------------+\n",
      "|Matei Zaharia|\n",
      "|  Reynold Xin|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "authors_df.drop('State').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common DataFrame Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "people_file = '/Users/bartev/dev/spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/resources/people.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programmatic way to define a schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "people_schema = StructType([StructField('name', StringType(), True),\n",
    "                           StructField('age', IntegerType(), True),\n",
    "                           StructField('job', StringType(), True)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read the file using DataFrameReader using format csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "people_df = spark.read.csv(people_file, header=True, schema=people_schema, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+---------+\n",
      "| name|age|      job|\n",
      "+-----+---+---------+\n",
      "|Jorge| 30|Developer|\n",
      "|  Bob| 32|Developer|\n",
      "+-----+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "people_tbl = people_df.write.format('parquet').save('people.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(name,StringType,true),StructField(age,IntegerType,true),StructField(job,StringType,true)))"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.parquet('people.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_table = 'people_tbl'\n",
    "(people_df.write\n",
    "    .format('parquet')\n",
    "    .saveAsTable(parquet_table))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projections and filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "people_df = spark.read.csv(people_file, header=True, schema=people_schema, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|age|\n",
      "+---+\n",
      "| 32|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(people_df.select('age')\n",
    "     .where('age > 30')\n",
    "    .show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_fname = '/Users/bartev/dev/github-bv/san-tan/lrn-spark/Data-ML-100k--master/ml-100k/u.item'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df = spark.read.csv(movie_fname, header=False, sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-----------+--------------------+\n",
      "| id|               title|       date|                 url|\n",
      "+---+--------------------+-----------+--------------------+\n",
      "| 93|Welcome to the Do...|24-May-1996|http://us.imdb.co...|\n",
      "|103|All Dogs Go to He...|29-Mar-1996|http://us.imdb.co...|\n",
      "|104| Theodore Rex (1995)|29-Mar-1996|http://us.imdb.co...|\n",
      "|105|   Sgt. Bilko (1996)|29-Mar-1996|http://us.imdb.co...|\n",
      "|111|Truth About Cats ...|26-Apr-1996|http://us.imdb.co...|\n",
      "+---+--------------------+-----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Rename columns\n",
    "\n",
    "(movies_df\n",
    " .select('_c0', '_c1', '_c2', '_c4')\n",
    " .withColumnRenamed('_c0', 'id')\n",
    " .withColumnRenamed('_c1', 'title')\n",
    " .withColumnRenamed('_c2', 'date')\n",
    " .withColumnRenamed('_c4', 'url')\n",
    " .where('date > \"1996-01-01\"')\n",
    " .where('id > 30')\n",
    "#  .schema\n",
    " .show(5)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(movies_df\n",
    " .select('_c0', '_c1', '_c2', '_c4')\n",
    " .withColumnRenamed('_c0', 'id')\n",
    " .withColumnRenamed('_c1', 'title')\n",
    " .withColumnRenamed('_c2', 'date')\n",
    " .withColumnRenamed('_c4', 'url')\n",
    " .where('date > \"1996-01-01\"')\n",
    " .where('id > 30')\n",
    "#  .schema\n",
    " .select('date')\n",
    " .distinct()\n",
    " .count()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+\n",
      "|id |date       |\n",
      "+---+-----------+\n",
      "|31 |01-Jan-1995|\n",
      "|32 |01-Jan-1994|\n",
      "|33 |01-Jan-1995|\n",
      "|34 |01-Jan-1995|\n",
      "|35 |01-Jan-1995|\n",
      "|36 |01-Jan-1995|\n",
      "|37 |01-Jan-1994|\n",
      "|100|14-Feb-1997|\n",
      "|101|08-Mar-1981|\n",
      "|102|01-Jan-1970|\n",
      "+---+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# how do I convert column 'id' to an int?\n",
    "\n",
    "(movies_df\n",
    " .select('_c0', '_c1', '_c2', '_c4')\n",
    " .withColumnRenamed('_c0', 'id')\n",
    " .withColumnRenamed('_c1', 'title')\n",
    " .withColumnRenamed('_c2', 'date')\n",
    " .withColumnRenamed('_c4', 'url')\n",
    "#  .where('date > \"1996-01-01\"')\n",
    " .where('id > 30')\n",
    " .where(F.col('id') < \"38\")\n",
    "#  .schema\n",
    " .select('id', 'date')\n",
    "#  .distinct()\n",
    " .show(10, False)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df2 = (movies_df\n",
    " .select('_c0', '_c1', '_c2', '_c4')\n",
    " .withColumnRenamed('_c0', 'id')\n",
    " .withColumnRenamed('_c1', 'title')\n",
    " .withColumnRenamed('_c2', 'date')\n",
    " .withColumnRenamed('_c4', 'url')\n",
    " .where('id > 30')\n",
    " .select('id', 'date')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1652"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_df2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(id,StringType,true),StructField(date,StringType,true)))"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_df2.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+\n",
      "| id|       date|\n",
      "+---+-----------+\n",
      "| 31|01-Jan-1995|\n",
      "| 32|01-Jan-1994|\n",
      "| 33|01-Jan-1995|\n",
      "| 34|01-Jan-1995|\n",
      "| 35|01-Jan-1995|\n",
      "| 36|01-Jan-1995|\n",
      "| 37|01-Jan-1994|\n",
      "| 38|01-Jan-1995|\n",
      "| 39|01-Jan-1995|\n",
      "| 40|01-Jan-1995|\n",
      "+---+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movies_df2.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+-----------+\n",
      "|summary|               id|       date|\n",
      "+-------+-----------------+-----------+\n",
      "|  count|             1652|       1651|\n",
      "|   mean|            856.5|       null|\n",
      "| stddev|477.0356380816846|       null|\n",
      "|    min|              100|01-Aug-1997|\n",
      "|    max|              999| 4-Feb-1971|\n",
      "+-------+-----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movies_df2.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+----------+-------------------+\n",
      "| id|       date|  new_date|             new_ts|\n",
      "+---+-----------+----------+-------------------+\n",
      "| 50|01-Jan-1977|1977-01-01|1977-01-01 00:00:00|\n",
      "| 74|01-Jan-1965|1965-01-01|1965-01-01 00:00:00|\n",
      "| 89|01-Jan-1982|1982-01-01|1982-01-01 00:00:00|\n",
      "| 99|01-Jan-1937|1937-01-01|1937-01-01 00:00:00|\n",
      "|101|08-Mar-1981|1981-03-08|1981-03-08 00:00:00|\n",
      "|102|01-Jan-1970|1970-01-01|1970-01-01 00:00:00|\n",
      "|127|01-Jan-1972|1972-01-01|1972-01-01 00:00:00|\n",
      "|131|01-Jan-1961|1961-01-01|1961-01-01 00:00:00|\n",
      "|132|01-Jan-1939|1939-01-01|1939-01-01 00:00:00|\n",
      "|133|01-Jan-1939|1939-01-01|1939-01-01 00:00:00|\n",
      "|134|01-Jan-1941|1941-01-01|1941-01-01 00:00:00|\n",
      "|135|01-Jan-1968|1968-01-01|1968-01-01 00:00:00|\n",
      "|136|01-Jan-1939|1939-01-01|1939-01-01 00:00:00|\n",
      "|139|01-Jan-1969|1969-01-01|1969-01-01 00:00:00|\n",
      "|141|01-Jan-1954|1954-01-01|1954-01-01 00:00:00|\n",
      "|142|01-Jan-1971|1971-01-01|1971-01-01 00:00:00|\n",
      "|143|01-Jan-1965|1965-01-01|1965-01-01 00:00:00|\n",
      "|144|01-Jan-1988|1988-01-01|1988-01-01 00:00:00|\n",
      "|151|01-Jan-1971|1971-01-01|1971-01-01 00:00:00|\n",
      "|152|01-Jan-1973|1973-01-01|1973-01-01 00:00:00|\n",
      "+---+-----------+----------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(movies_df2\n",
    " .withColumn('new_date', F.to_date(F.col('date'), 'dd-MMM-yyyy'))\n",
    " .withColumn('new_ts', F.to_timestamp(F.col('date'), 'dd-MMM-yyyy'))\n",
    " .where(F.col('new_date') < '1990-01-01')\n",
    " .show())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### order by year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+----------+-------------------+----+-----+\n",
      "|  id|       date|  new_date|             new_ts|year|month|\n",
      "+----+-----------+----------+-------------------+----+-----+\n",
      "|1198|28-Jun-1960|1960-06-28|1960-06-28 00:00:00|1960|    6|\n",
      "|1149|20-Dec-1971|1971-12-20|1971-12-20 00:00:00|1971|   12|\n",
      "|1373| 4-Feb-1971|1971-02-04|1971-02-04 00:00:00|1971|    2|\n",
      "|1187|17-May-1975|1975-05-17|1975-05-17 00:00:00|1975|    5|\n",
      "|1214|08-Mar-1976|1976-03-08|1976-03-08 00:00:00|1976|    3|\n",
      "| 101|08-Mar-1981|1981-03-08|1981-03-08 00:00:00|1981|    3|\n",
      "|1635|26-Apr-1986|1986-04-26|1986-04-26 00:00:00|1986|    4|\n",
      "|1078|29-Mar-1988|1988-03-29|1988-03-29 00:00:00|1988|    3|\n",
      "+----+-----------+----------+-------------------+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(movies_df2\n",
    " .withColumn('new_date', F.to_date(F.col('date'), 'dd-MMM-yyyy'))\n",
    " .withColumn('new_ts', F.to_timestamp(F.col('date'), 'dd-MMM-yyyy'))\n",
    " .where(F.col('new_date') < '1990-01-01')\n",
    " .orderBy(F.year('new_date'))\n",
    " .withColumn('year', F.year('new_date'))\n",
    " .withColumn('month', F.month('new_date'))\n",
    " .where(F.col('month') != 1)\n",
    " .show())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## write to csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With `repartition`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "(movies_df2\n",
    "    .withColumn('year', F.year(F.to_date(F.col('date'), 'dd-MMM-yyyy')))\n",
    "     .select('year')\n",
    "     .distinct()\n",
    "     .orderBy('year')\n",
    "     .where('date != \"null\"')\n",
    "     .repartition(1)\n",
    "    .write\n",
    " .format('csv')\n",
    " .option('header', 'true')\n",
    " .save('movie_dates.csv')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### with `coalesce`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "(movies_df2\n",
    "    .withColumn('year', F.year(F.to_date(F.col('date'), 'dd-MMM-yyyy')))\n",
    "     .select('year')\n",
    "     .distinct()\n",
    "     .orderBy('year')\n",
    "     .where('date != \"null\"')\n",
    "     .coalesce(1)\n",
    "    .write\n",
    " .format('csv')\n",
    " .option('header', 'true')\n",
    " .save('movie_dates_coalesce.csv')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### with `pandas`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "(movies_df2\n",
    "    .withColumn('year', F.year(F.to_date(F.col('date'), 'dd-MMM-yyyy')))\n",
    "     .select('year')\n",
    "     .distinct()\n",
    "     .orderBy('year')\n",
    "     .where('date != \"null\"')\n",
    "    .toPandas()\n",
    " .to_csv('movie_dates_pandas.csv', header=True, index=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+--------------------+--------------------+\n",
      "| id|       date|               title|                 url|\n",
      "+---+-----------+--------------------+--------------------+\n",
      "| 31|01-Jan-1995| Crimson Tide (1995)|http://us.imdb.co...|\n",
      "| 32|01-Jan-1994|        Crumb (1994)|http://us.imdb.co...|\n",
      "| 33|01-Jan-1995|    Desperado (1995)|http://us.imdb.co...|\n",
      "| 34|01-Jan-1995|Doom Generation, ...|http://us.imdb.co...|\n",
      "| 35|01-Jan-1995|Free Willy 2: The...|http://us.imdb.co...|\n",
      "| 36|01-Jan-1995|     Mad Love (1995)|http://us.imdb.co...|\n",
      "| 37|01-Jan-1994|        Nadja (1994)|http://us.imdb.co...|\n",
      "| 38|01-Jan-1995|     Net, The (1995)|http://us.imdb.co...|\n",
      "| 39|01-Jan-1995| Strange Days (1995)|http://us.imdb.co...|\n",
      "| 40|01-Jan-1995|To Wong Foo, Than...|http://us.imdb.co...|\n",
      "| 41|01-Jan-1995|Billy Madison (1995)|http://us.imdb.co...|\n",
      "| 42|01-Jan-1994|       Clerks (1994)|http://us.imdb.co...|\n",
      "| 43|01-Jan-1994|   Disclosure (1994)|http://us.imdb.co...|\n",
      "| 44|01-Jan-1994|Dolores Claiborne...|http://us.imdb.co...|\n",
      "| 45|01-Jan-1994|Eat Drink Man Wom...|http://us.imdb.co...|\n",
      "| 46|01-Jan-1994|      Exotica (1994)|http://us.imdb.co...|\n",
      "| 47|01-Jan-1994|      Ed Wood (1994)|http://us.imdb.co...|\n",
      "| 48|01-Jan-1994|  Hoop Dreams (1994)|http://us.imdb.co...|\n",
      "| 49|01-Jan-1994|         I.Q. (1994)|http://us.imdb.co...|\n",
      "| 50|01-Jan-1977|    Star Wars (1977)|http://us.imdb.co...|\n",
      "+---+-----------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movies_df = (spark.read\n",
    "             .csv(movie_fname, header=False, sep='|')\n",
    "             .select('_c0', '_c1', '_c2', '_c4')\n",
    "             .withColumnRenamed('_c0', 'id')\n",
    "             .withColumnRenamed('_c1', 'title')\n",
    "             .withColumnRenamed('_c2', 'date')\n",
    "             .withColumnRenamed('_c4', 'url')\n",
    "             .where('id > 30')\n",
    "             .select('id', 'date', 'title', 'url')\n",
    ")\n",
    "movies_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+------------------+----------+----------+\n",
      "|sum(count)|        avg(count)|stddev_samp(count)|min(count)|max(count)|\n",
      "+----------+------------------+------------------+----------+----------+\n",
      "|      1651|23.253521126760564| 62.53769567454248|         1|       347|\n",
      "+----------+------------------+------------------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(movies_df\n",
    " .where(F.col('date').isNotNull())\n",
    " .withColumn('new_date', F.to_date('date', 'dd-MMM-yyyy'))\n",
    " .withColumn('year', F.year(F.to_date(F.col('date'), 'dd-MMM-yyyy')))\n",
    " .groupBy('year')\n",
    " .count()\n",
    " .orderBy('count', ascending=False)\n",
    " .select(F.sum('count'), F.avg('count'), F.stddev('count'), F.min('count'), F.max('count'))\n",
    " .show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sum(count): long (nullable = true)\n",
      " |-- avg(count): double (nullable = true)\n",
      " |-- stddev_samp(count): double (nullable = true)\n",
      " |-- min(count): long (nullable = true)\n",
      " |-- max(count): long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(movies_df\n",
    " .where(F.col('date').isNotNull())\n",
    " .withColumn('new_date', F.to_date('date', 'dd-MMM-yyyy'))\n",
    " .withColumn('year', F.year(F.to_date(F.col('date'), 'dd-MMM-yyyy')))\n",
    " .groupBy('year')\n",
    " .count()\n",
    " .orderBy('count', ascending=False)\n",
    " .select(F.sum('count'), F.avg('count'), F.stddev('count'), F.min('count'), F.max('count'))\n",
    " .printSchema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
