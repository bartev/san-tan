{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create an RDD of tuples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## setup `SparkContext`"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext('local', 'First App')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sc():\n",
    "    \"\"\"get a SparkContext (don't recreate)\"\"\"\n",
    "    import pyspark\n",
    "\n",
    "    # don't try to redefine 'sc'\n",
    "    if not globals().get('sc', False):\n",
    "        sc = pyspark.SparkContext('local', 'ch3 notebook')\n",
    "    else:\n",
    "        print('not redefining sc')\n",
    "        sc = globals()['sc']\n",
    "    return sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = get_sc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_names_ages = [('Brooke', 20),\n",
    "                   ('Denny', 31),\n",
    "                   ('Jules', 30),\n",
    "                   ('TD', 35),\n",
    "                   ('Brooke', 25)]\n",
    "\n",
    "dataRDD = sc.parallelize(data_names_ages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use map and reduceByKey transformations with their \n",
    "# lambda expressions to aggregate and then compute average\n",
    "\n",
    "agesRDD = (dataRDD.map(lambda x, y: (x, (y, 1)))\n",
    "           .reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    "           .map(lambda x, y, z: (x, y/z))\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|  name|avg(age)|\n",
      "+------+--------+\n",
      "|Brooke|    22.5|\n",
      "| Jules|    30.0|\n",
      "|    TD|    35.0|\n",
      "| Denny|    31.0|\n",
      "+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Same thing, but with a dataframe\n",
    "\n",
    "from pyspark.sql.functions import avg\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# create a SparkSession\n",
    "spark = (SparkSession\n",
    "    .builder\n",
    "    .appName(\"example\")\n",
    "    .getOrCreate())\n",
    "\n",
    "data_df = spark.createDataFrame(data_names_ages, ['name', 'age'])\n",
    "\n",
    "# group names, aggregate age, get avg\n",
    "avg_df = data_df.groupBy('name').agg(avg('age'))\n",
    "\n",
    "\n",
    "avg_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://172.20.10.2:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>ch3 notebook</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x107665c88>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create DataFrame with schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "see file `ex3-6-define-schema.py`\n",
    "\n",
    "run it using either\n",
    "\n",
    "```\n",
    "spark-submit ex3-6-define-schema.py\n",
    "```\n",
    "\n",
    "or \n",
    "\n",
    "```\n",
    "python ex3-6-define-schema.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Python \n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# define schema for our data using DDL \n",
    "schema = \"`Id` INT,`First` STRING,`Last` STRING,`Url` STRING,`Published` STRING,`Hits` INT,`Campaigns` ARRAY<STRING>\"\n",
    "# create our static data\n",
    "data = [\n",
    "    [1, \"Jules\", \"Damji\", \"https://tinyurl.1\", \"1/4/2016\", 4535, [\"twitter\", \"LinkedIn\"]],\n",
    "    [2, \"Brooke\",\"Wenig\",\"https://tinyurl.2\", \"5/5/2018\", 8908, [\"twitter\", \"LinkedIn\"]],\n",
    "    [3, \"Denny\", \"Lee\", \"https://tinyurl.3\",\"6/7/2019\",7659, [\"web\", \"twitter\", \"FB\", \"LinkedIn\"]],\n",
    "    [4, \"Tathagata\", \"Das\",\"https://tinyurl.4\", \"5/12/2018\", 10568, [\"twitter\", \"FB\"]],\n",
    "    [5, \"Matei\",\"Zaharia\", \"https://tinyurl.5\", \"5/14/2014\", 40578, [\"web\", \"twitter\", \"FB\", \"LinkedIn\"]],\n",
    "    [6, \"Reynold\", \"Xin\", \"https://tinyurl.6\", \"3/2/2015\", 25568, [\"twitter\", \"LinkedIn\"]]\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a SparkSession\n",
    "spark = (SparkSession\n",
    "    .builder\n",
    "    .appName(\"Example-3_6\")\n",
    "    .getOrCreate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning Spark, 2nd ed ch 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a DataFrame using the schema defined above\n",
    "blogs_df = spark.createDataFrame(data, schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(Id,IntegerType,true),StructField(First,StringType,true),StructField(Last,StringType,true),StructField(Url,StringType,true),StructField(Published,StringType,true),StructField(Hits,IntegerType,true),StructField(Campaigns,ArrayType(StringType,true),true)))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blogs_df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these types are defined in spark.sql.types\n",
    "scm = StructType(\n",
    "    [StructField('Id', IntegerType(), True),\n",
    "         StructField('First', StringType(), True),\n",
    "         StructField('Last', StringType(), True),\n",
    "         StructField('Url', StringType(), True),\n",
    "         StructField('Published', StringType(), True),\n",
    "         StructField('Hits', IntegerType(), True),\n",
    "         StructField('Campaigns', ArrayType(StringType(), True), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "| Id|    First|   Last|              Url|Published| Hits|           Campaigns|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "|  1|    Jules|  Damji|https://tinyurl.1| 1/4/2016| 4535| [twitter, LinkedIn]|\n",
      "|  2|   Brooke|  Wenig|https://tinyurl.2| 5/5/2018| 8908| [twitter, LinkedIn]|\n",
      "|  3|    Denny|    Lee|https://tinyurl.3| 6/7/2019| 7659|[web, twitter, FB...|\n",
      "|  4|Tathagata|    Das|https://tinyurl.4|5/12/2018|10568|       [twitter, FB]|\n",
      "|  5|    Matei|Zaharia|https://tinyurl.5|5/14/2014|40578|[web, twitter, FB...|\n",
      "|  6|  Reynold|    Xin|https://tinyurl.6| 3/2/2015|25568| [twitter, LinkedIn]|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "blogs2_df = spark.createDataFrame(data, scm)\n",
    "blogs2_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Id', 'First', 'Last', 'Url', 'Published', 'Hits', 'Campaigns']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blogs_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|double| Hits|\n",
      "+------+-----+\n",
      "|  9070| 4535|\n",
      "| 17816| 8908|\n",
      "| 15318| 7659|\n",
      "| 21136|10568|\n",
      "| 81156|40578|\n",
      "| 51136|25568|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "blogs_df.selectExpr('Hits * 2 as double','Hits').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+----------+\n",
      "|(Hits * 2)| Hits|(Hits - 1)|\n",
      "+----------+-----+----------+\n",
      "|      9070| 4535|      4534|\n",
      "|     17816| 8908|      8907|\n",
      "|     15318| 7659|      7658|\n",
      "|     21136|10568|     10567|\n",
      "|     81156|40578|     40577|\n",
      "|     51136|25568|     25567|\n",
      "+----------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "blogs2_df.select(F.col('Hits') * 2, 'Hits', F.col('Hits') - 1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-------+-----------------+---------+-----+--------------------+-----------+\n",
      "| Id|    First|   Last|              Url|Published| Hits|           Campaigns|Big Hitters|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+-----------+\n",
      "|  1|    Jules|  Damji|https://tinyurl.1| 1/4/2016| 4535| [twitter, LinkedIn]|      false|\n",
      "|  2|   Brooke|  Wenig|https://tinyurl.2| 5/5/2018| 8908| [twitter, LinkedIn]|      false|\n",
      "|  3|    Denny|    Lee|https://tinyurl.3| 6/7/2019| 7659|[web, twitter, FB...|      false|\n",
      "|  4|Tathagata|    Das|https://tinyurl.4|5/12/2018|10568|       [twitter, FB]|       true|\n",
      "|  5|    Matei|Zaharia|https://tinyurl.5|5/14/2014|40578|[web, twitter, FB...|       true|\n",
      "|  6|  Reynold|    Xin|https://tinyurl.6| 3/2/2015|25568| [twitter, LinkedIn]|       true|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "blogs2_df.withColumn('Big Hitters', (F.expr('Hits > 10000'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|    AuthorsId|\n",
      "+-------------+\n",
      "|  JulesDamji1|\n",
      "| BrookeWenig2|\n",
      "|    DennyLee3|\n",
      "|TathagataDas4|\n",
      "+-------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(blogs2_df\n",
    " .withColumn('AuthorsId', \n",
    "             (F.concat(F.expr('First'),\n",
    "                       F.expr('Last'), \n",
    "                       F.expr('Id'))))\n",
    " .select('AuthorsId')\n",
    " .show(n=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using `expr`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use `expr` to compute a value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|(Hits * 2)|\n",
      "+----------+\n",
      "|      9070|\n",
      "|     17816|\n",
      "+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "blogs_df.select(expr('Hits * 2')).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### or use `col` to compute value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|(Hits * 2)|\n",
      "+----------+\n",
      "|      9070|\n",
      "|     17816|\n",
      "+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "blogs_df.select(col('Hits') * 2).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add new columns using `withColumn` and `expr`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>First</th>\n",
       "      <th>Last</th>\n",
       "      <th>Url</th>\n",
       "      <th>Published</th>\n",
       "      <th>Hits</th>\n",
       "      <th>Campaigns</th>\n",
       "      <th>Big Hitters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Jules</td>\n",
       "      <td>Damji</td>\n",
       "      <td>https://tinyurl.1</td>\n",
       "      <td>1/4/2016</td>\n",
       "      <td>4535</td>\n",
       "      <td>[twitter, LinkedIn]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Brooke</td>\n",
       "      <td>Wenig</td>\n",
       "      <td>https://tinyurl.2</td>\n",
       "      <td>5/5/2018</td>\n",
       "      <td>8908</td>\n",
       "      <td>[twitter, LinkedIn]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Denny</td>\n",
       "      <td>Lee</td>\n",
       "      <td>https://tinyurl.3</td>\n",
       "      <td>6/7/2019</td>\n",
       "      <td>7659</td>\n",
       "      <td>[web, twitter, FB, LinkedIn]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Tathagata</td>\n",
       "      <td>Das</td>\n",
       "      <td>https://tinyurl.4</td>\n",
       "      <td>5/12/2018</td>\n",
       "      <td>10568</td>\n",
       "      <td>[twitter, FB]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Matei</td>\n",
       "      <td>Zaharia</td>\n",
       "      <td>https://tinyurl.5</td>\n",
       "      <td>5/14/2014</td>\n",
       "      <td>40578</td>\n",
       "      <td>[web, twitter, FB, LinkedIn]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>Reynold</td>\n",
       "      <td>Xin</td>\n",
       "      <td>https://tinyurl.6</td>\n",
       "      <td>3/2/2015</td>\n",
       "      <td>25568</td>\n",
       "      <td>[twitter, LinkedIn]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id      First     Last                Url  Published   Hits  \\\n",
       "0   1      Jules    Damji  https://tinyurl.1   1/4/2016   4535   \n",
       "1   2     Brooke    Wenig  https://tinyurl.2   5/5/2018   8908   \n",
       "2   3      Denny      Lee  https://tinyurl.3   6/7/2019   7659   \n",
       "3   4  Tathagata      Das  https://tinyurl.4  5/12/2018  10568   \n",
       "4   5      Matei  Zaharia  https://tinyurl.5  5/14/2014  40578   \n",
       "5   6    Reynold      Xin  https://tinyurl.6   3/2/2015  25568   \n",
       "\n",
       "                      Campaigns  Big Hitters  \n",
       "0           [twitter, LinkedIn]        False  \n",
       "1           [twitter, LinkedIn]        False  \n",
       "2  [web, twitter, FB, LinkedIn]        False  \n",
       "3                 [twitter, FB]         True  \n",
       "4  [web, twitter, FB, LinkedIn]         True  \n",
       "5           [twitter, LinkedIn]         True  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(blogs_df\n",
    " .withColumn('Big Hitters', expr('Hits > 10000'))\n",
    " .toPandas()\n",
    "#  .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use `expr` to concatenate columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|    AuthorsId|\n",
      "+-------------+\n",
      "|  JulesDamji1|\n",
      "| BrookeWenig2|\n",
      "|    DennyLee3|\n",
      "|TathagataDas4|\n",
      "|MateiZaharia5|\n",
      "|  ReynoldXin6|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import concat\n",
    "\n",
    "(blogs_df\n",
    " .withColumn('AuthorsId', \n",
    "             concat(expr('First'), expr('Last'), expr('Id')))\n",
    "#  .select(expr('AuthorsId'))\n",
    " .select('AuthorsId')\n",
    " .show() \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 ways to do the same thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "| Id|    First|   Last|              Url|Published| Hits|           Campaigns|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "|  1|    Jules|  Damji|https://tinyurl.1| 1/4/2016| 4535| [twitter, LinkedIn]|\n",
      "|  2|   Brooke|  Wenig|https://tinyurl.2| 5/5/2018| 8908| [twitter, LinkedIn]|\n",
      "|  3|    Denny|    Lee|https://tinyurl.3| 6/7/2019| 7659|[web, twitter, FB...|\n",
      "|  4|Tathagata|    Das|https://tinyurl.4|5/12/2018|10568|       [twitter, FB]|\n",
      "|  5|    Matei|Zaharia|https://tinyurl.5|5/14/2014|40578|[web, twitter, FB...|\n",
      "|  6|  Reynold|    Xin|https://tinyurl.6| 3/2/2015|25568| [twitter, LinkedIn]|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "blogs_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "| Id|    First|   Last|              Url|Published| Hits|           Campaigns|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "|  1|    Jules|  Damji|https://tinyurl.1| 1/4/2016| 4535| [twitter, LinkedIn]|\n",
      "|  2|   Brooke|  Wenig|https://tinyurl.2| 5/5/2018| 8908| [twitter, LinkedIn]|\n",
      "|  3|    Denny|    Lee|https://tinyurl.3| 6/7/2019| 7659|[web, twitter, FB...|\n",
      "|  4|Tathagata|    Das|https://tinyurl.4|5/12/2018|10568|       [twitter, FB]|\n",
      "|  5|    Matei|Zaharia|https://tinyurl.5|5/14/2014|40578|[web, twitter, FB...|\n",
      "|  6|  Reynold|    Xin|https://tinyurl.6| 3/2/2015|25568| [twitter, LinkedIn]|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "blogs2_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|Hits|\n",
      "+----+\n",
      "|4535|\n",
      "|8908|\n",
      "+----+\n",
      "only showing top 2 rows\n",
      "\n",
      "+----+\n",
      "|Hits|\n",
      "+----+\n",
      "|4535|\n",
      "|8908|\n",
      "+----+\n",
      "only showing top 2 rows\n",
      "\n",
      "\"col\" is short for \"column\"\n",
      "+----+\n",
      "|Hits|\n",
      "+----+\n",
      "|4535|\n",
      "|8908|\n",
      "+----+\n",
      "only showing top 2 rows\n",
      "\n",
      "+----+\n",
      "|Hits|\n",
      "+----+\n",
      "|4535|\n",
      "|8908|\n",
      "+----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "blogs_df.select('Hits').show(2)\n",
    "blogs_df.select(expr('Hits')).show(2)\n",
    "\n",
    "print('\"col\" is short for \"column\"')\n",
    "blogs_df.select(col('Hits')).show(2)\n",
    "blogs_df.select(column('Hits')).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sort by `Id`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "| Id|    First|   Last|              Url|Published| Hits|           Campaigns|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "|  6|  Reynold|    Xin|https://tinyurl.6| 3/2/2015|25568| [twitter, LinkedIn]|\n",
      "|  5|    Matei|Zaharia|https://tinyurl.5|5/14/2014|40578|[web, twitter, FB...|\n",
      "|  4|Tathagata|    Das|https://tinyurl.4|5/12/2018|10568|       [twitter, FB]|\n",
      "|  3|    Denny|    Lee|https://tinyurl.3| 6/7/2019| 7659|[web, twitter, FB...|\n",
      "|  2|   Brooke|  Wenig|https://tinyurl.2| 5/5/2018| 8908| [twitter, LinkedIn]|\n",
      "|  1|    Jules|  Damji|https://tinyurl.1| 1/4/2016| 4535| [twitter, LinkedIn]|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "blogs_df.sort(col('Id').desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-------+-----------------+---------+-----+--------------------+--------+\n",
      "| Id|    First|   Last|              Url|Published| Hits|           Campaigns|     new|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+--------+\n",
      "|  1|    Jules|  Damji|https://tinyurl.1| 1/4/2016| 4535| [twitter, LinkedIn]|   Jules|\n",
      "|  2|   Brooke|  Wenig|https://tinyurl.2| 5/5/2018| 8908| [twitter, LinkedIn]|********|\n",
      "|  3|    Denny|    Lee|https://tinyurl.3| 6/7/2019| 7659|[web, twitter, FB...|   Denny|\n",
      "|  4|Tathagata|    Das|https://tinyurl.4|5/12/2018|10568|       [twitter, FB]|********|\n",
      "|  5|    Matei|Zaharia|https://tinyurl.5|5/14/2014|40578|[web, twitter, FB...|   Matei|\n",
      "|  6|  Reynold|    Xin|https://tinyurl.6| 3/2/2015|25568| [twitter, LinkedIn]|********|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(blogs_df\n",
    "    .withColumn('new',\n",
    "               when(blogs_df['Id'].isin([1,3,5]),\n",
    "                    col('First'))\n",
    "               .otherwise('*' * 8))\n",
    ".show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: Ch 3 says that `$` is a spark function that converts something to a column. However, it doesn't work here"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "blogs_df.sort($'Id'.desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "| Id|    First|   Last|              Url|Published| Hits|           Campaigns|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "|  5|    Matei|Zaharia|https://tinyurl.5|5/14/2014|40578|[web, twitter, FB...|\n",
      "|  4|Tathagata|    Das|https://tinyurl.4|5/12/2018|10568|       [twitter, FB]|\n",
      "|  3|    Denny|    Lee|https://tinyurl.3| 6/7/2019| 7659|[web, twitter, FB...|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(blogs2_df\n",
    " .filter('`Id` < 6')\n",
    " .filter('Id > 2')\n",
    " .sort(col('Id').desc())\n",
    " .show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `where` and `filter` are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-------+-----------------+---------+-----+--------------------+\n",
      "| Id|First|   Last|              Url|Published| Hits|           Campaigns|\n",
      "+---+-----+-------+-----------------+---------+-----+--------------------+\n",
      "|  5|Matei|Zaharia|https://tinyurl.5|5/14/2014|40578|[web, twitter, FB...|\n",
      "+---+-----+-------+-----------------+---------+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use plain SQL in `filter`\n",
    "(blogs2_df\n",
    " .filter('`Id` < 6')\n",
    " .filter('Id > 2')\n",
    " .filter(\"Last like 'Z%'\")\n",
    " .sort(col('Id').desc())\n",
    " .show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-------+-----------------+---------+-----+--------------------+\n",
      "| Id|First|   Last|              Url|Published| Hits|           Campaigns|\n",
      "+---+-----+-------+-----------------+---------+-----+--------------------+\n",
      "|  5|Matei|Zaharia|https://tinyurl.5|5/14/2014|40578|[web, twitter, FB...|\n",
      "+---+-----+-------+-----------------+---------+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use plain SQL in `filter`\n",
    "(blogs2_df\n",
    " .filter('`Id` < 6')\n",
    " .filter('Id > 2')\n",
    " .where(\"Last like 'Z%'\")\n",
    " .sort(col('Id').desc())\n",
    " .show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate a row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Python\n",
    "from pyspark.sql import Row\n",
    "\n",
    "blog_row = Row(6, \"Reynold\", \"Xin\", \"https://tinyurl.6\", 255568, \n",
    "               \"3/2/2015\", [\"twitter\", \"LinkedIn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index 0: 6\n",
      "index 1: Reynold\n",
      "\n",
      "6\n",
      "Reynold\n",
      "Xin\n",
      "https://tinyurl.6\n",
      "255568\n",
      "3/2/2015\n",
      "['twitter', 'LinkedIn']\n"
     ]
    }
   ],
   "source": [
    "# access using index for individual items\n",
    "print(f\"index 0: {blog_row[0]}\")\n",
    "print(f\"index 1: {blog_row[1]}\")\n",
    "\n",
    "print()\n",
    "\n",
    "for item in blog_row:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(blog_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Row objects can be used to create DataFrames if you need them for quick interactivity and exploration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Python \n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using DDL String to define a schema\n",
    "schema = \"`Author` STRING, `State` STRING\"\n",
    "rows = [Row(\"Matei Zaharia\", \"CA\"), Row(\"Reynold Xin\", \"CA\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|       Author|State|\n",
      "+-------------+-----+\n",
      "|Matei Zaharia|   CA|\n",
      "|  Reynold Xin|   CA|\n",
      "+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "authors_df = spark.createDataFrame(rows, schema)\n",
    "authors_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|       author|state|\n",
      "+-------------+-----+\n",
      "|Matei Zaharia|   CA|\n",
      "|  Reynold Xin|   CA|\n",
      "+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema2 = \"\"\"\n",
    "    author string, \n",
    "    state string\n",
    "    \"\"\"\n",
    "spark.createDataFrame(rows, schema2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|       author|state|\n",
      "+-------------+-----+\n",
      "|Matei Zaharia|   CA|\n",
      "|  Reynold Xin|   CA|\n",
      "+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.createDataFrame(rows, ['author', 'state']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## drop a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_df.drop('State').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common DataFrame Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_dir = '/Users/bartev/dev/spark-3.0.0-preview2-bin-hadoop2.7'\n",
    "people_file_relative = 'examples/src/main/resources/people.csv'\n",
    "people_file = os.path.join(spark_dir, people_file_relative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/bartev/dev/spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/resources/people.csv'"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programmatic way to define a schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "people_schema = StructType(\n",
    "    [StructField('name', StringType(), True),\n",
    "     StructField('age', IntegerType(), True),\n",
    "     StructField('job', StringType(), True)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## infer schema from a smaller sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|      name;age;job|\n",
      "+------------------+\n",
      "|Jorge;30;Developer|\n",
      "|  Bob;32;Developer|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(spark\n",
    " .read\n",
    " .option('samplingRatio', 0.5)\n",
    " .option('header', 'true')\n",
    " .csv(people_file)\n",
    " .show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read the file using DataFrameReader using format csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "people_df = spark.read.csv(people_file, \n",
    "                           header=True, \n",
    "                           schema=people_schema, \n",
    "                           sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can separate out options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+---------+\n",
      "| name|age|      job|\n",
      "+-----+---+---------+\n",
      "|Jorge| 30|Developer|\n",
      "|  Bob| 32|Developer|\n",
      "+-----+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(spark\n",
    " .read\n",
    " .option('header', 'true')\n",
    " .option('schema', people_schema)\n",
    " .option('sep', ';')\n",
    " .csv(people_file)\n",
    " .show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### option `True` or `\"true\"` - both ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+---------+\n",
      "| name|age|      job|\n",
      "+-----+---+---------+\n",
      "|Jorge| 30|Developer|\n",
      "|  Bob| 32|Developer|\n",
      "+-----+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(spark\n",
    " .read\n",
    " .option('header', True)\n",
    " .option('schema', people_schema)\n",
    " .option('sep', ';')\n",
    " .csv(people_file)\n",
    " .show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+---------+\n",
      "| name|age|      job|\n",
      "+-----+---+---------+\n",
      "|Jorge| 30|Developer|\n",
      "|  Bob| 32|Developer|\n",
      "+-----+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `DataFrameWriter`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parquet\n",
    "\n",
    "* default format\n",
    "* uses compression\n",
    "* preserves schema as part of metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "people_tbl = (people_df\n",
    "              .write\n",
    "              .format('parquet')\n",
    "              .mode('overwrite')\n",
    "              .save('people.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+---------+\n",
      "| name|age|      job|\n",
      "+-----+---+---------+\n",
      "|Jorge| 30|Developer|\n",
      "|  Bob| 32|Developer|\n",
      "+-----+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.parquet('people.parquet').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_table = 'people_tbl'\n",
    "(people_df.write\n",
    "    .format('parquet')\n",
    "    .mode('overwrite')\n",
    "    .saveAsTable(parquet_table))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projections and filters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* projection\n",
    "    * `select()`\n",
    "* filter\n",
    "    * `filter()`\n",
    "    * `where()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "people_df = spark.read.csv(people_file, header=True, \n",
    "                           schema=people_schema, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+---------+\n",
      "| name|age|      job|\n",
      "+-----+---+---------+\n",
      "|Jorge| 30|Developer|\n",
      "|  Bob| 32|Developer|\n",
      "+-----+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sql like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "|age|name|\n",
      "+---+----+\n",
      "| 32| Bob|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(people_df\n",
    " .select('age', 'name')\n",
    " .where('age != 30')\n",
    " .show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use `col`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "|age|name|\n",
      "+---+----+\n",
      "| 32| Bob|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(people_df\n",
    " .select('age', 'name')\n",
    " .where(col('age') > 30)\n",
    " .show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use an `expr`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "|age|name|\n",
      "+---+----+\n",
      "| 32| Bob|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(people_df\n",
    " .select('age', 'name')\n",
    " .where(expr('age > 30'))\n",
    " .show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_fname = '/Users/bartev/dev/github-bv/san-tan/lrn-spark/Data-ML-100k--master/ml-100k/u.item'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df = spark.read.csv(movie_fname, header=False, sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------+-----------+----+--------------------+---+---+---+---+---+----+----+----+----+----+----+----+----+----+----+----+----+----+----+\n",
      "|_c0|              _c1|        _c2| _c3|                 _c4|_c5|_c6|_c7|_c8|_c9|_c10|_c11|_c12|_c13|_c14|_c15|_c16|_c17|_c18|_c19|_c20|_c21|_c22|_c23|\n",
      "+---+-----------------+-----------+----+--------------------+---+---+---+---+---+----+----+----+----+----+----+----+----+----+----+----+----+----+----+\n",
      "|  1| Toy Story (1995)|01-Jan-1995|null|http://us.imdb.co...|  0|  0|  0|  1|  1|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|\n",
      "|  2| GoldenEye (1995)|01-Jan-1995|null|http://us.imdb.co...|  0|  1|  1|  0|  0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   1|   0|   0|\n",
      "|  3|Four Rooms (1995)|01-Jan-1995|null|http://us.imdb.co...|  0|  0|  0|  0|  0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   1|   0|   0|\n",
      "+---+-----------------+-----------+----+--------------------+---+---+---+---+---+----+----+----+----+----+----+----+----+----+----+----+----+----+----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movies_df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rename columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------+-----------+--------------------+\n",
      "| id|            title|       date|                 url|\n",
      "+---+-----------------+-----------+--------------------+\n",
      "|  1| Toy Story (1995)|01-Jan-1995|http://us.imdb.co...|\n",
      "|  2| GoldenEye (1995)|01-Jan-1995|http://us.imdb.co...|\n",
      "|  3|Four Rooms (1995)|01-Jan-1995|http://us.imdb.co...|\n",
      "|  4|Get Shorty (1995)|01-Jan-1995|http://us.imdb.co...|\n",
      "|  5|   Copycat (1995)|01-Jan-1995|http://us.imdb.co...|\n",
      "+---+-----------------+-----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movies = (movies_df\n",
    " .select('_c0', '_c1', '_c2', '_c4')\n",
    " .withColumnRenamed('_c0', 'id')\n",
    " .withColumnRenamed('_c1', 'title')\n",
    " .withColumnRenamed('_c2', 'date')\n",
    " .withColumnRenamed('_c4', 'url')\n",
    ")\n",
    "movies.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(id,StringType,true),StructField(title,StringType,true),StructField(date,StringType,true),StructField(url,StringType,true)))"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-----------+--------------------+\n",
      "| id|               title|       date|                 url|\n",
      "+---+--------------------+-----------+--------------------+\n",
      "| 93|Welcome to the Do...|24-May-1996|http://us.imdb.co...|\n",
      "|103|All Dogs Go to He...|29-Mar-1996|http://us.imdb.co...|\n",
      "|104| Theodore Rex (1995)|29-Mar-1996|http://us.imdb.co...|\n",
      "|105|   Sgt. Bilko (1996)|29-Mar-1996|http://us.imdb.co...|\n",
      "|111|Truth About Cats ...|26-Apr-1996|http://us.imdb.co...|\n",
      "+---+--------------------+-----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Rename columns\n",
    "\n",
    "(movies\n",
    " .where('date > \"1996-01-01\"')\n",
    " .where('id > 30')\n",
    " .show(5)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(movies\n",
    " .where('date > \"1996-01-01\"')\n",
    " .where('id > 30')\n",
    "#  .schema\n",
    " .select('date')\n",
    " .distinct()\n",
    " .count()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how do I convert column 'id' to an int?\n",
    "\n",
    "(movies_df\n",
    " .select('_c0', '_c1', '_c2', '_c4')\n",
    " .withColumnRenamed('_c0', 'id')\n",
    " .withColumnRenamed('_c1', 'title')\n",
    " .withColumnRenamed('_c2', 'date')\n",
    " .withColumnRenamed('_c4', 'url')\n",
    "#  .where('date > \"1996-01-01\"')\n",
    " .where('id > 30')\n",
    " .where(F.col('id') < \"38\")\n",
    "#  .schema\n",
    " .select('id', 'date')\n",
    "#  .distinct()\n",
    " .show(10, False)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df2 = (movies\n",
    " .where('id > 30')\n",
    " .select('id', 'date')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1652"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_df2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(id,StringType,true),StructField(date,StringType,true)))"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_df2.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+\n",
      "| id|       date|\n",
      "+---+-----------+\n",
      "| 31|01-Jan-1995|\n",
      "| 32|01-Jan-1994|\n",
      "| 33|01-Jan-1995|\n",
      "| 34|01-Jan-1995|\n",
      "| 35|01-Jan-1995|\n",
      "| 36|01-Jan-1995|\n",
      "| 37|01-Jan-1994|\n",
      "| 38|01-Jan-1995|\n",
      "| 39|01-Jan-1995|\n",
      "| 40|01-Jan-1995|\n",
      "+---+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movies_df2.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+-----------+\n",
      "|summary|               id|       date|\n",
      "+-------+-----------------+-----------+\n",
      "|  count|             1652|       1651|\n",
      "|   mean|            856.5|       null|\n",
      "| stddev|477.0356380816846|       null|\n",
      "|    min|              100|01-Aug-1997|\n",
      "|    max|              999| 4-Feb-1971|\n",
      "+-------+-----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movies_df2.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "aside: simple example from help docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(t='2020-06-13 12:52')]\n",
      "[Row(t='2020-06-13 12:52', date=datetime.date(2020, 6, 13))]\n",
      "[Row(t='2020-06-13 12:52', date=datetime.date(2020, 6, 13))]\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([('2020-06-13 12:52', )], ['t'])\n",
    "\n",
    "print(df.collect())\n",
    "\n",
    "print(df.select('t', to_date(df.t).alias('date')).collect())\n",
    "\n",
    "print(df.select('t', to_date(df.t, 'yyyy-MM-dd').alias('date')).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### convert to date, and rename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(other=datetime.date(2020, 6, 13))]"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(to_date(col('t')).alias('other')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(t,StringType,true),StructField(first,StringType,true),StructField(other,DateType,true)))"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df.select('t', \n",
    "          col('t').alias('first'),\n",
    "          to_date(col('t')).alias('other'))\n",
    "#  .collect()\n",
    "#  .show()\n",
    " .schema\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'current_date()'>"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.current_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- new_date: date (nullable = true)\n",
      " |-- new_ts: string (nullable = true)\n",
      " |-- cur_date: date (nullable = false)\n",
      " |-- cur_ts: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_date, to_timestamp, col, date_format\n",
    "from pyspark.sql.functions import current_date, current_timestamp\n",
    "\n",
    "(movies_df2\n",
    " .withColumn('new_date', to_date(col('date'), 'dd-MMM-yyyy'))\n",
    " .withColumn('new_ts', to_timestamp(col('date'), 'dd-MMM-yyyy'))\n",
    " .withColumn('new_ts', date_format(col('new_ts'), 'yyyy.MMM.dd E'))\n",
    "\n",
    " .withColumn('cur_date', current_date())\n",
    " .withColumn('cur_ts', date_format(current_timestamp(), 'MM-yyyy'))\n",
    " .where(col('new_date') < '1990-01-01')\n",
    "#  .show()\n",
    " .printSchema()\n",
    "#  .schema\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### order by year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+-----------+--------------------+----------+-------------------+----+-----+\n",
      "|  id|               title|       date|                 url|  new_date|             new_ts|year|month|\n",
      "+----+--------------------+-----------+--------------------+----------+-------------------+----+-----+\n",
      "|1198|  Purple Noon (1960)|28-Jun-1960|http://us.imdb.co...|1960-06-28|1960-06-28 00:00:00|1960|    6|\n",
      "|1149|    Walkabout (1971)|20-Dec-1971|http://us.imdb.co...|1971-12-20|1971-12-20 00:00:00|1971|   12|\n",
      "|1187|Switchblade Siste...|17-May-1975|http://us.imdb.co...|1975-05-17|1975-05-17 00:00:00|1975|    5|\n",
      "+----+--------------------+-----------+--------------------+----------+-------------------+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(movies\n",
    " .withColumn('new_date', F.to_date(F.col('date'), 'dd-MMM-yyyy'))\n",
    " .withColumn('new_ts', F.to_timestamp(F.col('date'), 'dd-MMM-yyyy'))\n",
    " .where(F.col('new_date') < '1990-01-01')\n",
    " .orderBy(F.year('new_date'))\n",
    " .withColumn('year', F.year('new_date'))\n",
    " .withColumn('month', F.month('new_date'))\n",
    " .where(F.col('month') != 1)\n",
    " .filter('month > 4')\n",
    " .show())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## write to csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With `repartition`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "(movies\n",
    " .withColumn('year', F.year(F.to_date(F.col('date'), 'dd-MMM-yyyy')))\n",
    " .select('year')\n",
    " .distinct()\n",
    " .orderBy('year')\n",
    " .where('date != \"null\"')\n",
    " .repartition(1)\n",
    " .write\n",
    " .format('csv')\n",
    " .option('header', True)\n",
    " .mode('overwrite')\n",
    " .save('movie_dates.csv')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### with `coalesce`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "(movies\n",
    " .withColumn('year', F.year(F.to_date(F.col('date'), 'dd-MMM-yyyy')))\n",
    " .select('year')\n",
    " .distinct()\n",
    " .orderBy('year')\n",
    " .where('date != \"null\"')\n",
    " .coalesce(1)\n",
    " .write\n",
    " .format('csv')\n",
    " .option('header', 'true')\n",
    " .mode('overwrite')\n",
    " .save('movie_dates_coalesce.csv')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### with `pandas`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "(movies\n",
    " .withColumn('year', F.year(F.to_date(F.col('date'), 'dd-MMM-yyyy')))\n",
    " .select('year', 'date')\n",
    " .distinct()\n",
    " .orderBy('year')\n",
    " .where('date != \"null\"')\n",
    " .toPandas()\n",
    " .to_csv('movie_dates_pandas.csv', header=True, index=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-----------+--------------------+\n",
      "| id|               title|       date|                 url|\n",
      "+---+--------------------+-----------+--------------------+\n",
      "|  1|    Toy Story (1995)|01-Jan-1995|http://us.imdb.co...|\n",
      "|  2|    GoldenEye (1995)|01-Jan-1995|http://us.imdb.co...|\n",
      "|  3|   Four Rooms (1995)|01-Jan-1995|http://us.imdb.co...|\n",
      "|  4|   Get Shorty (1995)|01-Jan-1995|http://us.imdb.co...|\n",
      "|  5|      Copycat (1995)|01-Jan-1995|http://us.imdb.co...|\n",
      "|  6|Shanghai Triad (Y...|01-Jan-1995|http://us.imdb.co...|\n",
      "|  7|Twelve Monkeys (1...|01-Jan-1995|http://us.imdb.co...|\n",
      "|  8|         Babe (1995)|01-Jan-1995|http://us.imdb.co...|\n",
      "|  9|Dead Man Walking ...|01-Jan-1995|http://us.imdb.co...|\n",
      "| 10|  Richard III (1995)|22-Jan-1996|http://us.imdb.co...|\n",
      "| 11|Seven (Se7en) (1995)|01-Jan-1995|http://us.imdb.co...|\n",
      "| 12|Usual Suspects, T...|14-Aug-1995|http://us.imdb.co...|\n",
      "| 13|Mighty Aphrodite ...|30-Oct-1995|http://us.imdb.co...|\n",
      "| 14|  Postino, Il (1994)|01-Jan-1994|http://us.imdb.co...|\n",
      "| 15|Mr. Holland's Opu...|29-Jan-1996|http://us.imdb.co...|\n",
      "| 16|French Twist (Gaz...|01-Jan-1995|http://us.imdb.co...|\n",
      "| 17|From Dusk Till Da...|05-Feb-1996|http://us.imdb.co...|\n",
      "| 18|White Balloon, Th...|01-Jan-1995|http://us.imdb.co...|\n",
      "| 19|Antonia's Line (1...|01-Jan-1995|http://us.imdb.co...|\n",
      "| 20|Angels and Insect...|01-Jan-1995|http://us.imdb.co...|\n",
      "+---+--------------------+-----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movies.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1652 rows\n",
      "+---+-----------+--------------------+--------------------+\n",
      "| id|       date|               title|                 url|\n",
      "+---+-----------+--------------------+--------------------+\n",
      "| 31|01-Jan-1995| Crimson Tide (1995)|http://us.imdb.co...|\n",
      "| 32|01-Jan-1994|        Crumb (1994)|http://us.imdb.co...|\n",
      "| 33|01-Jan-1995|    Desperado (1995)|http://us.imdb.co...|\n",
      "| 34|01-Jan-1995|Doom Generation, ...|http://us.imdb.co...|\n",
      "| 35|01-Jan-1995|Free Willy 2: The...|http://us.imdb.co...|\n",
      "| 36|01-Jan-1995|     Mad Love (1995)|http://us.imdb.co...|\n",
      "| 37|01-Jan-1994|        Nadja (1994)|http://us.imdb.co...|\n",
      "| 38|01-Jan-1995|     Net, The (1995)|http://us.imdb.co...|\n",
      "| 39|01-Jan-1995| Strange Days (1995)|http://us.imdb.co...|\n",
      "| 40|01-Jan-1995|To Wong Foo, Than...|http://us.imdb.co...|\n",
      "| 41|01-Jan-1995|Billy Madison (1995)|http://us.imdb.co...|\n",
      "| 42|01-Jan-1994|       Clerks (1994)|http://us.imdb.co...|\n",
      "| 43|01-Jan-1994|   Disclosure (1994)|http://us.imdb.co...|\n",
      "| 44|01-Jan-1994|Dolores Claiborne...|http://us.imdb.co...|\n",
      "| 45|01-Jan-1994|Eat Drink Man Wom...|http://us.imdb.co...|\n",
      "| 46|01-Jan-1994|      Exotica (1994)|http://us.imdb.co...|\n",
      "| 47|01-Jan-1994|      Ed Wood (1994)|http://us.imdb.co...|\n",
      "| 48|01-Jan-1994|  Hoop Dreams (1994)|http://us.imdb.co...|\n",
      "| 49|01-Jan-1994|         I.Q. (1994)|http://us.imdb.co...|\n",
      "| 50|01-Jan-1977|    Star Wars (1977)|http://us.imdb.co...|\n",
      "+---+-----------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movies_df = (spark.read\n",
    "             .csv(movie_fname, header=False, sep='|')\n",
    "             .select('_c0', '_c1', '_c2', '_c4')\n",
    "             .withColumnRenamed('_c0', 'id')\n",
    "             .withColumnRenamed('_c1', 'title')\n",
    "             .withColumnRenamed('_c2', 'date')\n",
    "             .withColumnRenamed('_c4', 'url')\n",
    "             .where('id > 30')\n",
    "             .select('id', 'date', 'title', 'url')\n",
    ")\n",
    "\n",
    "print(f'{movies_df.count()} rows')\n",
    "\n",
    "movies_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31</td>\n",
       "      <td>01-Jan-1995</td>\n",
       "      <td>Crimson Tide (1995)</td>\n",
       "      <td>http://us.imdb.com/M/title-exact?Crimson%20Tid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32</td>\n",
       "      <td>01-Jan-1994</td>\n",
       "      <td>Crumb (1994)</td>\n",
       "      <td>http://us.imdb.com/M/title-exact?Crumb%20(1994)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33</td>\n",
       "      <td>01-Jan-1995</td>\n",
       "      <td>Desperado (1995)</td>\n",
       "      <td>http://us.imdb.com/M/title-exact?Desperado%20(...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id         date                title  \\\n",
       "0  31  01-Jan-1995  Crimson Tide (1995)   \n",
       "1  32  01-Jan-1994         Crumb (1994)   \n",
       "2  33  01-Jan-1995     Desperado (1995)   \n",
       "\n",
       "                                                 url  \n",
       "0  http://us.imdb.com/M/title-exact?Crimson%20Tid...  \n",
       "1    http://us.imdb.com/M/title-exact?Crumb%20(1994)  \n",
       "2  http://us.imdb.com/M/title-exact?Desperado%20(...  "
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_df.toPandas().head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### String replacement using `regexp_repl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+--------------------+--------------------+--------------------+\n",
      "| id|       date|               title|                 url|                tit2|\n",
      "+---+-----------+--------------------+--------------------+--------------------+\n",
      "| 31|01-Jan-1995| Crimson Tide (1995)|http://us.imdb.co...| Crimson Tide (1995)|\n",
      "| 32|01-Jan-1994|        Crumb (1994)|http://us.imdb.co...|        Crumb (1994)|\n",
      "| 33|01-Jan-1995|    Desperado (1995)|http://us.imdb.co...|    Desperado (1995)|\n",
      "| 34|01-Jan-1995|Doom Generation, ...|http://us.imdb.co...|Doom Generation, ...|\n",
      "| 35|01-Jan-1995|Free Willy 2: The...|http://us.imdb.co...|Free Willy 2: The...|\n",
      "| 36|01-Jan-1995|     Mad Love (1995)|http://us.imdb.co...|     Mad Love (1995)|\n",
      "| 37|01-Jan-1994|        Nadja (1994)|http://us.imdb.co...|        Nadja (1994)|\n",
      "| 38|01-Jan-1995|     Net, The (1995)|http://us.imdb.co...|     Net, The (1995)|\n",
      "| 39|01-Jan-1995| Strange Days (1995)|http://us.imdb.co...| Strange Days (1995)|\n",
      "| 40|01-Jan-1995|To Wong Foo, Than...|http://us.imdb.co...|To Wong Foo, Than...|\n",
      "| 41|01-Jan-1995|Billy Madison (1995)|http://us.imdb.co...|Billy Madison (1995)|\n",
      "| 42|01-Jan-1994|       Clerks (1994)|http://us.imdb.co...|       Clerks (1994)|\n",
      "| 43|01-Jan-1994|   Disclosure (1994)|http://us.imdb.co...|   Disclosure (1994)|\n",
      "| 44|01-Jan-1994|Dolores Claiborne...|http://us.imdb.co...|Dolores Claiborne...|\n",
      "| 45|01-Jan-1994|Eat Drink Man Wom...|http://us.imdb.co...|Yinshi Nan Nu (1994)|\n",
      "| 46|01-Jan-1994|      Exotica (1994)|http://us.imdb.co...|      Exotica (1994)|\n",
      "| 47|01-Jan-1994|      Ed Wood (1994)|http://us.imdb.co...|      Ed Wood (1994)|\n",
      "| 48|01-Jan-1994|  Hoop Dreams (1994)|http://us.imdb.co...|  Hoop Dreams (1994)|\n",
      "| 49|01-Jan-1994|         I.Q. (1994)|http://us.imdb.co...|         I.Q. (1994)|\n",
      "| 50|01-Jan-1977|    Star Wars (1977)|http://us.imdb.co...|    Star Wars (1977)|\n",
      "+---+-----------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split, regexp_replace\n",
    "\n",
    "(movies_df\n",
    " .withColumn('tit2', split('url', 'title-exact\\?')[1])\n",
    " .withColumn('tit2', regexp_replace('tit2', '%20', ' '))\n",
    " .show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|yr  |count|\n",
      "+----+-----+\n",
      "|1996|347  |\n",
      "|1997|286  |\n",
      "|1994|213  |\n",
      "|1995|199  |\n",
      "|1993|126  |\n",
      "|1998|65   |\n",
      "|1992|37   |\n",
      "|1990|24   |\n",
      "|1991|22   |\n",
      "|1989|15   |\n",
      "|1986|15   |\n",
      "|1987|13   |\n",
      "|1982|13   |\n",
      "|1981|12   |\n",
      "|1988|11   |\n",
      "|1979|9    |\n",
      "|1958|9    |\n",
      "|1984|8    |\n",
      "|1940|8    |\n",
      "|1957|8    |\n",
      "+----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(movies_df\n",
    " .where(col('date').isNotNull())\n",
    " .select('date')\n",
    " .withColumn('dt', to_date('date', 'dd-MMM-yyyy'))\n",
    " .withColumn('yr', year(to_date(col('date'), 'dd-MMM-yyyy')))\n",
    " .groupBy('yr')\n",
    " .count()\n",
    " .orderBy('count', ascending=False)\n",
    " .show(truncate=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|  yr|count|\n",
      "+----+-----+\n",
      "|1996|  347|\n",
      "|1997|  286|\n",
      "|1994|  213|\n",
      "|1995|  199|\n",
      "|1993|  126|\n",
      "|1998|   65|\n",
      "|1992|   37|\n",
      "|1990|   24|\n",
      "|1991|   22|\n",
      "|1989|   15|\n",
      "|1986|   15|\n",
      "|1987|   13|\n",
      "|1982|   13|\n",
      "|1981|   12|\n",
      "|1988|   11|\n",
      "|1979|    9|\n",
      "|1958|    9|\n",
      "|1984|    8|\n",
      "|1940|    8|\n",
      "|1957|    8|\n",
      "+----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tmp = (movies_df\n",
    " .where(col('date').isNotNull())\n",
    " .select('date')\n",
    " .withColumn('dt', to_date('date', 'dd-MMM-yyyy'))\n",
    " .withColumn('yr', year(to_date(col('date'), 'dd-MMM-yyyy')))\n",
    " .groupBy('yr')\n",
    " .count()\n",
    " .orderBy('count', ascending=False)\n",
    "#  .select(F.sum('count'), F.avg('count'), F.stddev('count'), F.min('count'), F.max('count'))\n",
    "#  .show(truncate=False)\n",
    ")\n",
    "tmp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+------------------+----------+----------+\n",
      "|sum(count)|        avg(count)|stddev_samp(count)|min(count)|max(count)|\n",
      "+----------+------------------+------------------+----------+----------+\n",
      "|      1651|23.253521126760564| 62.53769567454248|         1|       347|\n",
      "+----------+------------------+------------------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(movies_df\n",
    " .where(F.col('date').isNotNull())\n",
    " .withColumn('new_date', F.to_date('date', 'dd-MMM-yyyy'))\n",
    " .withColumn('year', F.year(F.to_date(F.col('date'), 'dd-MMM-yyyy')))\n",
    " .groupBy('year')\n",
    " .count()\n",
    " .orderBy('count', ascending=False)\n",
    " .select(F.sum('count'), F.avg('count'), F.stddev('count'), F.min('count'), F.max('count'))\n",
    " .show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(movies_df\n",
    " .where(F.col('date').isNotNull())\n",
    " .withColumn('new_date', F.to_date('date', 'dd-MMM-yyyy'))\n",
    " .withColumn('year', F.year(F.to_date(F.col('date'), 'dd-MMM-yyyy')))\n",
    " .groupBy('year')\n",
    " .count()\n",
    " .orderBy('count', ascending=False)\n",
    " .select(F.sum('count'), F.avg('count'), F.stddev('count'), F.min('count'), F.max('count'))\n",
    " .printSchema())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "row = Row(350, True, 'Learning Spark 2E', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "row = Row(350, True, \"Learning Spark 2E\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[r for r in row]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
