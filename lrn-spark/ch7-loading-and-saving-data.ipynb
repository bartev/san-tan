{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 7. Loading and Saving Your Data\n",
    "\n",
    "In this chapter, we discuss strategies to organize data such as bucketing and partitioning data for storage, compression schemes, splittable and non-splittable files, and Parquet files.\n",
    "\n",
    "Both engineers and data scientists will find parts of this chapter useful, as they evaluate what storage format is best suited for downstream consumption for future Spark jobs using the saved data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "sc = SparkContext('local', 'Ch7')\n",
    "\n",
    "# create a SparkSession\n",
    "spark = (SparkSession\n",
    "    .builder\n",
    "    .appName(\"ch7 example\")\n",
    "    .getOrCreate())\n",
    "\n",
    "import os\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "data_dir = '/Users/bartev/dev/gitpie/LearningSparkV2/databricks-datasets/learning-spark-v2/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this chapter, we discuss strategies to organize data such as bucketing and partitioning data for storage, compression schemes, splittable and non-splittable files, and Parquet files.\n",
    "\n",
    "Both engineers and data scientists will find parts of this chapter useful, as they evaluate what storage format is best suited for downstream consumption for future Spark jobs using the saved data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## READING TEXT FILES\n",
    "\n",
    "Reading is simple. Use an instance of a SparkSession and DataFrameReader to read a text file into a DataFrame in Python or Dataset in Scala.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|value                  |\n",
      "+-----------------------+\n",
      "|name,age,person        |\n",
      "|jack,10,True           |\n",
      "|jill,9,True            |\n",
      "|humpty dumpty,egg,False|\n",
      "|red,12,True            |\n",
      "+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lines_df = (spark.read\n",
    "               .text('./dirty-data.csv'))\n",
    "\n",
    "lines_df.show(n=10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|value                                                                                                                                        |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|----------------------------------------------------------------                                                                             |\n",
      "|Wed Jun 24 16:19:04 MST 2020:                                                                                                                |\n",
      "|Booting Derby version The Apache Software Foundation - Apache Derby - 10.12.1.1 - (1704137): instance a816c00e-0172-e8a0-afbb-000008ac2aa0   |\n",
      "|on database directory /Users/bartev/dev/github-bv/san-tan/lrn-spark/metastore_db with class loader sun.misc.Launcher$AppClassLoader@15327b79 |\n",
      "|Loaded from file:/Users/bartev/dev/spark-2.4.5-bin-hadoop2.7/jars/derby-10.12.1.1.jar                                                        |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(spark\n",
    " .read\n",
    " .text('derby.log')\n",
    " .show(n=5, truncate=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|value                                                                                                                                                                                                                                                                                        |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|Spark Command: /Library/Java/JavaVirtualMachines/applejdk-8.jdk/Contents/Home/bin/java -cp /Users/bartev/dev/spark-2.4.5-bin-hadoop2.7/conf/:/Users/bartev/dev/spark-2.4.5-bin-hadoop2.7/jars/* -Xmx1g org.apache.spark.deploy.master.Master --host masis.local --port 7077 --webui-port 8080|\n",
      "|Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties                                                                                                                                                                                                              |\n",
      "|20/06/24 16:29:14 INFO Master: Starting Spark master at spark://masis.local:7077                                                                                                                                                                                                             |\n",
      "|20/06/24 16:29:14 INFO Master: Running Spark version 2.4.5                                                                                                                                                                                                                                   |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lines_df = (spark\n",
    " .read\n",
    " .text('spark-bartev-log.log')\n",
    " )\n",
    "\n",
    "(lines_df\n",
    " .filter(col('value').contains('Spark' or 'spark'))\n",
    " .show(n=10, truncate=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load apache log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------+\n",
      "|value                                                                         |\n",
      "+------------------------------------------------------------------------------+\n",
      "|# Apache Spark                                                                |\n",
      "|Spark is a fast and general cluster computing system for Big Data. It provides|\n",
      "|rich set of higher-level tools including Spark SQL for SQL and DataFrames,    |\n",
      "|and Spark Streaming for stream processing.                                    |\n",
      "|You can find the latest Spark documentation, including a programming          |\n",
      "+------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(spark\n",
    " .read\n",
    " .text(os.path.join(data_dir, 'SPARK_README.md'))\n",
    " .filter(col('value').contains('Spark'))\n",
    " .show(n=5, truncate=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines_df = (spark\n",
    "           .read\n",
    "           .text(os.path.join(data_dir, 'web1_access_log_20190715-064001.log')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|value                                                                                                                                                                                                                                                     |\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|161.239.128.119 - - [15/Jul/2019:06:44:31 -0700] \"DELETE /apps/cart.jsp?appID=3419 HTTP/1.0\" 200 4976 \"http://baker-kennedy.org/\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_1) AppleWebKit/5340 (KHTML, like Gecko) Chrome/13.0.849.0 Safari/5340\"     |\n",
      "|44.137.30.10 - - [15/Jul/2019:06:47:22 -0700] \"GET /search/tag/list HTTP/1.0\" 200 5033 \"http://wilson.net/blog/explore/posts/privacy/\" \"Mozilla/5.0 (Macintosh; U; PPC Mac OS X 10_5_9; rv:1.9.6.20) Gecko/2016-06-05 13:21:30 Firefox/3.6.13\"            |\n",
      "|172.55.193.211 - - [15/Jul/2019:06:48:45 -0700] \"PUT /search/tag/list HTTP/1.0\" 200 5100 \"http://thomas.com/search/categories/about/\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_6_3; rv:1.9.4.20) Gecko/2014-07-11 21:43:54 Firefox/3.8\"                 |\n",
      "|38.54.132.198 - - [15/Jul/2019:06:52:40 -0700] \"POST /list HTTP/1.0\" 200 4967 \"http://www.joseph.com/tag/category/\" \"Mozilla/5.0 (compatible; MSIE 5.0; Windows NT 4.0; Trident/3.1)\"                                                                     |\n",
      "|254.97.46.14 - - [15/Jul/2019:06:56:08 -0700] \"PUT /search/tag/list HTTP/1.0\" 200 5049 \"http://www.daniels.info/about.php\" \"Mozilla/5.0 (X11; Linux x86_64; rv:1.9.7.20) Gecko/2017-06-27 03:31:05 Firefox/3.8\"                                           |\n",
      "|58.255.183.241 - - [15/Jul/2019:07:00:06 -0700] \"GET /explore HTTP/1.0\" 200 5090 \"http://www.garner.com/terms.html\" \"Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10_8_1 rv:4.0; en-US) AppleWebKit/535.8.4 (KHTML, like Gecko) Version/5.0.2 Safari/535.8.4\"|\n",
      "|42.1.26.44 - - [15/Jul/2019:07:00:46 -0700] \"GET /posts/posts/explore HTTP/1.0\" 200 4938 \"http://www.dixon.com/search/category/explore/post.asp\" \"Mozilla/5.0 (Macintosh; U; PPC Mac OS X 10_7_0; rv:1.9.4.20) Gecko/2019-05-08 20:37:48 Firefox/3.6.8\"   |\n",
      "|180.245.245.197 - - [15/Jul/2019:07:03:21 -0700] \"GET /list HTTP/1.0\" 301 4992 \"http://payne.com/main/author/\" \"Opera/9.62.(Windows 98; Win 9x 4.90; it-IT) Presto/2.9.184 Version/11.00\"                                                                 |\n",
      "|252.247.154.59 - - [15/Jul/2019:07:04:10 -0700] \"PUT /list HTTP/1.0\" 404 5043 \"http://price.biz/author/\" \"Mozilla/5.0 (X11; Linux x86_64; rv:1.9.6.20) Gecko/2015-06-08 01:07:08 Firefox/3.8\"                                                             |\n",
      "|46.60.13.129 - - [15/Jul/2019:07:06:34 -0700] \"GET /posts/posts/explore HTTP/1.0\" 200 4982 \"http://www.campbell.net/\" \"Opera/9.32.(X11; Linux x86_64; sl-SI) Presto/2.9.164 Version/10.00\"                                                                |\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lines_df.show(n=10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(value='161.239.128.119 - - [15/Jul/2019:06:44:31 -0700] \"DELETE /apps/cart.jsp?appID=3419 HTTP/1.0\" 200 4976 \"http://baker-kennedy.org/\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_1) AppleWebKit/5340 (KHTML, like Gecko) Chrome/13.0.849.0 Safari/5340\"')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines_df.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do I parse this log using pyspark?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing text files\n",
    "\n",
    "Saving text files is just as simple as reading text files. In our first example above, we can save the filtered text README.txt after filtering out all but Spark occurrences in the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------+\n",
      "|value                                                                               |\n",
      "+------------------------------------------------------------------------------------+\n",
      "|# Apache Spark                                                                      |\n",
      "|Spark is a fast and general cluster computing system for Big Data. It provides      |\n",
      "|rich set of higher-level tools including Spark SQL for SQL and DataFrames,          |\n",
      "|and Spark Streaming for stream processing.                                          |\n",
      "|You can find the latest Spark documentation, including a programming                |\n",
      "|## Building Spark                                                                   |\n",
      "|Spark is built using [Apache Maven](http://maven.apache.org/).                      |\n",
      "|To build Spark and its example programs, run:                                       |\n",
      "|[\"Building Spark\"](http://spark.apache.org/docs/latest/building-spark.html).        |\n",
      "|The easiest way to start using Spark is through the Scala shell:                    |\n",
      "|Spark also comes with several sample programs in the `examples` directory.          |\n",
      "|    ./bin/run-example SparkPi                                                       |\n",
      "|    MASTER=spark://host:7077 ./bin/run-example SparkPi                              |\n",
      "|Testing first requires [building Spark](#building-spark). Once Spark is built, tests|\n",
      "|Spark uses the Hadoop core library to talk to HDFS and other Hadoop-supported       |\n",
      "|Hadoop, you must build Spark against the same version that your cluster runs.       |\n",
      "|in the online documentation for an overview on how to configure Spark.              |\n",
      "+------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lines_readme = (spark\n",
    " .read\n",
    " .text(os.path.join(data_dir, 'SPARK_README.md'))\n",
    ")\n",
    "\n",
    "(lines_readme\n",
    " .filter(col('value').contains('Spark'))\n",
    " .show(n=30, truncate=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "(lines_readme\n",
    " .filter(col('value').contains('Spark'))\n",
    " .write.text('./storage/SPARK_README.md'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------+\n",
      "|value                                                                               |\n",
      "+------------------------------------------------------------------------------------+\n",
      "|# Apache Spark                                                                      |\n",
      "|Spark is a fast and general cluster computing system for Big Data. It provides      |\n",
      "|rich set of higher-level tools including Spark SQL for SQL and DataFrames,          |\n",
      "|and Spark Streaming for stream processing.                                          |\n",
      "|You can find the latest Spark documentation, including a programming                |\n",
      "|## Building Spark                                                                   |\n",
      "|Spark is built using [Apache Maven](http://maven.apache.org/).                      |\n",
      "|To build Spark and its example programs, run:                                       |\n",
      "|[\"Building Spark\"](http://spark.apache.org/docs/latest/building-spark.html).        |\n",
      "|The easiest way to start using Spark is through the Scala shell:                    |\n",
      "|Spark also comes with several sample programs in the `examples` directory.          |\n",
      "|    ./bin/run-example SparkPi                                                       |\n",
      "|    MASTER=spark://host:7077 ./bin/run-example SparkPi                              |\n",
      "|Testing first requires [building Spark](#building-spark). Once Spark is built, tests|\n",
      "|Spark uses the Hadoop core library to talk to HDFS and other Hadoop-supported       |\n",
      "|Hadoop, you must build Spark against the same version that your cluster runs.       |\n",
      "|in the online documentation for an overview on how to configure Spark.              |\n",
      "+------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.text('./storage/SPARK_README.md/part-*.txt').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "(lines_readme\n",
    "#  .filter(col('value').contains('Spark'))\n",
    " .repartition(4)\n",
    " .write\n",
    " .format('parquet')\n",
    " .mode('overwrite')\n",
    " .save('./storage/SPARK_README.parquet')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|    build/mvn -Ds...|\n",
      "|    scala> sc.par...|\n",
      "|To run one of the...|\n",
      "|supports general ...|\n",
      "|Try the following...|\n",
      "|MLlib for machine...|\n",
      "|Spark uses the Ha...|\n",
      "|<http://spark.apa...|\n",
      "|can also use an a...|\n",
      "|This README file ...|\n",
      "|                    |\n",
      "|                    |\n",
      "|                    |\n",
      "|                    |\n",
      "|                    |\n",
      "|                    |\n",
      "|                    |\n",
      "|                    |\n",
      "|    ./bin/spark-s...|\n",
      "|high-level APIs i...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(spark\n",
    " .read\n",
    " .parquet('./storage/SPARK_README.parquet/part-*.parquet')\n",
    " .show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partition by a given field or column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---+------+\n",
      "|         name|age|person|\n",
      "+-------------+---+------+\n",
      "|         jack| 10|  True|\n",
      "|         jill|  9|  True|\n",
      "|humpty dumpty|egg| False|\n",
      "|          red| 12|  True|\n",
      "|          joe|  9| False|\n",
      "|          red| 10|  True|\n",
      "+-------------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "names_df = (\n",
    "    spark\n",
    "    .read\n",
    "    .csv('./dirty-data.csv', header=True))\n",
    "\n",
    "names_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "(names_df\n",
    "    .write\n",
    "    .mode('overwrite')\n",
    "    .format('parquet')\n",
    "    .partitionBy('name')\n",
    "    .save('./storage/people.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "(names_df\n",
    "    .write\n",
    "    .mode('overwrite')\n",
    "    .format('parquet')\n",
    "    .partitionBy('age')\n",
    "    .save('./storage/people.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "(names_df\n",
    "    .write\n",
    "    .mode('overwrite')\n",
    "    .format('parquet')\n",
    "    .partitionBy('age', 'name')\n",
    "    .save('./storage/people.parquet'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bucketing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a managed table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "(names_df\n",
    " .groupBy('name')\n",
    " .count()\n",
    " .orderBy(desc('count'))\n",
    " .write.format('parquet')\n",
    " .bucketBy(3, 'count', 'name')\n",
    " .saveAsTable('names_tbl')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine 'bucketed' properties of the bucketed table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+----------------------------------------------------------------------------+-------+\n",
      "|col_name                    |data_type                                                                   |comment|\n",
      "+----------------------------+----------------------------------------------------------------------------+-------+\n",
      "|name                        |string                                                                      |null   |\n",
      "|count                       |bigint                                                                      |null   |\n",
      "|                            |                                                                            |       |\n",
      "|# Detailed Table Information|                                                                            |       |\n",
      "|Database                    |default                                                                     |       |\n",
      "|Table                       |names_tbl                                                                   |       |\n",
      "|Created Time                |Thu Jun 25 16:26:21 MDT 2020                                                |       |\n",
      "|Last Access                 |Wed Dec 31 16:59:59 MST 1969                                                |       |\n",
      "|Created By                  |Spark 2.4.5                                                                 |       |\n",
      "|Type                        |MANAGED                                                                     |       |\n",
      "|Provider                    |parquet                                                                     |       |\n",
      "|Num Buckets                 |3                                                                           |       |\n",
      "|Bucket Columns              |[`count`, `name`]                                                           |       |\n",
      "|Sort Columns                |[]                                                                          |       |\n",
      "|Location                    |file:/Users/bartev/dev/github-bv/san-tan/lrn-spark/spark-warehouse/names_tbl|       |\n",
      "+----------------------------+----------------------------------------------------------------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(spark\n",
    " .sql('describe formatted names_tbl')\n",
    " .show(truncate=False)\n",
    "#  .toPandas()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-------+\n",
      "|col_name|data_type|comment|\n",
      "+--------+---------+-------+\n",
      "|name    |string   |null   |\n",
      "|count   |bigint   |null   |\n",
      "+--------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(spark\n",
    " .sql('describe names_tbl')\n",
    " .show(truncate=False)\n",
    "#  .toPandas()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+-------------+\n",
      "|person|age|         name|\n",
      "+------+---+-------------+\n",
      "| False|egg|humpty dumpty|\n",
      "| False|  9|          joe|\n",
      "|  True|  9|         jill|\n",
      "|  True| 10|          red|\n",
      "|  True| 12|          red|\n",
      "|  True| 10|         jack|\n",
      "+------+---+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "peeps_df = (spark.read\n",
    " .format('parquet')\n",
    " .load('./storage/people.parquet'))\n",
    "\n",
    "peeps_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not working??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o630.save.\n: java.lang.ClassNotFoundException: Failed to find data source: delta. Please find packages at http://spark.apache.org/third-party-projects.html\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:657)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:245)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:229)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.ClassNotFoundException: delta.DefaultSource\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:382)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:418)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:351)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$20$$anonfun$apply$12.apply(DataSource.scala:634)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$20$$anonfun$apply$12.apply(DataSource.scala:634)\n\tat scala.util.Try$.apply(Try.scala:192)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$20.apply(DataSource.scala:634)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$20.apply(DataSource.scala:634)\n\tat scala.util.Try.orElse(Try.scala:84)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:634)\n\t... 13 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-82-7ac3bc28a9b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'delta'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     .save('./storage/peeps.delta'))\n\u001b[0m",
      "\u001b[0;32m~/.venvs3/lrnpyspark/lib/python3.7/site-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m    737\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 739\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    741\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.venvs3/lrnpyspark/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.venvs3/lrnpyspark/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.venvs3/lrnpyspark/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o630.save.\n: java.lang.ClassNotFoundException: Failed to find data source: delta. Please find packages at http://spark.apache.org/third-party-projects.html\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:657)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:245)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:229)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.ClassNotFoundException: delta.DefaultSource\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:382)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:418)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:351)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$20$$anonfun$apply$12.apply(DataSource.scala:634)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$20$$anonfun$apply$12.apply(DataSource.scala:634)\n\tat scala.util.Try$.apply(Try.scala:192)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$20.apply(DataSource.scala:634)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$20.apply(DataSource.scala:634)\n\tat scala.util.Try.orElse(Try.scala:84)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:634)\n\t... 13 more\n"
     ]
    }
   ],
   "source": [
    "(peeps_df\n",
    "    .write\n",
    "    .format('delta')\n",
    "    .save('./storage/peeps.delta'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
