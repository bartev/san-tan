{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `spark.ml`\n",
    "    * newer API based on DataFrames\n",
    "----    \n",
    "* `spark.mllib` (DON'T USE ME)\n",
    "    * original ML API based on RDD API\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data used in this notebook is from SF housing data set from Inside Airbnb.\n",
    "\n",
    "`dev/github-bv/LearningSparkV2/databricks-datasets/learning-spark-v2/sf-airbnb`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .master('local[*]')\n",
    "         .appName(\"spark-ml-ch-10b\")\n",
    "         .config('ui.showConsoleProgress', 'false')\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def db_fname(fname):\n",
    "    import os.path as path\n",
    "    data_dir = '~/dev/github-bv/LearningSparkV2/databricks-datasets/learning-spark-v2/'\n",
    "    return path.expanduser(path.join(data_dir, fname))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Designing ML Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Pipeline API provides a high-level API built on top of DataFrames to organize ML worlflow\n",
    "* composed of a series of `transformers` and `estimators`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Transformer\n",
    "    * DF -> DF + 1 or more columns appended\n",
    "    * has `.transform()` method\n",
    "* Estimator\n",
    "    * DF -> Model (Transformer)\n",
    "    * learns ('fits') params\n",
    "    * has `.fit()` method\n",
    "* Pipeline\n",
    "    * organize a series of transformers and estimators into a single model\n",
    "    * Pipeline is an `estimator`\n",
    "    * `pipeline.fit()` returns a `PipelineModel`, which is a `transformer`\n",
    "* Model\n",
    "    * a Transformer which will return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Ingestion and Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They've done some cleansing already. See Databricks communitiy edition notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "filePath = db_fname('sf-airbnb/sf-airbnb-clean.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "airbnbDF = spark.read.parquet(filePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+---------------+--------+---------+-----------------+-----+\n",
      "|neighbourhood_cleansed|      room_type|bedrooms|bathrooms|number_of_reviews|price|\n",
      "+----------------------+---------------+--------+---------+-----------------+-----+\n",
      "|      Western Addition|Entire home/apt|     1.0|      1.0|            180.0|170.0|\n",
      "|        Bernal Heights|Entire home/apt|     2.0|      1.0|            111.0|235.0|\n",
      "|        Haight Ashbury|   Private room|     1.0|      4.0|             17.0| 65.0|\n",
      "|        Haight Ashbury|   Private room|     1.0|      4.0|              8.0| 65.0|\n",
      "|      Western Addition|Entire home/apt|     2.0|      1.5|             27.0|785.0|\n",
      "+----------------------+---------------+--------+---------+-----------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airbnbDF.select('neighbourhood_cleansed', 'room_type', 'bedrooms', 'bathrooms', 'number_of_reviews', 'price').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 5758 rows in the training set, and 1388 in the test set\n"
     ]
    }
   ],
   "source": [
    "trainDF, testDF = airbnbDF.randomSplit([0.8, 0.2], seed=42)\n",
    "print(f\"\"\"There are {trainDF.count()} rows in the training set, and {testDF.count()} in the test set\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Features with Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression (like many other algorithms in Spark) requires that **all the input features are contained within a single vector in your DataFrame**. Thus, we need to transform our data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `VectorAssembler` to combine all columns into a single vector. https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+-----+\n",
      "|bedrooms|features|price|\n",
      "+--------+--------+-----+\n",
      "|     1.0|   [1.0]|200.0|\n",
      "|     1.0|   [1.0]|250.0|\n",
      "|     3.0|   [3.0]|250.0|\n",
      "|     1.0|   [1.0]| 45.0|\n",
      "|     1.0|   [1.0]|115.0|\n",
      "|     1.0|   [1.0]| 70.0|\n",
      "|     1.0|   [1.0]|105.0|\n",
      "|     1.0|   [1.0]| 86.0|\n",
      "|     1.0|   [1.0]|100.0|\n",
      "|     2.0|   [2.0]|220.0|\n",
      "+--------+--------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "vecAssembler = VectorAssembler(inputCols = ['bedrooms'], outputCol='features')\n",
    "vecTrainDF = vecAssembler.transform(trainDF)\n",
    "vecTrainDF.select('bedrooms','features', 'price').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bedrooms']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vecAssembler.getInputCols()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'features'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vecAssembler.getOutputCol()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Estimators to Build Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "lr = LinearRegression(featuresCol='features', labelCol='price')\n",
    "lrModel = lr.fit(vecTrainDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.ml.regression.LinearRegressionModel"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(lrModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.ml.regression.LinearRegression"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`lr.fit()` returns a `LinearRegressionModel` (lrModel), which is a `transformer`. In other words, the **output** of an estimatorâ€™s `fit()` method is a `transformer`. Once the estimator has learned the parameters, the transformer can apply these parameters to new data points to generate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The formula for the linear regression line is  price = 119.32 x bedrooms + 54.11\n"
     ]
    }
   ],
   "source": [
    "m = round(lrModel.coefficients[0], 2)\n",
    "b = round(lrModel.intercept,2)\n",
    "\n",
    "print(f\"\"\"The formula for the linear regression line is  price = {m} x bedrooms + {b}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([119.3164])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrModel.coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp = lrModel.extractParamMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Param(parent='LinearRegression_96b02464c14f', name='aggregationDepth', doc='suggested depth for treeAggregate (>= 2)'): 2,\n",
       " Param(parent='LinearRegression_96b02464c14f', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty'): 0.0,\n",
       " Param(parent='LinearRegression_96b02464c14f', name='epsilon', doc='The shape parameter to control the amount of robustness. Must be > 1.0.'): 1.35,\n",
       " Param(parent='LinearRegression_96b02464c14f', name='featuresCol', doc='features column name'): 'features',\n",
       " Param(parent='LinearRegression_96b02464c14f', name='fitIntercept', doc='whether to fit an intercept term'): True,\n",
       " Param(parent='LinearRegression_96b02464c14f', name='labelCol', doc='label column name'): 'price',\n",
       " Param(parent='LinearRegression_96b02464c14f', name='loss', doc='The loss function to be optimized. Supported options: squaredError, huber. (Default squaredError)'): 'squaredError',\n",
       " Param(parent='LinearRegression_96b02464c14f', name='maxIter', doc='maximum number of iterations (>= 0)'): 100,\n",
       " Param(parent='LinearRegression_96b02464c14f', name='predictionCol', doc='prediction column name'): 'prediction',\n",
       " Param(parent='LinearRegression_96b02464c14f', name='regParam', doc='regularization parameter (>= 0)'): 0.0,\n",
       " Param(parent='LinearRegression_96b02464c14f', name='solver', doc='The solver algorithm for optimization. Supported options: auto, normal, l-bfgs. (Default auto)'): 'auto',\n",
       " Param(parent='LinearRegression_96b02464c14f', name='standardization', doc='whether to standardize the training features before fitting the model'): True,\n",
       " Param(parent='LinearRegression_96b02464c14f', name='tol', doc='the convergence tolerance for iterative algorithms (>= 0)'): 1e-06}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aggregationDepth: \t2\n",
      "elasticNetParam: \t0.0\n",
      "epsilon: \t1.35\n",
      "featuresCol: \tfeatures\n",
      "fitIntercept: \tTrue\n",
      "labelCol: \tprice\n",
      "loss: \tsquaredError\n",
      "maxIter: \t100\n",
      "predictionCol: \tprediction\n",
      "regParam: \t0.0\n",
      "solver: \tauto\n",
      "standardization: \tTrue\n",
      "tol: \t1e-06\n"
     ]
    }
   ],
   "source": [
    "for m in mp:\n",
    "    print(f'{m.name}: \\t{mp[m]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pipelineModel` is a `transformer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "pipeline = Pipeline(stages=[vecAssembler, lr])\n",
    "pipelineModel = pipeline.fit(trainDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply it to our test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "preDF = pipelineModel.transform(testDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['host_is_superhost',\n",
       " 'cancellation_policy',\n",
       " 'instant_bookable',\n",
       " 'host_total_listings_count',\n",
       " 'neighbourhood_cleansed',\n",
       " 'latitude',\n",
       " 'longitude',\n",
       " 'property_type',\n",
       " 'room_type',\n",
       " 'accommodates',\n",
       " 'bathrooms',\n",
       " 'bedrooms',\n",
       " 'beds',\n",
       " 'bed_type',\n",
       " 'minimum_nights',\n",
       " 'number_of_reviews',\n",
       " 'review_scores_rating',\n",
       " 'review_scores_accuracy',\n",
       " 'review_scores_cleanliness',\n",
       " 'review_scores_checkin',\n",
       " 'review_scores_communication',\n",
       " 'review_scores_location',\n",
       " 'review_scores_value',\n",
       " 'price',\n",
       " 'bedrooms_na',\n",
       " 'bathrooms_na',\n",
       " 'beds_na',\n",
       " 'review_scores_rating_na',\n",
       " 'review_scores_accuracy_na',\n",
       " 'review_scores_cleanliness_na',\n",
       " 'review_scores_checkin_na',\n",
       " 'review_scores_communication_na',\n",
       " 'review_scores_location_na',\n",
       " 'review_scores_value_na',\n",
       " 'features',\n",
       " 'prediction']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preDF.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+-----+------------------+\n",
      "|bedrooms|features|price|        prediction|\n",
      "+--------+--------+-----+------------------+\n",
      "|     1.0|   [1.0]|130.0|173.42588969100558|\n",
      "|     1.0|   [1.0]| 85.0|173.42588969100558|\n",
      "|     1.0|   [1.0]| 95.0|173.42588969100558|\n",
      "|     1.0|   [1.0]|128.0|173.42588969100558|\n",
      "|     1.0|   [1.0]|250.0|173.42588969100558|\n",
      "|     1.0|   [1.0]| 95.0|173.42588969100558|\n",
      "|     1.0|   [1.0]|105.0|173.42588969100558|\n",
      "|     0.0|   [0.0]|125.0| 54.10946937938496|\n",
      "|     3.0|   [3.0]|405.0|412.05873031424676|\n",
      "|     1.0|   [1.0]| 72.0|173.42588969100558|\n",
      "+--------+--------+-----+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preDF.select('bedrooms', 'features', 'price', 'prediction').show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert categorical values into numeric values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark uses a `SparseVector` when the majority of entries are 0, so OHE does not massively increase consumption of memory or compute resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple ways to one-hot encode data in Spark\n",
    "\n",
    "1. Use `StringIndexer` and `OneHotEncoder`\n",
    "\n",
    "  * apply `StringIndexer` estimator to convert categorical values into category indices (ordered by label frequencies)\n",
    "  * pass output to `OneHotEncoder` (`OneHotEncoderEstimator` for us, since we're using Spark 2.4.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, OneHotEncoderEstimator\n",
    "\n",
    "# categoricalCols = [field for (field, dataType) in trainDF.dtypes if dataType == 'string']\n",
    "# indexOutputCols = [x + 'Index' for x in categoricalCols]\n",
    "# oheOutputCols = [x + 'OHE' for x in categoricalCols]\n",
    "\n",
    "# stringIndexer = StringIndexer(inputCols=categoricalCols,\n",
    "#                               outputCols=indexOutputCols,\n",
    "#                               handleInvalid='skip')\n",
    "# oheEncoder = OneHotEncoder(inputCols=indexOutputCols,\n",
    "#                            outputCols=oheOutputCols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_fields = [field for (field, dataType) in trainDF.dtypes if dataType == 'string']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['host_is_superhost',\n",
       " 'cancellation_policy',\n",
       " 'instant_bookable',\n",
       " 'neighbourhood_cleansed',\n",
       " 'property_type',\n",
       " 'room_type',\n",
       " 'bed_type']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_string_indexer(col_name):\n",
    "    \"\"\"valid values of handleInvalid\n",
    "    skip (filter rows)\n",
    "    error (throw an error)\n",
    "    keep (put in a special additional bucket)\n",
    "    \n",
    "    NOTE: spark 3.0 will accept multple columns as input/output\n",
    "    \"\"\"\n",
    "    encoded_col_name = f'{col_name}_Index'\n",
    "    string_indexer = StringIndexer(inputCol=col_name, \n",
    "                                   outputCol=encoded_col_name, \n",
    "                                   handleInvalid='keep')\n",
    "    return string_indexer\n",
    "\n",
    "def make_one_hot_encoder(col_names):\n",
    "    \"\"\"each `*_OHE` column will be a SparseVector after fitting and transformation\n",
    "    \n",
    "    Usage:\n",
    "    ohe_room_type = make_one_hot_encoder(['room_type'])\n",
    "    encoded_room_type = ohe_room_type.fit(transformed_room_type)\n",
    "\n",
    "    encoded_room_type.transform(transformed_room_type).show()\n",
    "    \n",
    "    +---------------+-----+---------------+-------------+\n",
    "    |      room_type|price|room_type_Index|room_type_OHE|\n",
    "    +---------------+-----+---------------+-------------+\n",
    "    |   Private room|200.0|            1.0|(3,[1],[1.0])|\n",
    "    |Entire home/apt|250.0|            0.0|(3,[0],[1.0])|\n",
    "    |Entire home/apt|250.0|            0.0|(3,[0],[1.0])|\n",
    "    \"\"\"\n",
    "    input_col_names = [f'{col_name}_Index' for col_name in col_names]\n",
    "    output_col_names = [f'{col_name}_OHE' for col_name in col_names]\n",
    "    estimator = OneHotEncoderEstimator(inputCols=input_col_names,\n",
    "                                  outputCols=output_col_names)\n",
    "    return estimator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "stages_cat_str_index = [make_string_indexer(c) for c in cat_fields]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "oheEncoder = make_one_hot_encoder(cat_fields)\n",
    "oheOutputCols = oheEncoder.getOutputCols()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Param(parent='OneHotEncoderEstimator_cad88dda0193', name='handleInvalid', doc=\"How to handle invalid data during transform(). Options are 'keep' (invalid data presented as an extra categorical feature) or error (throw an error). Note that this Param is only used during transform; during fitting, invalid data will result in an error.\"): 'error',\n",
       " Param(parent='OneHotEncoderEstimator_cad88dda0193', name='dropLast', doc='whether to drop the last category'): True,\n",
       " Param(parent='OneHotEncoderEstimator_cad88dda0193', name='inputCols', doc='input column names.'): ['host_is_superhost_Index',\n",
       "  'cancellation_policy_Index',\n",
       "  'instant_bookable_Index',\n",
       "  'neighbourhood_cleansed_Index',\n",
       "  'property_type_Index',\n",
       "  'room_type_Index',\n",
       "  'bed_type_Index'],\n",
       " Param(parent='OneHotEncoderEstimator_cad88dda0193', name='outputCols', doc='output column names.'): ['host_is_superhost_OHE',\n",
       "  'cancellation_policy_OHE',\n",
       "  'instant_bookable_OHE',\n",
       "  'neighbourhood_cleansed_OHE',\n",
       "  'property_type_OHE',\n",
       "  'room_type_OHE',\n",
       "  'bed_type_OHE']}"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oheEncoder.extractParamMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['host_total_listings_count',\n",
       " 'latitude',\n",
       " 'longitude',\n",
       " 'accommodates',\n",
       " 'bathrooms',\n",
       " 'bedrooms',\n",
       " 'beds',\n",
       " 'minimum_nights',\n",
       " 'number_of_reviews',\n",
       " 'review_scores_rating',\n",
       " 'review_scores_accuracy',\n",
       " 'review_scores_cleanliness',\n",
       " 'review_scores_checkin',\n",
       " 'review_scores_communication',\n",
       " 'review_scores_location',\n",
       " 'review_scores_value',\n",
       " 'bedrooms_na',\n",
       " 'bathrooms_na',\n",
       " 'beds_na',\n",
       " 'review_scores_rating_na',\n",
       " 'review_scores_accuracy_na',\n",
       " 'review_scores_cleanliness_na',\n",
       " 'review_scores_checkin_na',\n",
       " 'review_scores_communication_na',\n",
       " 'review_scores_location_na',\n",
       " 'review_scores_value_na']"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numericCols = [field for (field, dataType) in trainDF.dtypes if ((dataType == 'double') & (field != 'price'))]\n",
    "numericCols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "assemblerInputs = oheOutputCols + numericCols\n",
    "vecAssembler = VectorAssembler(inputCols=assemblerInputs, outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['host_is_superhost_OHE',\n",
       " 'cancellation_policy_OHE',\n",
       " 'instant_bookable_OHE',\n",
       " 'neighbourhood_cleansed_OHE',\n",
       " 'property_type_OHE',\n",
       " 'room_type_OHE',\n",
       " 'bed_type_OHE',\n",
       " 'host_total_listings_count',\n",
       " 'latitude',\n",
       " 'longitude',\n",
       " 'accommodates',\n",
       " 'bathrooms',\n",
       " 'bedrooms',\n",
       " 'beds',\n",
       " 'minimum_nights',\n",
       " 'number_of_reviews',\n",
       " 'review_scores_rating',\n",
       " 'review_scores_accuracy',\n",
       " 'review_scores_cleanliness',\n",
       " 'review_scores_checkin',\n",
       " 'review_scores_communication',\n",
       " 'review_scores_location',\n",
       " 'review_scores_value',\n",
       " 'bedrooms_na',\n",
       " 'bathrooms_na',\n",
       " 'beds_na',\n",
       " 'review_scores_rating_na',\n",
       " 'review_scores_accuracy_na',\n",
       " 'review_scores_cleanliness_na',\n",
       " 'review_scores_checkin_na',\n",
       " 'review_scores_communication_na',\n",
       " 'review_scores_location_na',\n",
       " 'review_scores_value_na']"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assemblerInputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### What does `StringIndexer` do?\n",
    "\n",
    "* `StringIndexer` creates an estimator that will convert a column of strings to a column of numbers ordered by frequency\n",
    "* to use it, `.fit()` your dataframe to get a `model` object out\n",
    "* then, `.transform()` new data to append the indexed columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Make the indexer object (no fitting yet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "se_1 = make_string_indexer('room_type')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### `.fit()` the indexer object (returns a `model`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n",
      "|      room_type|price|\n",
      "+---------------+-----+\n",
      "|   Private room|200.0|\n",
      "|Entire home/apt|250.0|\n",
      "|Entire home/apt|250.0|\n",
      "|   Private room| 45.0|\n",
      "|   Private room|115.0|\n",
      "|   Private room| 70.0|\n",
      "|   Private room|105.0|\n",
      "|   Private room| 86.0|\n",
      "|Entire home/apt|100.0|\n",
      "|Entire home/apt|220.0|\n",
      "|Entire home/apt|110.0|\n",
      "|   Private room|130.0|\n",
      "|   Private room|100.0|\n",
      "|Entire home/apt|350.0|\n",
      "|   Private room|159.0|\n",
      "|Entire home/apt|200.0|\n",
      "|Entire home/apt|250.0|\n",
      "|Entire home/apt|299.0|\n",
      "|Entire home/apt|250.0|\n",
      "|   Private room| 95.0|\n",
      "+---------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainDF.select('room_type', 'price').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "se_1_model = se_1.fit(trainDF.select('room_type', 'price'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### `.transform()` some data using the fitted indexer object (the `model`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+---------------+\n",
      "|      room_type|price|room_type_Index|\n",
      "+---------------+-----+---------------+\n",
      "|   Private room|200.0|            1.0|\n",
      "|Entire home/apt|250.0|            0.0|\n",
      "|Entire home/apt|250.0|            0.0|\n",
      "|   Private room| 45.0|            1.0|\n",
      "|   Private room|115.0|            1.0|\n",
      "|   Private room| 70.0|            1.0|\n",
      "|   Private room|105.0|            1.0|\n",
      "|   Private room| 86.0|            1.0|\n",
      "|Entire home/apt|100.0|            0.0|\n",
      "|Entire home/apt|220.0|            0.0|\n",
      "|Entire home/apt|110.0|            0.0|\n",
      "|   Private room|130.0|            1.0|\n",
      "|   Private room|100.0|            1.0|\n",
      "|Entire home/apt|350.0|            0.0|\n",
      "|   Private room|159.0|            1.0|\n",
      "|Entire home/apt|200.0|            0.0|\n",
      "|Entire home/apt|250.0|            0.0|\n",
      "|Entire home/apt|299.0|            0.0|\n",
      "|Entire home/apt|250.0|            0.0|\n",
      "|   Private room| 95.0|            1.0|\n",
      "+---------------+-----+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transformed_room_type = se_1_model.transform(trainDF.select('room_type', 'price'))\n",
    "transformed_room_type.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+---------------+-------------+\n",
      "|      room_type|price|room_type_Index|room_type_OHE|\n",
      "+---------------+-----+---------------+-------------+\n",
      "|   Private room|200.0|            1.0|(3,[1],[1.0])|\n",
      "|Entire home/apt|250.0|            0.0|(3,[0],[1.0])|\n",
      "|Entire home/apt|250.0|            0.0|(3,[0],[1.0])|\n",
      "|   Private room| 45.0|            1.0|(3,[1],[1.0])|\n",
      "|   Private room|115.0|            1.0|(3,[1],[1.0])|\n",
      "|   Private room| 70.0|            1.0|(3,[1],[1.0])|\n",
      "|   Private room|105.0|            1.0|(3,[1],[1.0])|\n",
      "|   Private room| 86.0|            1.0|(3,[1],[1.0])|\n",
      "|Entire home/apt|100.0|            0.0|(3,[0],[1.0])|\n",
      "|Entire home/apt|220.0|            0.0|(3,[0],[1.0])|\n",
      "|Entire home/apt|110.0|            0.0|(3,[0],[1.0])|\n",
      "|   Private room|130.0|            1.0|(3,[1],[1.0])|\n",
      "|   Private room|100.0|            1.0|(3,[1],[1.0])|\n",
      "|Entire home/apt|350.0|            0.0|(3,[0],[1.0])|\n",
      "|   Private room|159.0|            1.0|(3,[1],[1.0])|\n",
      "|Entire home/apt|200.0|            0.0|(3,[0],[1.0])|\n",
      "|Entire home/apt|250.0|            0.0|(3,[0],[1.0])|\n",
      "|Entire home/apt|299.0|            0.0|(3,[0],[1.0])|\n",
      "|Entire home/apt|250.0|            0.0|(3,[0],[1.0])|\n",
      "|   Private room| 95.0|            1.0|(3,[1],[1.0])|\n",
      "+---------------+-----+---------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ohe_room_type = make_one_hot_encoder(['room_type'])\n",
    "encoded_room_type = ohe_room_type.fit(transformed_room_type)\n",
    "\n",
    "encoded_room_type.transform(transformed_room_type).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|      room_type|\n",
      "+---------------+\n",
      "|    Shared room|\n",
      "|Entire home/apt|\n",
      "|   Private room|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transformed_room_type.('room_type').distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back to example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `RFormula`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RFormula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "rFormula = RFormula(formula='price ~.',\n",
    "                    featuresCol='features',\n",
    "                    labelCol='price',\n",
    "                    handleInvalid='keep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_transformer = rFormula.fit(trainDF.select('room_type', 'price'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+-------------+\n",
      "|      room_type|price|     features|\n",
      "+---------------+-----+-------------+\n",
      "|   Private room|200.0|[0.0,1.0,0.0]|\n",
      "|Entire home/apt|250.0|[1.0,0.0,0.0]|\n",
      "|Entire home/apt|250.0|[1.0,0.0,0.0]|\n",
      "|   Private room| 45.0|[0.0,1.0,0.0]|\n",
      "|   Private room|115.0|[0.0,1.0,0.0]|\n",
      "|   Private room| 70.0|[0.0,1.0,0.0]|\n",
      "|   Private room|105.0|[0.0,1.0,0.0]|\n",
      "|   Private room| 86.0|[0.0,1.0,0.0]|\n",
      "|Entire home/apt|100.0|[1.0,0.0,0.0]|\n",
      "|Entire home/apt|220.0|[1.0,0.0,0.0]|\n",
      "|Entire home/apt|110.0|[1.0,0.0,0.0]|\n",
      "|   Private room|130.0|[0.0,1.0,0.0]|\n",
      "|   Private room|100.0|[0.0,1.0,0.0]|\n",
      "|Entire home/apt|350.0|[1.0,0.0,0.0]|\n",
      "|   Private room|159.0|[0.0,1.0,0.0]|\n",
      "|Entire home/apt|200.0|[1.0,0.0,0.0]|\n",
      "|Entire home/apt|250.0|[1.0,0.0,0.0]|\n",
      "|Entire home/apt|299.0|[1.0,0.0,0.0]|\n",
      "|Entire home/apt|250.0|[1.0,0.0,0.0]|\n",
      "|   Private room| 95.0|[0.0,1.0,0.0]|\n",
      "+---------------+-----+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf_transformer.transform(trainDF.select('room_type', 'price')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_transformer = rFormula.fit(trainDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|room_type      |price|features                                                                                                                                                                                                                                 |\n",
      "+---------------+-----+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|Private room   |200.0|(104,[0,4,8,43,46,47,49,74,76,77,78,79,80,85,86,87,88,89,90,91,92,93],[1.0,1.0,1.0,1.0,37.7431,-122.44509,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,100.0,10.0,10.0,10.0,10.0,10.0,10.0])                                                      |\n",
      "|Entire home/apt|250.0|(104,[0,4,8,10,28,46,47,48,73,76,77,78,79,80,85,86,87,88,89,90,91,92,93],[1.0,1.0,1.0,1.0,1.0,37.72979,-122.37094,1.0,1.0,2.0,1.0,1.0,1.0,1.0,180.0,1.0,100.0,10.0,10.0,10.0,10.0,10.0,10.0])                                            |\n",
      "|Entire home/apt|250.0|(104,[0,4,8,10,28,46,47,49,73,76,77,78,79,80,85,87,88,89,90,91,92,93,97,98,99,100,101,102,103],[1.0,1.0,1.0,1.0,1.0,37.73072,-122.38907,1.0,1.0,6.0,3.0,3.0,3.0,1.0,30.0,98.0,10.0,10.0,10.0,10.0,10.0,10.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])|\n",
      "|Private room   |45.0 |(104,[0,4,8,10,28,46,47,49,74,76,77,78,79,80,85,87,88,89,90,91,92,93,97,98,99,100,101,102,103],[1.0,1.0,1.0,1.0,1.0,37.7325,-122.39221,1.0,1.0,1.0,1.0,1.0,1.0,1.0,31.0,98.0,10.0,10.0,10.0,10.0,10.0,10.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]) |\n",
      "|Private room   |115.0|(104,[0,4,8,10,28,46,47,49,74,76,77,78,79,80,85,86,87,88,89,90,91,92,93],[1.0,1.0,1.0,1.0,1.0,37.7352,-122.38566,1.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,100.0,96.0,10.0,9.0,10.0,10.0,9.0,10.0])                                                |\n",
      "|Private room   |70.0 |(104,[0,4,8,10,28,46,47,49,74,76,77,78,79,80,85,87,88,89,90,91,92,93,97,98,99,100,101,102,103],[1.0,1.0,1.0,1.0,1.0,37.73555,-122.39779,1.0,1.0,1.0,1.0,1.0,1.0,1.0,30.0,98.0,10.0,10.0,10.0,10.0,10.0,10.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])|\n",
      "|Private room   |105.0|(104,[0,4,8,10,16,46,47,50,74,76,77,78,79,80,85,86,87,88,89,90,91,92,93],[1.0,1.0,1.0,1.0,1.0,37.7326,-122.41423,1.0,1.0,2.0,1.5,1.0,1.0,1.0,2.0,36.0,96.0,10.0,10.0,10.0,10.0,10.0,10.0])                                               |\n",
      "|Private room   |86.0 |(104,[0,4,8,10,16,46,47,49,74,76,77,78,79,80,85,86,87,88,89,90,91,92,93],[1.0,1.0,1.0,1.0,1.0,37.73615,-122.41245,1.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,194.0,91.0,9.0,9.0,10.0,10.0,9.0,9.0])                                                 |\n",
      "|Entire home/apt|100.0|(104,[0,4,8,10,16,46,47,48,73,76,77,78,79,80,85,86,87,88,89,90,91,92,93],[1.0,1.0,1.0,1.0,1.0,37.73765,-122.41247,1.0,1.0,4.0,1.0,1.0,2.0,1.0,2.0,4.0,95.0,10.0,10.0,10.0,9.0,9.0,10.0])                                                 |\n",
      "|Entire home/apt|220.0|(104,[0,4,8,10,16,46,47,49,73,76,77,78,79,80,85,86,87,88,89,90,91,92,93],[1.0,1.0,1.0,1.0,1.0,37.73826,-122.41693,1.0,1.0,4.0,1.0,2.0,2.0,1.0,4.0,2.0,100.0,10.0,10.0,10.0,10.0,10.0,10.0])                                              |\n",
      "|Entire home/apt|110.0|(104,[0,4,8,10,16,46,47,51,73,76,77,78,79,80,85,86,87,88,89,90,91,92,93],[1.0,1.0,1.0,1.0,1.0,37.73994,-122.41502,1.0,1.0,3.0,1.0,1.0,2.0,1.0,30.0,2.0,100.0,10.0,9.0,10.0,10.0,10.0,10.0])                                              |\n",
      "|Private room   |130.0|(104,[0,4,8,10,16,46,47,49,74,76,77,78,79,80,85,86,87,88,89,90,91,92,93],[1.0,1.0,1.0,1.0,1.0,37.7418,-122.41674,1.0,1.0,3.0,1.0,1.0,1.0,1.0,2.0,5.0,100.0,10.0,10.0,10.0,10.0,10.0,10.0])                                               |\n",
      "|Private room   |100.0|(104,[0,4,8,10,16,46,47,51,74,76,77,78,79,80,85,86,87,88,89,90,91,92,93],[1.0,1.0,1.0,1.0,1.0,37.7422,-122.42091,1.0,1.0,4.0,1.0,1.0,3.0,1.0,3.0,49.0,95.0,10.0,10.0,10.0,10.0,10.0,9.0])                                                |\n",
      "|Entire home/apt|350.0|(104,[0,4,8,10,16,46,47,48,73,76,77,78,79,80,85,86,87,88,89,90,91,92,93],[1.0,1.0,1.0,1.0,1.0,37.74387,-122.42338,1.0,1.0,4.0,2.0,3.0,2.0,1.0,30.0,10.0,98.0,10.0,10.0,10.0,10.0,10.0,10.0])                                             |\n",
      "|Private room   |159.0|(104,[0,4,8,10,16,46,47,49,74,76,77,78,79,80,85,86,87,88,89,90,91,92,93],[1.0,1.0,1.0,1.0,1.0,37.74473,-122.41516,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,100.0,10.0,10.0,10.0,10.0,10.0,10.0])                                              |\n",
      "|Entire home/apt|200.0|(104,[0,4,8,10,16,46,47,49,73,76,77,78,79,82,85,87,88,89,90,91,92,93,97,98,99,100,101,102,103],[1.0,1.0,1.0,1.0,1.0,37.74494,-122.41034,1.0,1.0,4.0,2.0,2.0,2.0,1.0,30.0,98.0,10.0,10.0,10.0,10.0,10.0,10.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])|\n",
      "|Entire home/apt|250.0|(104,[0,4,8,10,16,46,47,48,73,76,77,78,79,80,85,86,87,88,89,90,91,92,93],[1.0,1.0,1.0,1.0,1.0,37.74552,-122.41195,1.0,1.0,2.0,2.0,1.0,1.0,1.0,2.0,4.0,100.0,10.0,10.0,10.0,10.0,10.0,10.0])                                              |\n",
      "|Entire home/apt|299.0|(104,[0,4,8,10,16,46,47,51,73,76,77,78,79,80,85,87,88,89,90,91,92,93,97,98,99,100,101,102,103],[1.0,1.0,1.0,1.0,1.0,37.74605,-122.42209,1.0,1.0,4.0,1.0,2.0,3.0,1.0,2.0,98.0,10.0,10.0,10.0,10.0,10.0,10.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]) |\n",
      "|Entire home/apt|250.0|(104,[0,4,8,10,16,46,47,49,73,76,77,78,79,80,85,86,87,88,89,90,91,92,93],[1.0,1.0,1.0,1.0,1.0,37.74697,-122.41193,1.0,1.0,3.0,1.0,2.0,2.0,1.0,2.0,15.0,96.0,10.0,10.0,10.0,10.0,10.0,10.0])                                              |\n",
      "|Private room   |95.0 |(104,[0,4,8,10,15,46,47,50,74,76,77,78,79,80,85,86,87,88,89,90,91,92,93],[1.0,1.0,1.0,1.0,1.0,37.758,-122.42991,1.0,1.0,2.0,1.5,1.0,1.0,1.0,2.0,27.0,100.0,10.0,10.0,10.0,10.0,10.0,10.0])                                               |\n",
      "+---------------+-----+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf_transformer.transform(trainDF).select('room_type', 'price', 'features').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You do not need to one-hot encode categorical features for tree-based methods, and it will often make your tree-based models worse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add LinearRegression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression(labelCol='price', featuresCol='features')\n",
    "pipeline = Pipeline(stages= stages_cat_str_index + [oheEncoder, vecAssembler, lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[host_is_superhost: string, cancellation_policy: string, instant_bookable: string, host_total_listings_count: double, neighbourhood_cleansed: string, latitude: double, longitude: double, property_type: string, room_type: string, accommodates: double, bathrooms: double, bedrooms: double, beds: double, bed_type: string, minimum_nights: double, number_of_reviews: double, review_scores_rating: double, review_scores_accuracy: double, review_scores_cleanliness: double, review_scores_checkin: double, review_scores_communication: double, review_scores_location: double, review_scores_value: double, price: double, bedrooms_na: double, bathrooms_na: double, beds_na: double, review_scores_rating_na: double, review_scores_accuracy_na: double, review_scores_cleanliness_na: double, review_scores_checkin_na: double, review_scores_communication_na: double, review_scores_location_na: double, review_scores_value_na: double, host_is_superhost_Index: double, cancellation_policy_Index: double, instant_bookable_Index: double, neighbourhood_cleansed_Index: double, property_type_Index: double, room_type_Index: double, bed_type_Index: double, property_type_OHE: vector, room_type_OHE: vector, instant_bookable_OHE: vector, bed_type_OHE: vector, cancellation_policy_OHE: vector, host_is_superhost_OHE: vector, neighbourhood_cleansed_OHE: vector, features: vector, prediction: double]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipelineModel = pipeline.fit(trainDF)\n",
    "\n",
    "predDF = pipelineModel.transform(testDF)\n",
    "\n",
    "predDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['host_is_superhost',\n",
       " 'cancellation_policy',\n",
       " 'instant_bookable',\n",
       " 'host_total_listings_count',\n",
       " 'neighbourhood_cleansed',\n",
       " 'latitude',\n",
       " 'longitude',\n",
       " 'property_type',\n",
       " 'room_type',\n",
       " 'accommodates',\n",
       " 'bathrooms',\n",
       " 'bedrooms',\n",
       " 'beds',\n",
       " 'bed_type',\n",
       " 'minimum_nights',\n",
       " 'number_of_reviews',\n",
       " 'review_scores_rating',\n",
       " 'review_scores_accuracy',\n",
       " 'review_scores_cleanliness',\n",
       " 'review_scores_checkin',\n",
       " 'review_scores_communication',\n",
       " 'review_scores_location',\n",
       " 'review_scores_value',\n",
       " 'price',\n",
       " 'bedrooms_na',\n",
       " 'bathrooms_na',\n",
       " 'beds_na',\n",
       " 'review_scores_rating_na',\n",
       " 'review_scores_accuracy_na',\n",
       " 'review_scores_cleanliness_na',\n",
       " 'review_scores_checkin_na',\n",
       " 'review_scores_communication_na',\n",
       " 'review_scores_location_na',\n",
       " 'review_scores_value_na',\n",
       " 'host_is_superhost_Index',\n",
       " 'cancellation_policy_Index',\n",
       " 'instant_bookable_Index',\n",
       " 'neighbourhood_cleansed_Index',\n",
       " 'property_type_Index',\n",
       " 'room_type_Index',\n",
       " 'bed_type_Index',\n",
       " 'property_type_OHE',\n",
       " 'room_type_OHE',\n",
       " 'instant_bookable_OHE',\n",
       " 'bed_type_OHE',\n",
       " 'cancellation_policy_OHE',\n",
       " 'host_is_superhost_OHE',\n",
       " 'neighbourhood_cleansed_OHE',\n",
       " 'features',\n",
       " 'prediction']"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predDF.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+--------------------+\n",
      "|price|        prediction|            features|\n",
      "+-----+------------------+--------------------+\n",
      "|130.0|-50.08206919874374|(104,[0,4,8,27,45...|\n",
      "| 85.0| 69.30273500843668|(104,[0,4,8,27,46...|\n",
      "| 95.0|122.44764330428825|(104,[0,4,8,27,48...|\n",
      "|128.0|-73.85987476664286|(104,[0,4,8,15,45...|\n",
      "|250.0| 122.6754680966792|(104,[0,4,8,15,46...|\n",
      "| 95.0| 200.5074117194099|(104,[0,4,8,35,45...|\n",
      "|105.0|128.48438720450395|(104,[0,4,8,36,46...|\n",
      "|125.0|108.70731811393489|(104,[0,4,8,36,45...|\n",
      "|405.0|444.05924130765334|(104,[0,4,8,16,50...|\n",
      "| 72.0|187.88334714553457|(104,[0,4,8,16,47...|\n",
      "|150.0|185.72996926754968|(104,[0,4,8,21,47...|\n",
      "|450.0|268.26018917100646|(104,[0,4,8,22,47...|\n",
      "|165.0|357.28925461431845|(104,[0,4,8,10,47...|\n",
      "| 85.0|200.10202003698168|(104,[0,4,8,10,47...|\n",
      "|100.0| 187.3679853991889|(104,[0,4,8,19,46...|\n",
      "|100.0| 52.38409496248187|(104,[0,4,8,30,45...|\n",
      "| 57.0|203.43439302167099|(104,[0,4,8,29,45...|\n",
      "| 99.0| 277.2300165159686|(104,[0,4,8,29,45...|\n",
      "|165.0| 445.4617629407994|(104,[0,4,8,29,47...|\n",
      "|410.0|473.10293052621455|(104,[0,4,8,32,46...|\n",
      "+-----+------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predDF.select('price', 'prediction', 'features').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In spark.ml there are classification, regression, clustering, and ranking evaluators (introduced in Spark 3.0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSE (root mean square error)\n",
    "\n",
    "use this and R2 since regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\text{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "regressionEvaluator = RegressionEvaluator(\n",
    "    predictionCol='prediction',\n",
    "    labelCol='price',\n",
    "    metricName='rmse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE is 286.2\n"
     ]
    }
   ],
   "source": [
    "rmse = regressionEvaluator.setMetricName('rmse').evaluate(predDF)\n",
    "print(f'RMSE is {rmse:.1f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "286.19910175951"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressionEvaluator.evaluate(predDF, {regressionEvaluator.metricName: 'rmse'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92.50341120810286"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressionEvaluator.evaluate(predDF, {regressionEvaluator.metricName: 'mae'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg, lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|        avg(price)|\n",
      "+------------------+\n",
      "|214.60020840569643|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainDF.select(avg('price')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(avg(price)=214.60020840569643)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDF.select(avg('price')).first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "214.60020840569643"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDF.select(avg('price')).first()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "avgPrice = trainDF.select(avg('price')).first()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we don't need to do a `model.transform(testDF)` to get a prediction.\n",
    "\n",
    "Instead, we are assigning the average price as the prediction value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "predDF_baseline = testDF.withColumn('avgPrediction', lit(avgPrice))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['host_is_superhost',\n",
       " 'cancellation_policy',\n",
       " 'instant_bookable',\n",
       " 'host_total_listings_count',\n",
       " 'neighbourhood_cleansed',\n",
       " 'latitude',\n",
       " 'longitude',\n",
       " 'property_type',\n",
       " 'room_type',\n",
       " 'accommodates',\n",
       " 'bathrooms',\n",
       " 'bedrooms',\n",
       " 'beds',\n",
       " 'bed_type',\n",
       " 'minimum_nights',\n",
       " 'number_of_reviews',\n",
       " 'review_scores_rating',\n",
       " 'review_scores_accuracy',\n",
       " 'review_scores_cleanliness',\n",
       " 'review_scores_checkin',\n",
       " 'review_scores_communication',\n",
       " 'review_scores_location',\n",
       " 'review_scores_value',\n",
       " 'price',\n",
       " 'bedrooms_na',\n",
       " 'bathrooms_na',\n",
       " 'beds_na',\n",
       " 'review_scores_rating_na',\n",
       " 'review_scores_accuracy_na',\n",
       " 'review_scores_cleanliness_na',\n",
       " 'review_scores_checkin_na',\n",
       " 'review_scores_communication_na',\n",
       " 'review_scores_location_na',\n",
       " 'review_scores_value_na',\n",
       " 'avgPrediction']"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predDF_baseline.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressionMeanEvaluator = RegressionEvaluator(predictionCol='avgPrediction', labelCol='price', metricName='rmse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for predicting avg price is: 311.16\n"
     ]
    }
   ],
   "source": [
    "rmse_baseline = regressionMeanEvaluator.evaluate(predDF_baseline)\n",
    "print(f'RMSE for predicting avg price is: {rmse_baseline:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE model: 286.20\n",
      "RMSE baseline: 311.16\n",
      "The model beat the baseline.\n"
     ]
    }
   ],
   "source": [
    "print(f'RMSE model: {rmse:.2f}\\nRMSE baseline: {rmse_baseline:.2f}')\n",
    "print('The model beat the baseline.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## R2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $R^2$ values range from $(-\\infty, 1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "R^2 &= 1 - \\frac{SS_{res}}{SS_{tot}} \\\\\n",
    "SS_{tot} &= \\sum_{i=1}^n (y_i - \\bar{y})^2 \\\\\n",
    "SS_{res} &= \\sum_{i=1}^n (y_i - \\hat{y})^2\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2 = regressionEvaluator.setMetricName('r2').evaluate(predDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15360837656049942"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressionEvaluator.evaluate(predDF, {regressionEvaluator.metricName: 'r2'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15360837656049942"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0003858279473081261"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2 - (1 - (rmse/rmse_baseline)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict price on log scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "logTrainDF = trainDF.withColumn('log_price', log(col('price')))\n",
    "logTestDF = testDF.withColumn('log_price', log(col('price')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_lr = LinearRegression(labelCol='log_price', featuresCol='features', predictionCol='log_pred')\n",
    "log_pipeline = Pipeline(stages = stages_cat_str_index + [oheEncoder, vecAssembler, log_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[host_is_superhost: string, cancellation_policy: string, instant_bookable: string, host_total_listings_count: double, neighbourhood_cleansed: string, latitude: double, longitude: double, property_type: string, room_type: string, accommodates: double, bathrooms: double, bedrooms: double, beds: double, bed_type: string, minimum_nights: double, number_of_reviews: double, review_scores_rating: double, review_scores_accuracy: double, review_scores_cleanliness: double, review_scores_checkin: double, review_scores_communication: double, review_scores_location: double, review_scores_value: double, price: double, bedrooms_na: double, bathrooms_na: double, beds_na: double, review_scores_rating_na: double, review_scores_accuracy_na: double, review_scores_cleanliness_na: double, review_scores_checkin_na: double, review_scores_communication_na: double, review_scores_location_na: double, review_scores_value_na: double, log_price: double, host_is_superhost_Index: double, cancellation_policy_Index: double, instant_bookable_Index: double, neighbourhood_cleansed_Index: double, property_type_Index: double, room_type_Index: double, bed_type_Index: double, property_type_OHE: vector, room_type_OHE: vector, instant_bookable_OHE: vector, bed_type_OHE: vector, cancellation_policy_OHE: vector, host_is_superhost_OHE: vector, neighbourhood_cleansed_OHE: vector, features: vector, log_pred: double]"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_pipeline_model = log_pipeline.fit(logTrainDF)\n",
    "\n",
    "log_predDF = log_pipeline_model.transform(logTestDF)\n",
    "\n",
    "log_predDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['host_is_superhost',\n",
       " 'cancellation_policy',\n",
       " 'instant_bookable',\n",
       " 'host_total_listings_count',\n",
       " 'neighbourhood_cleansed',\n",
       " 'latitude',\n",
       " 'longitude',\n",
       " 'property_type',\n",
       " 'room_type',\n",
       " 'accommodates',\n",
       " 'bathrooms',\n",
       " 'bedrooms',\n",
       " 'beds',\n",
       " 'bed_type',\n",
       " 'minimum_nights',\n",
       " 'number_of_reviews',\n",
       " 'review_scores_rating',\n",
       " 'review_scores_accuracy',\n",
       " 'review_scores_cleanliness',\n",
       " 'review_scores_checkin',\n",
       " 'review_scores_communication',\n",
       " 'review_scores_location',\n",
       " 'review_scores_value',\n",
       " 'price',\n",
       " 'bedrooms_na',\n",
       " 'bathrooms_na',\n",
       " 'beds_na',\n",
       " 'review_scores_rating_na',\n",
       " 'review_scores_accuracy_na',\n",
       " 'review_scores_cleanliness_na',\n",
       " 'review_scores_checkin_na',\n",
       " 'review_scores_communication_na',\n",
       " 'review_scores_location_na',\n",
       " 'review_scores_value_na',\n",
       " 'log_price',\n",
       " 'host_is_superhost_Index',\n",
       " 'cancellation_policy_Index',\n",
       " 'instant_bookable_Index',\n",
       " 'neighbourhood_cleansed_Index',\n",
       " 'property_type_Index',\n",
       " 'room_type_Index',\n",
       " 'bed_type_Index',\n",
       " 'property_type_OHE',\n",
       " 'room_type_OHE',\n",
       " 'instant_bookable_OHE',\n",
       " 'bed_type_OHE',\n",
       " 'cancellation_policy_OHE',\n",
       " 'host_is_superhost_OHE',\n",
       " 'neighbourhood_cleansed_OHE',\n",
       " 'features',\n",
       " 'log_pred']"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_predDF.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exponentiate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, exp\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+------------------+\n",
      "|price|        prediction|          log_pred|\n",
      "+-----+------------------+------------------+\n",
      "|130.0| 64.90612595675911| 4.172942009960593|\n",
      "| 85.0|103.75846825102599| 4.642065777476574|\n",
      "| 95.0|117.26581005219721| 4.764443238766688|\n",
      "|128.0| 47.53748348391692|3.8615185258215945|\n",
      "|250.0|107.07401774424223| 4.673520349929049|\n",
      "| 95.0|159.17878797836354| 5.070028023190929|\n",
      "|105.0|110.26373784158828| 4.702875112837148|\n",
      "|125.0|123.76142608947094|4.8183557292225885|\n",
      "|405.0| 378.0789841136678|5.9351031264736775|\n",
      "| 72.0|135.11715474504877| 4.906142215032787|\n",
      "|150.0| 161.5131508284563| 5.084586568625895|\n",
      "|450.0| 147.4394921654137| 4.993417869008198|\n",
      "|165.0|243.79652313459525| 5.496333955807643|\n",
      "| 85.0|123.53302787706579|4.8165085525173765|\n",
      "|100.0|119.54310069714214| 4.783676981633761|\n",
      "|100.0|109.49555077883437| 4.695883916274283|\n",
      "| 57.0|115.30904518886132| 4.747615873364225|\n",
      "| 99.0|185.54644286828037| 5.223305216577927|\n",
      "|165.0| 281.4170711500708| 5.639837807908009|\n",
      "|410.0| 259.9842664508151| 5.560621115533792|\n",
      "+-----+------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_predDF.withColumn('prediction', exp(col('log_pred'))).select('price', 'prediction', 'log_pred').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "expDF = log_predDF.withColumn('prediction', exp(col('log_pred')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 286.20\n",
      "r2: 0.15\n",
      "\n",
      "log RMSE: 278.56\n",
      "log r2: 0.20\n"
     ]
    }
   ],
   "source": [
    "log_regr_eval = RegressionEvaluator(labelCol='price', predictionCol='prediction')\n",
    "log_rmse = log_regr_eval.setMetricName('rmse').evaluate(expDF)\n",
    "log_r2 = log_regr_eval.setMetricName('r2').evaluate(expDF)\n",
    "\n",
    "print(f'RMSE: {rmse:.2f}')\n",
    "print(f'r2: {r2:.2f}')\n",
    "print()\n",
    "print(f'log RMSE: {log_rmse:.2f}')\n",
    "print(f'log r2: {log_r2:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice: prices are lognormal (the log of the prices is a normal distribution)\n",
    "\n",
    "building a model to predict log prices, then exponentiating to get actual price results in a lower RMSE and higher $R^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelinePath = './lr-pipeline-model'\n",
    "(pipelineModel\n",
    " .write()\n",
    " .overwrite()\n",
    " .save(pipelinePath))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When loading you need to specify the type of model you are loading (e.g. `LinearRegressionModel` or `LogisticRegressionModel`).\n",
    "\n",
    "If you always put transformers/estimators in a `Pipeline`, then you'll always load a `PipelineModel`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import PipelineModel\n",
    "\n",
    "savedPipelineModel = PipelineModel.load(pipelinePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df_saved = savedPipelineModel.transform(testDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "286.19910175951"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressionEvaluator.setMetricName('rmse').evaluate(pred_df_saved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse: 286.19910175951\n",
      "R2: 0.15360837656049942\n"
     ]
    }
   ],
   "source": [
    "print(f'rmse: {regressionEvaluator.setMetricName(\"rmse\").evaluate(pred_df_saved)}')\n",
    "print(f'R2: {regressionEvaluator.setMetricName(\"r2\").evaluate(pred_df_saved)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree Based Models\n",
    "\n",
    "https://community.cloud.databricks.com/?o=6130133108755320#notebook/3114109954854034/command/3114109954854035"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision trees:\n",
    "\n",
    "* scale-invariant\n",
    "* depth = longest path from root node to any leaf node\n",
    "    * if depth is too large, risk overfitting\n",
    "    * if too small, underfit\n",
    "* feature prep\n",
    "    * no need to standardize (scale) data\n",
    "    * take care about how to prepare categorical features\n",
    "    * pass to `StringIndexer`\n",
    "    \n",
    "**Do not OHE variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeRegressor(labelCol='price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter for just numeric columns and exclude price (the label)\n",
    "numericCols = [field for (field, dataType) in trainDF.dtypes\n",
    "               if ((dataType == 'double') & (field != 'price'))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we're on Spark 2.4, we `StringIndexer` only takes a single column.\n",
    "\n",
    "See this function that was defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoricalCols = [field for (field, dataType) in trainDF.dtypes if dataType == 'string']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indexOutputCols = [x + 'Index' for x in categoricalCols]\n",
    "# oheOutputCols = [x + 'OHE' for x in categoricalCols]\n",
    "\n",
    "# stringIndexer = StringIndexer(inputCols=categoricalCols,\n",
    "#                               outputCols=indexOutputCols,\n",
    "#                               handleInvalid='skip')\n",
    "# oheEncoder = OneHotEncoder(inputCols=indexOutputCols,\n",
    "#                            outputCols=oheOutputCols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['host_is_superhost',\n",
       " 'cancellation_policy',\n",
       " 'instant_bookable',\n",
       " 'neighbourhood_cleansed',\n",
       " 'property_type',\n",
       " 'room_type',\n",
       " 'bed_type']"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categoricalCols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def make_string_indexer(col_name):\n",
    "#     \"\"\"valid values of handleInvalid\n",
    "#     skip (filter rows)\n",
    "#     error (throw an error)\n",
    "#     keep (put in a special additional bucket)\n",
    "    \n",
    "#     NOTE: spark 3.0 will accept multple columns as input/output\n",
    "#     \"\"\"\n",
    "#     encoded_col_name = f'{col_name}_Index'\n",
    "#     string_indexer = StringIndexer(inputCol=col_name, \n",
    "#                                    outputCol=encoded_col_name, \n",
    "#                                    handleInvalid='keep')\n",
    "#     return string_indexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "stages_cat_str_index = [make_string_indexer(c) for c in cat_fields]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['host_is_superhost_Index',\n",
       " 'cancellation_policy_Index',\n",
       " 'instant_bookable_Index',\n",
       " 'neighbourhood_cleansed_Index',\n",
       " 'property_type_Index',\n",
       " 'room_type_Index',\n",
       " 'bed_type_Index']"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexOutputCols = [indexer.getOutputCol() for indexer in stages_cat_str_index]\n",
    "indexOutputCols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# combine output of StringIndexer and numeric columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "assemblerInputs = indexOutputCols + numericCols\n",
    "\n",
    "vecAssembler = VectorAssembler(inputCols=assemblerInputs, outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['host_is_superhost_Index',\n",
       " 'cancellation_policy_Index',\n",
       " 'instant_bookable_Index',\n",
       " 'neighbourhood_cleansed_Index',\n",
       " 'property_type_Index',\n",
       " 'room_type_Index',\n",
       " 'bed_type_Index',\n",
       " 'host_total_listings_count',\n",
       " 'latitude',\n",
       " 'longitude',\n",
       " 'accommodates',\n",
       " 'bathrooms',\n",
       " 'bedrooms',\n",
       " 'beds',\n",
       " 'minimum_nights',\n",
       " 'number_of_reviews',\n",
       " 'review_scores_rating',\n",
       " 'review_scores_accuracy',\n",
       " 'review_scores_cleanliness',\n",
       " 'review_scores_checkin',\n",
       " 'review_scores_communication',\n",
       " 'review_scores_location',\n",
       " 'review_scores_value',\n",
       " 'bedrooms_na',\n",
       " 'bathrooms_na',\n",
       " 'beds_na',\n",
       " 'review_scores_rating_na',\n",
       " 'review_scores_accuracy_na',\n",
       " 'review_scores_cleanliness_na',\n",
       " 'review_scores_checkin_na',\n",
       " 'review_scores_communication_na',\n",
       " 'review_scores_location_na',\n",
       " 'review_scores_value_na']"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assemblerInputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_scores_rating</th>\n",
       "      <th>review_scores_rating_na</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>98.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>98.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>96.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5753</th>\n",
       "      <td>97.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5754</th>\n",
       "      <td>97.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5755</th>\n",
       "      <td>100.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5756</th>\n",
       "      <td>97.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5757</th>\n",
       "      <td>98.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      review_scores_rating  review_scores_rating_na\n",
       "0                  100.000                    0.000\n",
       "1                  100.000                    0.000\n",
       "2                   98.000                    1.000\n",
       "3                   98.000                    1.000\n",
       "4                   96.000                    0.000\n",
       "...                    ...                      ...\n",
       "5753                97.000                    0.000\n",
       "5754                97.000                    0.000\n",
       "5755               100.000                    0.000\n",
       "5756                97.000                    0.000\n",
       "5757                98.000                    0.000"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDF.select('review_scores_rating', 'review_scores_rating_na').toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine stages into a pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: unlike `StringIndexer` in Spark 3.0, I have a list of individual `StringIndexer` objects, so need to add as a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "'requirement failed: DecisionTree requires maxBins (= 32) to be at least as large as the number of values in each categorical feature, but categorical feature 3 has 36 values. Considering remove this and other categorical features with a large number of values, or add more training examples.'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/.venvs3/lrnpyspark/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.venvs3/lrnpyspark/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o4457.fit.\n: java.lang.IllegalArgumentException: requirement failed: DecisionTree requires maxBins (= 32) to be at least as large as the number of values in each categorical feature, but categorical feature 3 has 36 values. Considering remove this and other categorical features with a large number of values, or add more training examples.\n\tat scala.Predef$.require(Predef.scala:224)\n\tat org.apache.spark.ml.tree.impl.DecisionTreeMetadata$.buildMetadata(DecisionTreeMetadata.scala:137)\n\tat org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:106)\n\tat org.apache.spark.ml.regression.DecisionTreeRegressor$$anonfun$train$1.apply(DecisionTreeRegressor.scala:114)\n\tat org.apache.spark.ml.regression.DecisionTreeRegressor$$anonfun$train$1.apply(DecisionTreeRegressor.scala:104)\n\tat org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)\n\tat scala.util.Try$.apply(Try.scala:192)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)\n\tat org.apache.spark.ml.regression.DecisionTreeRegressor.train(DecisionTreeRegressor.scala:104)\n\tat org.apache.spark.ml.regression.DecisionTreeRegressor.train(DecisionTreeRegressor.scala:47)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:118)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:82)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-251-0a6c2e6172ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mstages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstages_cat_str_index\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mvecAssembler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpipeline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpipelineModel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainDF\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.venvs3/lrnpyspark/lib/python3.7/site-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m~/.venvs3/lrnpyspark/lib/python3.7/site-packages/pyspark/ml/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    107\u001b[0m                     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# must be an Estimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m                     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m                     \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mindexOfLastEstimator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.venvs3/lrnpyspark/lib/python3.7/site-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m~/.venvs3/lrnpyspark/lib/python3.7/site-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.venvs3/lrnpyspark/lib/python3.7/site-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    290\u001b[0m         \"\"\"\n\u001b[1;32m    291\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.venvs3/lrnpyspark/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.venvs3/lrnpyspark/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mQueryExecutionException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'java.lang.IllegalArgumentException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mIllegalArgumentException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: 'requirement failed: DecisionTree requires maxBins (= 32) to be at least as large as the number of values in each categorical feature, but categorical feature 3 has 36 values. Considering remove this and other categorical features with a large number of values, or add more training examples.'"
     ]
    }
   ],
   "source": [
    "stages = stages_cat_str_index + [vecAssembler, dt]\n",
    "pipeline = Pipeline(stages=stages)\n",
    "pipelineModel = pipeline.fit(trainDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check the `maxBins` parameter against our data\n",
    "* determines the number of bins into which the continuous featres are discretized or split\n",
    "* no `maxBins` param in `scikit-learn` because all the data an dmodel reside on a single machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt.getMaxBins()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cacheNodeIds: If false, the algorithm will pass trees to executors to match instances with nodes. If true, the algorithm will cache node IDs for each instance. Caching can speed up training of deeper trees. Users can set how often should the cache be checkpointed or disable it by setting checkpointInterval. (default: False)\n",
      "checkpointInterval: set checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations. Note: this setting will be ignored if the checkpoint directory is not set in the SparkContext. (default: 10)\n",
      "featuresCol: features column name. (default: features)\n",
      "impurity: Criterion used for information gain calculation (case-insensitive). Supported options: variance (default: variance)\n",
      "labelCol: label column name. (default: label, current: price)\n",
      "maxBins: Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature. (default: 32)\n",
      "maxDepth: Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. (default: 5)\n",
      "maxMemoryInMB: Maximum memory in MB allocated to histogram aggregation. If too small, then 1 node will be split per iteration, and its aggregates may exceed this size. (default: 256)\n",
      "minInfoGain: Minimum information gain for a split to be considered at a tree node. (default: 0.0)\n",
      "minInstancesPerNode: Minimum number of instances each child must have after split. If a split causes the left or right child to have fewer than minInstancesPerNode, the split will be discarded as invalid. Should be >= 1. (default: 1)\n",
      "predictionCol: prediction column name. (default: prediction)\n",
      "seed: random seed. (default: 1254355883992560603)\n",
      "varianceCol: column name for the biased sample variance of prediction. (undefined)\n"
     ]
    }
   ],
   "source": [
    "print(dt.explainParams())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.setMaxBins(40)\n",
    "pipelineModel = pipeline.fit(trainDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Extract if-then-else rules learned by the decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StringIndexer_bc694ed20c29,\n",
       " StringIndexer_beab10951a16,\n",
       " StringIndexer_4a9cd2322900,\n",
       " StringIndexer_0a05c84fc7c9,\n",
       " StringIndexer_ae7af6e96052,\n",
       " StringIndexer_527c2f801436,\n",
       " StringIndexer_3146f15f2444,\n",
       " VectorAssembler_a62bbd5cc750,\n",
       " DecisionTreeRegressionModel (uid=DecisionTreeRegressor_2e3d4bf687a9) of depth 5 with 49 nodes]"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipelineModel.stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dtModel = pipelineModel.stages[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeRegressionModel (uid=DecisionTreeRegressor_2e3d4bf687a9) of depth 5 with 49 nodes\n",
      "  If (feature 12 <= 2.5)\n",
      "   If (feature 12 <= 1.5)\n",
      "    If (feature 5 in {1.0,2.0})\n",
      "     If (feature 3 in {0.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0,11.0,14.0,15.0,16.0,17.0,18.0,20.0,23.0,24.0,26.0,27.0,28.0,29.0,30.0,31.0,33.0})\n",
      "      If (feature 10 <= 6.5)\n",
      "       Predict: 104.18763636363636\n",
      "      Else (feature 10 > 6.5)\n",
      "       Predict: 984.0\n",
      "     Else (feature 3 not in {0.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0,11.0,14.0,15.0,16.0,17.0,18.0,20.0,23.0,24.0,26.0,27.0,28.0,29.0,30.0,31.0,33.0})\n",
      "      If (feature 8 <= 37.743825)\n",
      "       Predict: 347.06666666666666\n",
      "      Else (feature 8 > 37.743825)\n",
      "       Predict: 147.8656509695291\n",
      "    Else (feature 5 not in {1.0,2.0})\n",
      "     If (feature 3 in {0.0,1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,11.0,14.0,15.0,16.0,17.0,18.0,19.0,20.0,22.0,23.0,24.0,25.0,26.0,27.0,28.0,29.0,30.0,31.0,32.0,34.0})\n",
      "      If (feature 10 <= 8.5)\n",
      "       Predict: 170.92544378698224\n",
      "      Else (feature 10 > 8.5)\n",
      "       Predict: 937.5\n",
      "     Else (feature 3 not in {0.0,1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,11.0,14.0,15.0,16.0,17.0,18.0,19.0,20.0,22.0,23.0,24.0,25.0,26.0,27.0,28.0,29.0,30.0,31.0,32.0,34.0})\n",
      "      If (feature 20 <= 8.5)\n",
      "       Predict: 1559.4\n",
      "      Else (feature 20 > 8.5)\n",
      "       Predict: 245.4\n",
      "   Else (feature 12 > 1.5)\n",
      "    If (feature 13 <= 4.5)\n",
      "     If (feature 3 in {0.0,1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0,11.0,12.0,14.0,15.0,16.0,17.0,18.0,20.0,22.0,23.0,24.0,25.0,26.0,27.0,28.0,29.0,30.0,31.0,32.0,34.0})\n",
      "      If (feature 3 in {3.0,5.0,8.0,11.0,14.0,15.0,16.0,17.0,20.0,22.0,23.0,24.0,25.0,28.0,29.0,30.0,32.0,34.0})\n",
      "       Predict: 202.24683544303798\n",
      "      Else (feature 3 not in {3.0,5.0,8.0,11.0,14.0,15.0,16.0,17.0,20.0,22.0,23.0,24.0,25.0,28.0,29.0,30.0,32.0,34.0})\n",
      "       Predict: 282.613707165109\n",
      "     Else (feature 3 not in {0.0,1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0,11.0,12.0,14.0,15.0,16.0,17.0,18.0,20.0,22.0,23.0,24.0,25.0,26.0,27.0,28.0,29.0,30.0,31.0,32.0,34.0})\n",
      "      If (feature 14 <= 3.5)\n",
      "       Predict: 701.0\n",
      "      Else (feature 14 > 3.5)\n",
      "       Predict: 319.56\n",
      "    Else (feature 13 > 4.5)\n",
      "     If (feature 15 <= 0.5)\n",
      "      If (feature 2 in {1.0})\n",
      "       Predict: 300.0\n",
      "      Else (feature 2 not in {1.0})\n",
      "       Predict: 10000.0\n",
      "     Else (feature 15 > 0.5)\n",
      "      If (feature 3 in {0.0,2.0,4.0,5.0,7.0,11.0,20.0,23.0})\n",
      "       Predict: 271.61538461538464\n",
      "      Else (feature 3 not in {0.0,2.0,4.0,5.0,7.0,11.0,20.0,23.0})\n",
      "       Predict: 849.5\n",
      "  Else (feature 12 > 2.5)\n",
      "   If (feature 1 in {0.0,1.0,2.0,3.0,4.0})\n",
      "    If (feature 12 <= 4.5)\n",
      "     If (feature 14 <= 8.0)\n",
      "      If (feature 3 in {0.0,1.0,3.0,4.0,5.0,6.0,7.0,8.0,10.0,11.0,14.0,15.0,16.0,17.0,20.0,22.0,23.0,24.0,26.0,27.0,28.0,29.0,30.0,32.0})\n",
      "       Predict: 455.16184971098266\n",
      "      Else (feature 3 not in {0.0,1.0,3.0,4.0,5.0,6.0,7.0,8.0,10.0,11.0,14.0,15.0,16.0,17.0,20.0,22.0,23.0,24.0,26.0,27.0,28.0,29.0,30.0,32.0})\n",
      "       Predict: 760.1923076923077\n",
      "     Else (feature 14 > 8.0)\n",
      "      If (feature 11 <= 2.25)\n",
      "       Predict: 280.38953488372096\n",
      "      Else (feature 11 > 2.25)\n",
      "       Predict: 406.72222222222223\n",
      "    Else (feature 12 > 4.5)\n",
      "     If (feature 4 in {0.0,1.0,5.0,6.0,7.0,14.0})\n",
      "      If (feature 3 in {0.0,3.0,4.0,6.0,8.0,13.0,15.0,19.0,23.0,24.0,30.0})\n",
      "       Predict: 617.7222222222222\n",
      "      Else (feature 3 not in {0.0,3.0,4.0,6.0,8.0,13.0,15.0,19.0,23.0,24.0,30.0})\n",
      "       Predict: 1524.2727272727273\n",
      "     Else (feature 4 not in {0.0,1.0,5.0,6.0,7.0,14.0})\n",
      "      If (feature 2 in {0.0})\n",
      "       Predict: 700.0\n",
      "      Else (feature 2 not in {0.0})\n",
      "       Predict: 8000.0\n",
      "   Else (feature 1 not in {0.0,1.0,2.0,3.0,4.0})\n",
      "    Predict: 8000.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(dtModel.toDebugString)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Interpreting Feature Importance\n",
    "-----\n",
    "hard to know what feature 12 vs 5 is..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Extract feature importance scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>bedrooms</td>\n",
       "      <td>0.264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>instant_bookable_Index</td>\n",
       "      <td>0.223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cancellation_policy_Index</td>\n",
       "      <td>0.171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>number_of_reviews</td>\n",
       "      <td>0.123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>property_type_Index</td>\n",
       "      <td>0.065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neighbourhood_cleansed_Index</td>\n",
       "      <td>0.059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>minimum_nights</td>\n",
       "      <td>0.026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>review_scores_communication</td>\n",
       "      <td>0.026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>beds</td>\n",
       "      <td>0.021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>room_type_Index</td>\n",
       "      <td>0.011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>accommodates</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>latitude</td>\n",
       "      <td>0.003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>bathrooms</td>\n",
       "      <td>0.002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>review_scores_accuracy_na</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>review_scores_rating_na</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>review_scores_cleanliness_na</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>beds_na</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>host_is_superhost_Index</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>bedrooms_na</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>review_scores_checkin_na</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>review_scores_communication_na</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>review_scores_location_na</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>bathrooms_na</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>review_scores_rating</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>review_scores_value</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>review_scores_location</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>review_scores_checkin</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>review_scores_cleanliness</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>review_scores_accuracy</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>longitude</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>host_total_listings_count</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>bed_type_Index</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>review_scores_value_na</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           feature  importance\n",
       "12                        bedrooms       0.264\n",
       "2           instant_bookable_Index       0.223\n",
       "1        cancellation_policy_Index       0.171\n",
       "15               number_of_reviews       0.123\n",
       "4              property_type_Index       0.065\n",
       "3     neighbourhood_cleansed_Index       0.059\n",
       "14                  minimum_nights       0.026\n",
       "20     review_scores_communication       0.026\n",
       "13                            beds       0.021\n",
       "5                  room_type_Index       0.011\n",
       "10                    accommodates       0.006\n",
       "8                         latitude       0.003\n",
       "11                       bathrooms       0.002\n",
       "27       review_scores_accuracy_na       0.000\n",
       "26         review_scores_rating_na       0.000\n",
       "28    review_scores_cleanliness_na       0.000\n",
       "25                         beds_na       0.000\n",
       "0          host_is_superhost_Index       0.000\n",
       "23                     bedrooms_na       0.000\n",
       "29        review_scores_checkin_na       0.000\n",
       "30  review_scores_communication_na       0.000\n",
       "31       review_scores_location_na       0.000\n",
       "24                    bathrooms_na       0.000\n",
       "16            review_scores_rating       0.000\n",
       "22             review_scores_value       0.000\n",
       "21          review_scores_location       0.000\n",
       "19           review_scores_checkin       0.000\n",
       "18       review_scores_cleanliness       0.000\n",
       "17          review_scores_accuracy       0.000\n",
       "9                        longitude       0.000\n",
       "7        host_total_listings_count       0.000\n",
       "6                   bed_type_Index       0.000\n",
       "32          review_scores_value_na       0.000"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "featureImp = (pd.DataFrame(\n",
    "    list(zip(vecAssembler.getInputCols(), dtModel.featureImportances)),\n",
    "    columns=['feature', 'importance'])\n",
    "              .sort_values(by='importance', ascending=False)\n",
    "             )\n",
    "featureImp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Apply model to test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+------------------+\n",
      "|            features| price|        prediction|\n",
      "+--------------------+------+------------------+\n",
      "|(33,[2,3,4,7,8,9,...|9000.0|280.38953488372096|\n",
      "|[0.0,2.0,1.0,14.0...|3800.0|202.24683544303798|\n",
      "|(33,[0,1,3,4,7,8,...|1850.0| 760.1923076923077|\n",
      "|(33,[0,3,4,7,8,9,...|1495.0|455.16184971098266|\n",
      "|(33,[3,4,7,8,9,10...|1450.0| 617.7222222222222|\n",
      "|(33,[1,3,4,7,8,9,...|1300.0|             701.0|\n",
      "|(33,[1,3,4,7,8,9,...|1252.0|            8000.0|\n",
      "|(33,[0,1,3,4,7,8,...|1250.0| 760.1923076923077|\n",
      "|[1.0,0.0,1.0,3.0,...|1200.0|170.92544378698224|\n",
      "|(33,[1,3,7,8,9,10...|1195.0|455.16184971098266|\n",
      "|(33,[3,4,7,8,9,10...|1100.0|455.16184971098266|\n",
      "|(33,[3,4,7,8,9,10...|1099.0|             701.0|\n",
      "|(33,[2,3,4,5,7,8,...|1099.0|             701.0|\n",
      "|(33,[1,3,4,7,8,9,...|1082.0|             245.4|\n",
      "|(33,[3,4,7,8,9,10...|1075.0| 760.1923076923077|\n",
      "|(33,[3,4,7,8,9,10...|1049.0| 760.1923076923077|\n",
      "|[0.0,0.0,1.0,5.0,...|1000.0|202.24683544303798|\n",
      "|(33,[2,3,4,7,8,9,...| 950.0|280.38953488372096|\n",
      "|(33,[0,3,4,7,8,9,...| 945.0|455.16184971098266|\n",
      "|(33,[2,3,7,8,9,10...| 900.0|  282.613707165109|\n",
      "+--------------------+------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predDF = pipelineModel.transform(testDF)\n",
    "predDF.select('features', 'price', 'prediction').orderBy('price', ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Pitfall\n",
    "\n",
    "What if we get a massive Airbnb rental? It was 20 bedrooms and 20 bathrooms. What will a decision tree predict?\n",
    "\n",
    "It turns out decision trees cannot predict any values larger than they were trained on. The max value in our training set was $10,000, so we can't predict any values larger than that (or technically any values larger than the )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE is 344.2356073869826\n",
      "R2 is -0.2244650863331945\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "regressionEvaluator = RegressionEvaluator(predictionCol=\"prediction\", \n",
    "                                          labelCol=\"price\", \n",
    "                                          metricName=\"rmse\")\n",
    "\n",
    "rmse = regressionEvaluator.evaluate(predDF)\n",
    "r2 = regressionEvaluator.setMetricName(\"r2\").evaluate(predDF)\n",
    "print(f\"RMSE is {rmse}\")\n",
    "print(f\"R2 is {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Ensemble\n",
    "    * build many models, and combine/average their predictions\n",
    "* Random forest - ensemble of decision tree\n",
    "    * Bootstrapping samples by rows\n",
    "        * sample with replacement\n",
    "    * each tree is trained on a different bootstrap sample of the data set\n",
    "    * aggregate the predictions\n",
    "    * *bootstrap aggregatine*, or *bagging*\n",
    "    * each tree samples the same number of data points with replacement from the original data set\n",
    "    * `subsamplingRate` - how many data points to sample for each tree\n",
    "* Random feature selection by columns\n",
    "    * With bagging, all the trees are highly correlated\n",
    "    * For each split, only consider a **random subset of columns**\n",
    "        * 1/3 the features for `RandomForeestRegressor`\n",
    "        * $\\sqrt{\\text{num features}}$ for `RandomForestClassifier`\n",
    "    * Keep each tree shallow (due to this extra randomness. why?)\n",
    "    * each tree is worse than a single decision tree\n",
    "    * each tree is a **weak learner**\n",
    "    * combining weak learners into an ensemble makes th forest more robust than a single decision tree\n",
    "----\n",
    "* Demonstrates power of distributed machine learning\n",
    "    * can build each tree independently of others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "\n",
    "rf = RandomForestRegressor(labelCol='price', maxBins=40, seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-fold Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What data set to use to optimze hyperparameters?\n",
    "* training set -> overfit\n",
    "* testing set -> cannot verify how well model generalizes\n",
    "* validation set!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* break the data in to k groups\n",
    "* train the model on folds 1-(k-1), and evaluate on fold k\n",
    "* train the model on folds 1-(k-2) + k, and evaluate on fold (k-1)\n",
    "* repeat k times, each time evaluating on a different fold\n",
    "* average the evaluation metrics of the k trials to get an estimate of how it will perform on unseen data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform a hyperparameter search in Spark, take the following steps :\n",
    "\n",
    "1. Define the estimator you want to evaluate.\n",
    "\n",
    "2. Specify which hyperparameters you want to vary, as well as their respective values, using the `ParamGridBuilder`.\n",
    "\n",
    "3. Define an `evaluator` to specify which metric to use to compare the various models.\n",
    "\n",
    "4. Use the `CrossValidator` to perform cross-validation, evaluating each of the various models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages = stages_cat_str_index + [vecAssembler, rf])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup `ParamGrid`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our `ParamGridBuilder`, weâ€™ll vary our `maxDepth` to be 2, 4, or 6 and `numTrees` (the number of trees in our random forest) to be 10 or 100. This will give us a grid of 6 (3 x 2) different hyperparameter configurations in total:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "paramGrid = (ParamGridBuilder()\n",
    "            .addGrid(rf.maxDepth, [2, 4, 6])\n",
    "            .addGrid(rf.numTrees, [10, 100])\n",
    "            .build())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have set up our hyperparameter grid, we need to define how to evaluate each of the models to determine which one performed best. For this task we will use the `RegressionEvaluator`, and weâ€™ll use RMSE as our metric of interest:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = RegressionEvaluator(labelCol='price',\n",
    "                               predictionCol='prediction',\n",
    "                               metricName='rmse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We will perform our k-fold cross-validation using the `CrossValidator`, which accepts an `estimator`, `evaluator`, \n",
    "and `estimatorParamMaps` so that it knows which model to use, how to evaluate the model, \n",
    "and which hyperparameters to set for the model. \n",
    "* We can also set the number of folds we \n",
    "want to split our data into (`numFolds=3`), as well as setting a seed so we have reproducible splits \n",
    "across the folds (`seed=42`). \n",
    "* Letâ€™s then fit this cross-validator to our training data set:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator\n",
    "\n",
    "cv = CrossValidator(estimator=pipeline,\n",
    "                    evaluator=evaluator,\n",
    "                    estimatorParamMaps=paramGrid,\n",
    "                    numFolds=3,\n",
    "                    seed=42)\n",
    "cvModel = cv.fit(trainDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
