{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d-sandbox\n",
    "# Data Cleansing with Airbnb\n",
    "\n",
    "We're going to start by doing some exploratory data analysis & cleansing. We will be using the SF Airbnb rental dataset from [Inside Airbnb](http://insideairbnb.com/get-the-data.html).\n",
    "\n",
    "<img src=\"http://insideairbnb.com/images/insideairbnb_graphic_site_1200px.png\" style=\"width:800px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the SF Airbnb dataset (comment out each of the options if you want to see what they do)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filePath = \"/databricks-datasets/learning-spark-v2/sf-airbnb/sf-airbnb.csv\"\n",
    "\n",
    "rawDF = spark.read.csv(filePath, header=\"true\", inferSchema=\"true\", multiLine=\"true\", escape='\"')\n",
    "\n",
    "display(rawDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawDF.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of simplicity, only keep certain columns from this dataset. We will talk about feature selection later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnsToKeep = [\n",
    "  \"host_is_superhost\",\n",
    "  \"cancellation_policy\",\n",
    "  \"instant_bookable\",\n",
    "  \"host_total_listings_count\",\n",
    "  \"neighbourhood_cleansed\",\n",
    "  \"latitude\",\n",
    "  \"longitude\",\n",
    "  \"property_type\",\n",
    "  \"room_type\",\n",
    "  \"accommodates\",\n",
    "  \"bathrooms\",\n",
    "  \"bedrooms\",\n",
    "  \"beds\",\n",
    "  \"bed_type\",\n",
    "  \"minimum_nights\",\n",
    "  \"number_of_reviews\",\n",
    "  \"review_scores_rating\",\n",
    "  \"review_scores_accuracy\",\n",
    "  \"review_scores_cleanliness\",\n",
    "  \"review_scores_checkin\",\n",
    "  \"review_scores_communication\",\n",
    "  \"review_scores_location\",\n",
    "  \"review_scores_value\",\n",
    "  \"price\"]\n",
    "\n",
    "baseDF = rawDF.select(columnsToKeep)\n",
    "baseDF.cache().count()\n",
    "display(baseDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixing Data Types\n",
    "\n",
    "Take a look at the schema above. You'll notice that the `price` field got picked up as string. For our task, we need it to be a numeric (double type) field. \n",
    "\n",
    "Let's fix that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, translate\n",
    "\n",
    "fixedPriceDF = baseDF.withColumn(\"price\", translate(col(\"price\"), \"$,\", \"\").cast(\"double\"))\n",
    "\n",
    "display(fixedPriceDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary statistics\n",
    "\n",
    "Two options:\n",
    "* describe\n",
    "* summary (describe + IQR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(fixedPriceDF.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(fixedPriceDF.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nulls\n",
    "\n",
    "There are a lot of different ways to handle null values. Sometimes, null can actually be a key indicator of the thing you are trying to predict (e.g. if you don't fill in certain portions of a form, probability of it getting approved decreases).\n",
    "\n",
    "Some ways to handle nulls:\n",
    "* Drop any records that contain nulls\n",
    "* Numeric:\n",
    "  * Replace them with mean/median/zero/etc.\n",
    "* Categorical:\n",
    "  * Replace them with the mode\n",
    "  * Create a special category for null\n",
    "* Use techniques like ALS which are designed to impute missing values\n",
    "  \n",
    "**If you do ANY imputation techniques for categorical/numerical features, you MUST include an additional field specifying that field was imputed (think about why this is necessary)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few nulls in the categorical feature `host_is_superhost`. Let's get rid of those rows where any of these columns is null.\n",
    "\n",
    "SparkML's Imputer (will cover below) does not support imputation for categorical features, so this is the simplest approach for the time being."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "noNullsDF = fixedPriceDF.na.drop(subset=[\"host_is_superhost\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impute: Cast to Double\n",
    "\n",
    "SparkML's `Imputer` requires all fields be of type double [Python](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.Imputer)/[Scala](https://spark.apache.org/docs/latest/api/scala/#org.apache.spark.ml.feature.Imputer). Let's cast all integer fields to double."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "integerColumns = [x.name for x in baseDF.schema.fields if x.dataType == IntegerType()]\n",
    "doublesDF = noNullsDF\n",
    "\n",
    "for c in integerColumns:\n",
    "  doublesDF = doublesDF.withColumn(c, col(c).cast(\"double\"))\n",
    "\n",
    "columns = \"\\n - \".join(integerColumns)\n",
    "print(f\"Columns converted from Integer to Double:\\n - {columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add in dummy variable if we will impute any value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "imputeCols = [\n",
    "  \"bedrooms\",\n",
    "  \"bathrooms\",\n",
    "  \"beds\", \n",
    "  \"review_scores_rating\",\n",
    "  \"review_scores_accuracy\",\n",
    "  \"review_scores_cleanliness\",\n",
    "  \"review_scores_checkin\",\n",
    "  \"review_scores_communication\",\n",
    "  \"review_scores_location\",\n",
    "  \"review_scores_value\"\n",
    "]\n",
    "\n",
    "for c in imputeCols:\n",
    "  doublesDF = doublesDF.withColumn(c + \"_na\", when(col(c).isNull(), 1.0).otherwise(0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(doublesDF.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "imputer = Imputer(strategy=\"median\", inputCols=imputeCols, outputCols=imputeCols)\n",
    "\n",
    "imputedDF = imputer.fit(doublesDF).transform(doublesDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting rid of extreme values\n",
    "\n",
    "Let's take a look at the *min* and *max* values of the `price` column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(imputedDF.select(\"price\").describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some super-expensive listings. But that's the Data Scientist's job to decide what to do with them. We can certainly filter the \"free\" Airbnbs though.\n",
    "\n",
    "Let's see first how many listings we can find where the *price* is zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputedDF.filter(col(\"price\") == 0).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now only keep rows with a strictly positive *price*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "posPricesDF = imputedDF.filter(col(\"price\") > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the *min* and *max* values of the *minimum_nights* column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(posPricesDF.select(\"minimum_nights\").describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(posPricesDF\n",
    "  .groupBy(\"minimum_nights\").count()\n",
    "  .orderBy(col(\"count\").desc(), col(\"minimum_nights\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A minimum stay of one year seems to be a reasonable limit here. Let's filter out those records where the *minimum_nights* is greater then 365:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanDF = posPricesDF.filter(col(\"minimum_nights\") <= 365)\n",
    "\n",
    "display(cleanDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, our data is cleansed now. Let's save this DataFrame to a file so that we can start building models with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputPath = \"/tmp/sf-airbnb/sf-airbnb-clean.parquet\"\n",
    "\n",
    "cleanDF.write.mode(\"overwrite\").parquet(outputPath)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "name": "10-1 Data Cleansing",
  "notebookId": 3114109954854114,
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
