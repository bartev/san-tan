{"cells":[{"cell_type":"markdown","source":["d\n# Regression: Predicting Rental Price\n\nIn this notebook, we will use the dataset we cleansed in the previous lab to predict Airbnb rental prices in San Francisco."],"metadata":{}},{"cell_type":"code","source":["filePath = \"/databricks-datasets/learning-spark-v2/sf-airbnb/sf-airbnb-clean.parquet\"\nairbnbDF = spark.read.parquet(filePath)\ndisplay(airbnbDF)"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":["## Train/Test Split\n\nWhen we are building ML models, we don't want to look at our test data (why is that?). \n\nLet's keep 80% for the training set and set aside 20% of our data for the test set. We will use the `randomSplit` method [Python](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.randomSplit)/[Scala](https://spark.apache.org/docs/latest/api/scala/#org.apache.spark.sql.Dataset).\n\n**Question**: Why is it necessary to set a seed?"],"metadata":{}},{"cell_type":"code","source":["trainDF, testDF = airbnbDF.randomSplit([.8, .2], seed=42)\nprint(f\"There are {trainDF.cache().count()} rows in the training set, and {testDF.cache().count()} in the test set\")"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":["**Question**: What happens if you change your cluster configuration?\n\nTo test this out, try spinning up a cluster with just one worker, and another with two workers. NOTE: This data is quite small (one partition), and you will need to test it out with a larger dataset (e.g. 2+ partitions) to see the difference, such as: `databricks-datasets/learning-spark-v2/sf-airbnb/sf-airbnb-clean-100p.parquet.` However, in this code below, we will simply repartition our data to simulate how it could have been partitioned differently on a different cluster configuration, and see if we get the same number of data points in our training set."],"metadata":{}},{"cell_type":"code","source":["(trainRepartitionDF, testRepartitionDF) = (airbnbDF\n                                           .repartition(24)\n                                           .randomSplit([.8, .2], seed=42))\n\nprint(trainRepartitionDF.count())"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":["When you do an 80/20 train/test split, it is an \"approximate\" 80/20 split. It is not an exact 80/20 split, and when we the partitioning of our data changes, we show that we get not only a different # of data points in train/test, but also different data points.\n\nOur recommendation is to split your data once, then write it out to its own train/test folder so you don't have these reproducibility issues."],"metadata":{}},{"cell_type":"markdown","source":["We are going to build a very simple linear regression model predicting `price` just given the number of `bedrooms`.\n\n**Question**: What are some assumptions of the linear regression model?"],"metadata":{}},{"cell_type":"code","source":["display(trainDF.select(\"price\", \"bedrooms\").summary())"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["There do appear some outliers in our dataset for the price ($10,000 a night??). Just keep this in mind when we are building our models :)."],"metadata":{}},{"cell_type":"markdown","source":["## Vector Assembler\n\nLinear Regression expects a column of Vector type as input.\n\nWe can easily get the values from the `bedrooms` column into a single vector using `VectorAssembler` [Python](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.VectorAssembler)/[Scala](https://spark.apache.org/docs/latest/api/scala/#org.apache.spark.ml.feature.VectorAssembler). VectorAssembler is an example of a **transformer**. Transformers take in a DataFrame, and return a new DataFrame with one or more columns appended to it. They do not learn from your data, but apply rule based transformations."],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.feature import VectorAssembler\n\nvecAssembler = VectorAssembler(inputCols=[\"bedrooms\"], outputCol=\"features\")\n\nvecTrainDF = vecAssembler.transform(trainDF)\n\nvecTrainDF.select(\"bedrooms\", \"features\", \"price\").show(10)"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["## Linear Regression\n\nNow that we have prepared our data, we can use the `LinearRegression` estimator to build our first model [Python](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.regression.LinearRegression)/[Scala](https://spark.apache.org/docs/latest/api/scala/#org.apache.spark.ml.regression.LinearRegression). Estimators accept a DataFrame as input and return a model, and have a `.fit()` method."],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.regression import LinearRegression\n\nlr = LinearRegression(featuresCol=\"features\", labelCol=\"price\")\nlrModel = lr.fit(vecTrainDF)"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":["## Inspect the model"],"metadata":{}},{"cell_type":"code","source":["m = round(lrModel.coefficients[0], 2)\nb = round(lrModel.intercept, 2)\n\nprint(f\"The formula for the linear regression line is price = {m}*bedrooms + {b}\")"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":["## Pipeline"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml import Pipeline\n\npipeline = Pipeline(stages=[vecAssembler, lr])\npipelineModel = pipeline.fit(trainDF)"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":["## Apply to Test Set"],"metadata":{}},{"cell_type":"code","source":["predDF = pipelineModel.transform(testDF)\n\npredDF.select(\"bedrooms\", \"features\", \"price\", \"prediction\").show(10)\n"],"metadata":{},"outputs":[],"execution_count":20}],"metadata":{"name":"10-2 Linear Regression","notebookId":3114109954854093},"nbformat":4,"nbformat_minor":0}
