{"cells":[{"cell_type":"markdown","source":["d\n# File Formats\n\nIn this notebook, we cover how different file formats impact your Spark Job performance.\n\nSpark Summit 2016: [Why You Should Care about Data Layout in the Filesystem](https://databricks.com/session/why-you-should-care-about-data-layout-in-the-filesystem)"],"metadata":{}},{"cell_type":"markdown","source":["Let's read in a colon delimited file."],"metadata":{}},{"cell_type":"code","source":["%fs ls /databricks-datasets/learning-spark-v2/people/people-with-header-10m.txt"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["%fs head --maxBytes=1000 /databricks-datasets/learning-spark-v2/people/people-with-header-10m.txt"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["csvDF = (spark\n         .read\n         .csv(\"/databricks-datasets/learning-spark-v2/people/people-with-header-10m.txt\", header=\"true\", sep=\":\"))"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["Are these data types correct? All of them are string types.\n\nWe need to tell Spark to infer the schema."],"metadata":{}},{"cell_type":"code","source":["csvDF = (spark\n         .read\n         .csv(\"/databricks-datasets/learning-spark-v2/people/people-with-header-10m.txt\", header=\"true\", sep=\":\", inferSchema=\"true\"))"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["Wow, that took a long time just to figure out the schema for this file! \n\nNow let's try the same thing with compressed files (Gzip and Snappy formats).\n\nNotice that the gzip file is the most compact - we will see if it is the fastest to operate on."],"metadata":{}},{"cell_type":"code","source":["%fs ls /databricks-datasets/learning-spark-v2/people/people-with-header-10m.txt"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["%fs ls /databricks-datasets/learning-spark-v2/people/people-with-header-10m.txt.gz"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["%fs ls /databricks-datasets/learning-spark-v2/people/people-with-header-10m.txt.snappy"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":["Read in the Gzip compression format file."],"metadata":{}},{"cell_type":"code","source":["csvDFgz = (spark\n           .read\n           .csv(\"/databricks-datasets/learning-spark-v2/people/people-with-header-10m.txt.gz\", header=\"true\", sep=\":\", inferSchema=\"true\"))"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["Although the uncompressed format took up more space than the Gzip format, it was significantly faster to operate on than the Gzip format."],"metadata":{}},{"cell_type":"code","source":["csvDFsnappy = (spark\n               .read\n               .csv(\"/databricks-datasets/learning-spark-v2/people/people-with-header-10m.txt.snappy\", header=\"true\", sep=\":\", inferSchema=\"true\"))"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":["Wait, I thought Snappy was supposed to be splittable - why was only one slot reading in the file?\n\nRegular CSV files that are compressed with Snappy format are not splittable. If you want to work with non-column based formats, you should use `bzip2` (Snappy is great for Parquet, which we'll see later)."],"metadata":{}},{"cell_type":"code","source":["%fs ls /databricks-datasets/learning-spark-v2/people/people-with-header-10m.csv.bzip"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":["Wow! The bzip file actually takes up less space than the snappy or gzip file. Let's read it in."],"metadata":{}},{"cell_type":"code","source":["csvBzip = (spark\n           .read\n           .csv(\"/databricks-datasets/learning-spark-v2/people/people-with-header-10m.csv.bzip\", header=True, sep=\":\", inferSchema=True))"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":["Look at how much faster that was! Note how many partitions it has now.\n\nLet's dig into compression schemes and `inferSchema`...\n\nHow can we avoid this painful schema inference step?"],"metadata":{}},{"cell_type":"code","source":["csvDF.schema.json()"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["dbutils.fs.put(\"/tmp/myschema.json\", csvDF.schema.json(), True)\n\nfrom pyspark.sql.types import StructType\nimport json\n\nschema_json = dbutils.fs.head(\"/tmp/myschema.json\", 1024)\nknownSchema = StructType.fromJson(json.loads(schema_json))"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["csvDFgz = (spark\n          .read\n          .csv(\"/databricks-datasets/learning-spark-v2/people/people-with-header-10m.txt.gz\", \n               header=\"true\", sep=\":\", schema=knownSchema))"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":["Much better, we loaded it in less than a second!\n\nNow let's compare this CSV file to Parquet."],"metadata":{}},{"cell_type":"code","source":["%fs ls /databricks-datasets/learning-spark-v2/people/people-10m.parquet/"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":["%python\nsize = [i.size for i in dbutils.fs.ls(\"/databricks-datasets/learning-spark-v2/people/people-10m.parquet/\") if i.name.endswith(\".parquet\")]\n__builtin__.sum(size)"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":["In addition to the Parquet file taking up less than 1/2 of the space required to store the uncompressed text file, it also encodes the column names and their associated data types.\n\n***BONUS*** - Why did we go from 1 CSV file to 8 Parquet files??"],"metadata":{}},{"cell_type":"code","source":["parquetDF = spark.read.parquet(\"/databricks-datasets/learning-spark-v2/people/people-10m.parquet/\")"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"markdown","source":["Lastly, it is much faster to operate on Parquet files than CSV files (especially when we are filtering or selecting a subset of columns). \n\nLook at the difference in times below! `%timeit` is a built-in Python function, so we are going to create temporary views to access the data in Python."],"metadata":{}},{"cell_type":"code","source":["parquetDF.createOrReplaceTempView(\"parquetDF\")\ncsvDF.createOrReplaceTempView(\"csvDF\")\ncsvDFgz.createOrReplaceTempView(\"csvDFgz\")"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"code","source":["%python\n%timeit -n1 -r1 spark.table(\"parquetDF\").select(\"gender\", \"salary\").where(\"salary > 10000\").count()"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":["If you're running on Databricks, subsequent calls to this Parquet file will be faster due to automatic caching!"],"metadata":{}},{"cell_type":"code","source":["%python\n%timeit -n1 -r1 spark.table(\"parquetDF\").select(\"gender\", \"salary\").where(\"salary > 10000\").count()"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"code","source":["%python\n%timeit -n1 -r1 spark.table(\"csvDF\").select(\"gender\", \"salary\").where(\"salary > 10000\").count()"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"code","source":["%python\n%timeit -n1 -r1 spark.table(\"csvDFgz\").select(\"gender\", \"salary\").where(\"salary > 10000\").count()"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"markdown","source":["-sandbox\n\n## Comparison\n| Type    | <span style=\"white-space:nowrap\">Inference Type</span> | <span style=\"white-space:nowrap\">Inference Speed</span> | Reason                                          | <span style=\"white-space:nowrap\">Should Supply Schema?</span> |\n|---------|--------------------------------------------------------|---------------------------------------------------------|----------------------------------------------------|:--------------:|\n| <b>CSV</b>     | <span style=\"white-space:nowrap\">Full-Data-Read</span> | <span style=\"white-space:nowrap\">Slow</span>            | <span style=\"white-space:nowrap\">File size</span>  | Yes            |\n| <b>Parquet</b> | <span style=\"white-space:nowrap\">Metadata-Read</span>  | <span style=\"white-space:nowrap\">Fast/Medium</span>     | <span style=\"white-space:nowrap\">Number of Partitions</span> | No (most cases)             |\n| <b>Tables</b>  | <span style=\"white-space:nowrap\">n/a</span>            | <span style=\"white-space:nowrap\">n/a</span>            | <span style=\"white-space:nowrap\">Predefined</span> | n/a            |\n| <b>JSON</b>    | <span style=\"white-space:nowrap\">Full-Read-Data</span> | <span style=\"white-space:nowrap\">Slow</span>            | <span style=\"white-space:nowrap\">File size</span>  | Yes            |\n| <b>Text</b>    | <span style=\"white-space:nowrap\">Dictated</span>       | <span style=\"white-space:nowrap\">Zero</span>            | <span style=\"white-space:nowrap\">Only 1 Column</span>   | Never          |\n| <b>JDBC</b>    | <span style=\"white-space:nowrap\">DB-Read</span>        | <span style=\"white-space:nowrap\">Fast</span>            | <span style=\"white-space:nowrap\">DB Schema</span>  | No             |"],"metadata":{}},{"cell_type":"markdown","source":["##Reading CSV\n- `spark.read.csv(..)`\n- There are a large number of options when reading CSV files including headers, column separator, escaping, etc.\n- We can allow Spark to infer the schema at the cost of first reading in the entire file.\n- Large CSV files should always have a schema pre-defined."],"metadata":{}},{"cell_type":"markdown","source":["## Reading Parquet\n- `spark.read.parquet(..)`\n- Parquet files are the preferred file format for big-data.\n- It is a columnar file format.\n- It is a splittable file format.\n- It offers a lot of performance benefits over other formats including predicate pushdown.\n- Unlike CSV, the schema is read in, not inferred.\n- Reading the schema from Parquet's metadata can be extremely efficient."],"metadata":{}},{"cell_type":"markdown","source":["## Reading Tables\n- `spark.read.table(..)`\n- The Databricks platform allows us to register a huge variety of data sources as tables via the Databricks UI.\n- Any `DataFrame` (from CSV, Parquet, whatever) can be registered as a temporary view.\n- Tables/Views can be loaded via the `DataFrameReader` to produce a `DataFrame`\n- Tables/Views can be used directly in SQL statements."],"metadata":{}},{"cell_type":"markdown","source":["## Reading JSON\n- `spark.read.json(..)`\n- JSON represents complex data types unlike CSV's flat format.\n- Has many of the same limitations as CSV (needing to read the entire file to infer the schema)\n- Like CSV has a lot of options allowing control on date formats, escaping, single vs. multiline JSON, etc."],"metadata":{}},{"cell_type":"markdown","source":["## Reading Text\n- `spark.read.text(..)`\n- Reads one line of text as a single column named `value`.\n- Is the basis for more complex file formats such as fixed-width text files."],"metadata":{}},{"cell_type":"markdown","source":["## Reading JDBC\n- `spark.read.jdbc(..)`\n- Requires one database connection per partition.\n- Has the potential to overwhelm the database.\n- Requires specification of a stride to properly balance partitions."],"metadata":{}}],"metadata":{"name":"7-8 File Formats","notebookId":3114109954854497},"nbformat":4,"nbformat_minor":0}
