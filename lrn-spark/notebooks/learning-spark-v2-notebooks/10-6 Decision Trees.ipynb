{"cells":[{"cell_type":"markdown","source":["d\n# Decision Trees\n\nIn the previous notebook, you were working with the parametric model, Linear Regression. We could do some more hyperparameter tuning with the linear regression model, but we're going to try tree based methods and see if our performance improves."],"metadata":{}},{"cell_type":"code","source":["filePath = \"/databricks-datasets/learning-spark-v2/sf-airbnb/sf-airbnb-clean.parquet\"\nairbnbDF = spark.read.parquet(filePath)\ntrainDF, testDF = airbnbDF.randomSplit([.8, .2], seed=42)"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":["## How to Handle Categorical Features?\n\nWe saw in the previous notebook that we can use StringIndexer/OneHotEncoderEstimator/VectorAssembler or RFormula.\n\n**However, for decision trees, and in particular, random forests, we should not OHE our variables.**\n\nWhy is that? Well, how the splits are made is different (you can see this when you visualize the tree) and the feature importance scores are not correct.\n\nFor random forests (which we will discuss shortly), the result can change dramatically. So instead of using RFormula, we are going to use just StringIndexer/VectorAssembler."],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.feature import StringIndexer\n\ncategoricalCols = [field for (field, dataType) in trainDF.dtypes if dataType == \"string\"]\nindexOutputCols = [x + \"Index\" for x in categoricalCols]\n\nstringIndexer = StringIndexer(inputCols=categoricalCols, outputCols=indexOutputCols, handleInvalid=\"skip\")"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":["## VectorAssembler\n\nLet's use the VectorAssembler to combine all of our categorical and numeric inputs [Python](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.VectorAssembler)/[Scala](https://spark.apache.org/docs/latest/api/scala/#org.apache.spark.ml.feature.VectorAssembler)."],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.feature import VectorAssembler\n\n# Filter for just numeric columns (and exclude price, our label)\nnumericCols = [field for (field, dataType) in trainDF.dtypes \n               if ((dataType == \"double\") & (field != \"price\"))]\n# Combine output of StringIndexer defined above and numeric columns\nassemblerInputs = indexOutputCols + numericCols\nvecAssembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":["## Decision Tree\n\nNow let's build a `DecisionTreeRegressor` with the default hyperparameters [Python](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.regression.DecisionTreeRegressor)/[Scala](https://spark.apache.org/docs/latest/api/scala/#org.apache.spark.ml.regression.DecisionTreeRegressor)."],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.regression import DecisionTreeRegressor\n\ndt = DecisionTreeRegressor(labelCol=\"price\")"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":["## Fit Pipeline"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml import Pipeline\n\n# Combine stages into pipeline\nstages = [stringIndexer, vecAssembler, dt]\npipeline = Pipeline(stages=stages)\n\n# Uncomment to perform fit\n# pipelineModel = pipeline.fit(trainDF)"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["## maxBins\n\nWhat is this parameter [maxBins](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.regression.DecisionTreeRegressor.maxBins)? Let's take a look at the PLANET implementation of distributed decision trees (which Spark uses) and compare it to this paper called [Yggdrasil](https://cs.stanford.edu/~matei/papers/2016/nips_yggdrasil.pdf) by Matei Zaharia and others. This will help explain the `maxBins` parameter."],"metadata":{}},{"cell_type":"markdown","source":["<img src=\"https://files.training.databricks.com/images/DistDecisionTrees.png\" height=500px>"],"metadata":{}},{"cell_type":"markdown","source":["In Spark, data is partitioned by row. So when it needs to make a split, each worker has to compute summary statistics for every feature for  each split point. Then these summary statistics have to be aggregated (via tree reduce) for a split to be made.\n\nThink about it: What if worker 1 had the value `32` but none of the others had it. How could you communicate how good of a split that would be? So, Spark has a maxBins parameter for discretizing continuous variables into buckets, but the number of buckets has to be as large as the number of categorical variables."],"metadata":{}},{"cell_type":"markdown","source":["Let's go ahead and increase maxBins to `40`."],"metadata":{}},{"cell_type":"code","source":["dt.setMaxBins(40)"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":["Take two."],"metadata":{}},{"cell_type":"code","source":["pipelineModel = pipeline.fit(trainDF)"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":["## Visualize the Decision Tree"],"metadata":{}},{"cell_type":"code","source":["dtModel = pipelineModel.stages[-1]\nprint(dtModel.toDebugString)"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":["## Feature Importance\n\nLet's go ahead and get the fitted decision tree model, and look at the feature importance scores."],"metadata":{}},{"cell_type":"code","source":["dtModel = pipelineModel.stages[-1]\ndtModel.featureImportances"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":["## Interpreting Feature Importance\n\nHmmm... it's a little hard to know what feature 4 vs 11 is. Given that the feature importance scores are \"small data\", let's use Pandas to help us recover the original column names."],"metadata":{}},{"cell_type":"code","source":["import pandas as pd\ndtModel = pipelineModel.stages[-1]\nfeatureImp = pd.DataFrame(\n  list(zip(vecAssembler.getInputCols(), dtModel.featureImportances)),\n  columns=[\"feature\", \"importance\"])\nfeatureImp.sort_values(by=\"importance\", ascending=False)"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":["## Apply model to test set"],"metadata":{}},{"cell_type":"code","source":["predDF = pipelineModel.transform(testDF)\n\ndisplay(predDF.select(\"features\", \"price\", \"prediction\").orderBy(\"price\", ascending=False))"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":["## Pitfall\n\nWhat if we get a massive Airbnb rental? It was 20 bedrooms and 20 bathrooms. What will a decision tree predict?\n\nIt turns out decision trees cannot predict any values larger than they were trained on. The max value in our training set was $10,000, so we can't predict any values larger than that (or technically any values larger than the )"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.evaluation import RegressionEvaluator\n\nregressionEvaluator = RegressionEvaluator(predictionCol=\"prediction\", \n                                          labelCol=\"price\", \n                                          metricName=\"rmse\")\n\nrmse = regressionEvaluator.evaluate(predDF)\nr2 = regressionEvaluator.setMetricName(\"r2\").evaluate(predDF)\nprint(f\"RMSE is {rmse}\")\nprint(f\"R2 is {r2}\")"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":["## Uh oh!\n\nThis model is worse than the linear regression model.\n\nIn the next few notebooks, let's look at hyperparameter tuning and ensemble models to improve upon the performance of our singular decision tree."],"metadata":{}}],"metadata":{"name":"10-6 Decision Trees","notebookId":3114109954854034},"nbformat":4,"nbformat_minor":0}
