{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorized User Defined Functions\n",
    "\n",
    "Let's compare the performance of UDFs, Vectorized UDFs, and built-in methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "sc = SparkContext('local', 'Ch7')\n",
    "\n",
    "# create a SparkSession\n",
    "spark = (SparkSession\n",
    "    .builder\n",
    "    .appName(\"ch7 example\")\n",
    "    .getOrCreate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by generating some dummy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %python\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col, count, rand, collect_list, explode, struct, count, pandas_udf\n",
    "\n",
    "df = (spark\n",
    "      .range(0, 10 * 1000 * 1000)\n",
    "      .withColumn('id', (col('id') / 1000).cast('integer'))\n",
    "      .withColumn('v', rand()))\n",
    "\n",
    "df.cache()\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+\n",
      "| id|                  v|\n",
      "+---+-------------------+\n",
      "|  0| 0.9109347581494166|\n",
      "|  0|  0.739109745589041|\n",
      "|  0|  0.920475809342571|\n",
      "|  0|0.33639462245798346|\n",
      "|  0|0.09574976989894335|\n",
      "|  0| 0.9508584574095044|\n",
      "|  0|0.09610891482282824|\n",
      "|  0| 0.2524175277094679|\n",
      "|  0| 0.5161393955497544|\n",
      "|  0|0.20049548061393263|\n",
      "|  0| 0.6430599957934754|\n",
      "|  0|0.10246413719352865|\n",
      "|  0| 0.3404484432709951|\n",
      "|  0| 0.4787482801467926|\n",
      "|  0| 0.6243570509377939|\n",
      "|  0|0.03769182053216935|\n",
      "|  0| 0.5113357452982034|\n",
      "|  0| 0.3064215506338288|\n",
      "|  0|  0.311064735952431|\n",
      "|  0|0.49032979464614346|\n",
      "+---+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incrementing a column by one\n",
    "\n",
    "Let's start off with a simple example of adding one to each value in our DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PySpark UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'udf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-2bcb790ae65f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# %python\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;34m@\u001b[0m\u001b[0mudf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"double\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplus_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'udf' is not defined"
     ]
    }
   ],
   "source": [
    "# %python\n",
    "# @udf(\"double\")\n",
    "def plus_one(v):\n",
    "    return v + 1\n",
    "\n",
    "%timeit -n1 -r1 df.withColumn('v', plus_one(df.v)).agg(count(col('v'))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternate Syntax (can also use in the SQL namespace now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(v)|\n",
      "+--------+\n",
      "|10000000|\n",
      "+--------+\n",
      "\n",
      "7.52 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "# %python\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "def plus_one(v):\n",
    "    return v + 1\n",
    "  \n",
    "spark.udf.register(\"plus_one_udf\", plus_one, DoubleType())\n",
    "\n",
    "%timeit -n1 -r1 df.selectExpr(\"id\", \"plus_one_udf(v) as v\").agg(count(col('v'))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scala UDF\n",
    "\n",
    "Yikes! That took awhile to add 1 to each value. Let's see how long this takes with a Scala UDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %python\n",
    "df.createOrReplaceTempView(\"df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-10-c3b448ebe0a7>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-10-c3b448ebe0a7>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    val df = spark.table(\"df\")\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "%scala\n",
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "val df = spark.table(\"df\")\n",
    "\n",
    "def plusOne: (Double => Double) = { v => v+1 }\n",
    "val plus_one = udf(plusOne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%scala\n",
    "df.withColumn(\"v\", plus_one($\"v\"))\n",
    "  .agg(count(col(\"v\")))\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow! That Scala UDF was a lot faster. However, as of Spark 2.3, there are Vectorized UDFs available in Python to help speed up the computation.\n",
    "\n",
    "* [Blog post](https://databricks.com/blog/2017/10/30/introducing-vectorized-udfs-for-pyspark.html)\n",
    "* [Documentation](https://spark.apache.org/docs/latest/sql-programming-guide.html#pyspark-usage-guide-for-pandas-with-apache-arrow)\n",
    "\n",
    "![Benchmark](https://databricks.com/wp-content/uploads/2017/10/image1-4.png)\n",
    "\n",
    "Vectorized UDFs utilize Apache Arrow to speed up computation. Let's see how that helps improve our processing time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Apache Arrow](https://arrow.apache.org/), is an in-memory columnar data format that is used in Spark to efficiently transfer data between JVM and Python processes. See more [here](https://spark.apache.org/docs/latest/sql-pyspark-pandas-with-arrow.html).\n",
    "\n",
    "Let's take a look at how long it takes to convert a Spark DataFrame to Pandas with and without Apache Arrow enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "%python\n",
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "\n",
    "%timeit -n1 -r1 df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "%python\n",
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"false\")\n",
    "\n",
    "%timeit -n1 -r1 df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorized UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "%python\n",
    "@pandas_udf('double')\n",
    "def vectorized_plus_one(v):\n",
    "    return v + 1\n",
    "\n",
    "%timeit -n1 -r1 df.withColumn('v', vectorized_plus_one(df.v)).agg(count(col('v'))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright! Still not as great as the Scala UDF, but at least it's better than the regular Python UDF!\n",
    "\n",
    "Here's some alternate syntax for the Pandas UDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "\n",
    "def vectorized_plus_one(v):\n",
    "    return v + 1\n",
    "\n",
    "vectorized_plus_one_udf = pandas_udf(vectorized_plus_one, \"double\")\n",
    "\n",
    "%timeit -n1 -r1 df.withColumn('v', vectorized_plus_one_udf(df.v)).agg(count(col('v'))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Built-in Method\n",
    "\n",
    "Let's compare the performance of the UDFs with using the built-in method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "%timeit -n1 -r1 df.withColumn('v', df.v + lit(1)).agg(count(col('v'))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing subtract mean\n",
    "\n",
    "Up above, we were working with Scalar return types. Now we can use grouped UDFs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PySpark UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.sql import Row\n",
    "import pandas as pd\n",
    "\n",
    "@udf(ArrayType(df.schema))\n",
    "def subtract_mean(rows):\n",
    "  vs = pd.Series([r.v for r in rows])\n",
    "  vs = vs - vs.mean()\n",
    "  return [Row(id=rows[i]['id'], v=float(vs[i])) for i in range(len(rows))]\n",
    "  \n",
    "%timeit -n1 -r1 (df.groupby('id').agg(collect_list(struct(df['id'], df['v'])).alias('rows')).withColumn('new_rows', subtract_mean(col('rows'))).withColumn('new_row', explode(col('new_rows'))).withColumn('id', col('new_row.id')).withColumn('v', col('new_row.v')).agg(count(col('v'))).show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorized UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.sql.functions import PandasUDFType\n",
    "\n",
    "@pandas_udf(df.schema, PandasUDFType.GROUPED_MAP)\n",
    "def vectorized_subtract_mean(pdf):\n",
    "\treturn pdf.assign(v=pdf.v - pdf.v.mean())\n",
    "\n",
    "%timeit -n1 -r1 df.groupby('id').apply(vectorized_subtract_mean).agg(count(col('v'))).show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "name": "7-4 Vectorized UDFs",
  "notebookId": 3114109954854469,
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
