{"cells":[{"cell_type":"markdown","source":["d\n# Tracking Models with MLflow\n\nMLflow is pre-installed on the Databricks Runtime for ML. If you are not using the ML Runtime, you will need to install mlflow."],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml import Pipeline\nfrom pyspark.ml.feature import StringIndexer, VectorAssembler\nfrom pyspark.ml.regression import RandomForestRegressor\nfrom pyspark.ml.evaluation import RegressionEvaluator\n\nfilePath = \"/databricks-datasets/learning-spark-v2/sf-airbnb/sf-airbnb-clean.parquet\"\nairbnbDF = spark.read.parquet(filePath)\n(trainDF, testDF) = airbnbDF.randomSplit([.8, .2], seed=42)\n\ncategoricalCols = [field for (field, dataType) in trainDF.dtypes \n                   if dataType == \"string\"]\nindexOutputCols = [x + \"Index\" for x in categoricalCols]\nstringIndexer = StringIndexer(inputCols=categoricalCols, \n                              outputCols=indexOutputCols, \n                              handleInvalid=\"skip\")\n\nnumericCols = [field for (field, dataType) in trainDF.dtypes \n               if ((dataType == \"double\") & (field != \"price\"))]\nassemblerInputs = indexOutputCols + numericCols\nvecAssembler = VectorAssembler(inputCols=assemblerInputs, \n                               outputCol=\"features\")\n\nrf = RandomForestRegressor(labelCol=\"price\", maxBins=40, maxDepth=5, \n                           numTrees=100, seed=42)\n\npipeline = Pipeline(stages=[stringIndexer, vecAssembler, rf])"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":["## MLflow"],"metadata":{}},{"cell_type":"code","source":["import mlflow\nimport mlflow.spark\nimport pandas as pd\n\nwith mlflow.start_run(run_name=\"random-forest\") as run:\n  # Log params: Num Trees and Max Depth\n  mlflow.log_param(\"num_trees\", rf.getNumTrees())\n  mlflow.log_param(\"max_depth\", rf.getMaxDepth())\n \n  # Log model\n  pipelineModel = pipeline.fit(trainDF)\n  mlflow.spark.log_model(pipelineModel, \"model\")\n\n  # Log metrics: RMSE and R2\n  predDF = pipelineModel.transform(testDF)\n  regressionEvaluator = RegressionEvaluator(predictionCol=\"prediction\", \n                                            labelCol=\"price\")\n  rmse = regressionEvaluator.setMetricName(\"rmse\").evaluate(predDF)\n  r2 = regressionEvaluator.setMetricName(\"r2\").evaluate(predDF)\n  mlflow.log_metrics({\"rmse\": rmse, \"r2\": r2})\n\n  # Log artifact: Feature Importance Scores\n  rfModel = pipelineModel.stages[-1]\n  pandasDF = (pd.DataFrame(list(zip(vecAssembler.getInputCols(), \n                                    rfModel.featureImportances)), \n                          columns=[\"feature\", \"importance\"])\n              .sort_values(by=\"importance\", ascending=False))\n  # First write to local filesystem, then tell MLflow where to find that file\n  pandasDF.to_csv(\"feature-importance.csv\", index=False)\n  mlflow.log_artifact(\"feature-importance.csv\")"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":["## MLflowClient"],"metadata":{}},{"cell_type":"code","source":["from mlflow.tracking import MlflowClient\n\nclient = MlflowClient()\nruns = client.search_runs(run.info.experiment_id,\n                          order_by=[\"attributes.start_time desc\"], \n                          max_results=1)\nrun_id = runs[0].info.run_id\nruns[0].data.metrics"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":["%md ## Generate Batch Predictions\n\nLet's load the model back in to generate batch predictions"],"metadata":{}},{"cell_type":"code","source":["# Load saved model with MLflow\npipelineModel = mlflow.spark.load_model(f\"runs:/{run_id}/model\")\n\n# Generate Predictions\ninputPath = \"/databricks-datasets/learning-spark-v2/sf-airbnb/sf-airbnb-clean.parquet\"\ninputDF = spark.read.parquet(inputPath)\npredDF = pipelineModel.transform(inputDF)"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":["## Generate Streaming Predictions\n\nWe can do the same thing to generate streaming predictions."],"metadata":{}},{"cell_type":"code","source":["# Load saved model with MLflow\npipelineModel = mlflow.spark.load_model(f\"runs:/{run_id}/model\")\n\n# Set up simulated streaming data\nrepartitionedPath = \"/databricks-datasets/learning-spark-v2/sf-airbnb/sf-airbnb-clean-100p.parquet\"\nschema = spark.read.parquet(repartitionedPath).schema\n\nstreamingData = (spark\n                 .readStream\n                 .schema(schema) # Can set the schema this way\n                 .option(\"maxFilesPerTrigger\", 1)\n                 .parquet(repartitionedPath))\n\n# Generate Predictions\nstreamPred = pipelineModel.transform(streamingData)\n\n# Uncomment the line below to see the streaming predictions\n# display(streamPred)\n\n# Just remember to stop your stream at the end!\n# streamPred.exit()\n"],"metadata":{},"outputs":[],"execution_count":10}],"metadata":{"name":"11-1 MLflow","notebookId":3114109954854638},"nbformat":4,"nbformat_minor":0}
