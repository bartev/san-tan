{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description\n",
    "----\n",
    "Getting started with mleap\n",
    "\n",
    "https://mleap-docs.combust.ml/getting-started/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Typical MLeap Workflow\n",
    "\n",
    "A typical MLeap workflow consists of 3 parts:\n",
    "\n",
    "1. Training: Write your ML Pipelines the same way you do today\n",
    "2. Serialization: Serialize all of the data processing (ml pipeline) and the algorithms to Bundle.ML\n",
    "3. Execution: Use MLeap runtime to execute your serialized pipeline without dependencies on Spark or Scikit (you'll still need TensorFlow binaries)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serialization\n",
    "\n",
    "Once you have your pipeline trained, MLeap provides functionality to serialize the entire ML/Data Pipeline and your trained algorithm (linear models, tree-based models, neural networks) to Bundle.ML. Serialization generates something called a `bundle` which is a physical representation of your pipeline and algorithm that you can deploy, share, view all of the pieces of the pipeline.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Execution\n",
    "\n",
    "The goal of MLeap was initially to enable scoring of Spark's ML pipelines without the dependency on Spark. That functionality is powered by MLeap Runtime, which loads your serialized bundle and executes it on incoming dataframes (LeapFrames).\n",
    "\n",
    "Did we mention that MLeap Runtime is extremely fast? We have recorded benchmarks of micro-second execution on LeapFrames and sub-5ms response times when part of a RESTful API service.\n",
    "\n",
    "**Note: As of right now, MLeap runtime is only provided as a Java/Scala library, but we do plan to add python bindings in the future.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load libraries and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# imports for adhoc notebooks\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
    "\n",
    "# magics\n",
    "%load_ext blackcellmagic\n",
    "# start cell with `%% black` to format using `black`\n",
    "\n",
    "%load_ext autoreload\n",
    "# start cell with `%autoreload` to reload module\n",
    "# https://ipython.org/ipython-doc/stable/config/extensions/autoreload.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mleap import pyspark\n",
    "\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from mleap.pyspark.spark_support import SimpleSparkSerializer\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler, OneHotEncoder, StringIndexer\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import src.spark.utils as uts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "mleap_jars = uts.get_mleap_jars()\n",
    "# avro_jar = '/Users/bartev/dev/github-bv/san-tan/lrn-spark/references/spark-avro_2.11-4.0.0.jar'\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName('mleap-ex')\n",
    "    .config('spark.jars', mleap_jars)\n",
    "#     .config('spark.jars', f\"{mleap_jars},{avro_jar}\")\n",
    "    .getOrCreate()\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.17:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>mleap-ex</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x114a940b8>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Simple MLeap example\n",
    "----\n",
    "https://mleap-docs.combust.ml/py-spark/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Create simple spark pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Imports MLeap serialization functionality for PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import mleap.pyspark\n",
    "from mleap.pyspark.spark_support import SimpleSparkSerializer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Import standard PySpark Transformers and packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, StandardScaler, OneHotEncoder, StringIndexer\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Create a test data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name='Alice', age=1), Row(name='Bob', age=2)]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = [('Alice', 1), ('Bob', 2)]\n",
    "rdd = sc.parallelize(l)\n",
    "Person = Row('name', 'age')\n",
    "person = rdd.map(lambda r: Person(*r))\n",
    "df2 = spark.createDataFrame(person)\n",
    "df2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| name|age|\n",
      "+-----+---+\n",
      "|Alice|  1|\n",
      "|  Bob|  2|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Build a very simple pipeline using two transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "string_indexer = StringIndexer(inputCol='name', outputCol='name_string_index')\n",
    "\n",
    "feature_assembler = VectorAssembler(inputCols=[string_indexer.getOutputCol()],\n",
    "                                    outputCol='features')\n",
    "\n",
    "feature_pipeline = [string_indexer, feature_assembler]\n",
    "\n",
    "featurePipeline = Pipeline(stages=feature_pipeline)\n",
    "\n",
    "fittedPipeline = featurePipeline.fit(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.ml.pipeline.PipelineModel"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(fittedPipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fittedPipeline.serializeToBundle??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Serialize to zip file\n",
    "----\n",
    "In order to serialize to a zip file, make sure the URI begins with jar:file and ends with a .zip.\n",
    "\n",
    "For example `jar:file:/tmp/mleap-bundle.zip.`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "current_path = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/bartev/dev/github-bv/san-tan/lrn-spark/notebooks'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jar:file:/Users/bartev/dev/github-bv/san-tan/lrn-spark/notebooks/foo.zip'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%autoreload\n",
    "uts.create_mleap_fname('foo.zip', '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jar:file:/Users/bartev/dev/github-bv/san-tan/lrn-spark/notebooks/pyspark.example.zip\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o911.serializeToBundle.\n: java.nio.file.FileAlreadyExistsException: root/VectorAssembler_ce2795015298.node/model.json\n\tat com.sun.nio.zipfs.ZipFileSystem.newOutputStream(ZipFileSystem.java:516)\n\tat com.sun.nio.zipfs.ZipPath.newOutputStream(ZipPath.java:790)\n\tat com.sun.nio.zipfs.ZipFileSystemProvider.newOutputStream(ZipFileSystemProvider.java:285)\n\tat java.nio.file.Files.newOutputStream(Files.java:216)\n\tat java.nio.file.Files.write(Files.java:3292)\n\tat ml.combust.bundle.serializer.JsonFormatModelSerializer.write(ModelSerializer.scala:48)\n\tat ml.combust.bundle.serializer.ModelSerializer$$anonfun$write$2$$anonfun$apply$1.apply$mcV$sp(ModelSerializer.scala:89)\n\tat ml.combust.bundle.serializer.ModelSerializer$$anonfun$write$2$$anonfun$apply$1.apply(ModelSerializer.scala:89)\n\tat ml.combust.bundle.serializer.ModelSerializer$$anonfun$write$2$$anonfun$apply$1.apply(ModelSerializer.scala:89)\n\tat scala.util.Try$.apply(Try.scala:192)\n\tat ml.combust.bundle.serializer.ModelSerializer$$anonfun$write$2.apply(ModelSerializer.scala:89)\n\tat ml.combust.bundle.serializer.ModelSerializer$$anonfun$write$2.apply(ModelSerializer.scala:89)\n\tat scala.util.Success.flatMap(Try.scala:231)\n\tat ml.combust.bundle.serializer.ModelSerializer.write(ModelSerializer.scala:88)\n\tat ml.combust.bundle.serializer.NodeSerializer$$anonfun$write$1.apply(NodeSerializer.scala:85)\n\tat ml.combust.bundle.serializer.NodeSerializer$$anonfun$write$1.apply(NodeSerializer.scala:81)\n\tat scala.util.Try$.apply(Try.scala:192)\n\tat ml.combust.bundle.serializer.NodeSerializer.write(NodeSerializer.scala:81)\n\tat ml.combust.bundle.serializer.GraphSerializer$$anonfun$writeNode$1.apply(GraphSerializer.scala:34)\n\tat ml.combust.bundle.serializer.GraphSerializer$$anonfun$writeNode$1.apply(GraphSerializer.scala:30)\n\tat scala.util.Try$.apply(Try.scala:192)\n\tat ml.combust.bundle.serializer.GraphSerializer.writeNode(GraphSerializer.scala:30)\n\tat ml.combust.bundle.serializer.GraphSerializer$$anonfun$write$2.apply(GraphSerializer.scala:21)\n\tat ml.combust.bundle.serializer.GraphSerializer$$anonfun$write$2.apply(GraphSerializer.scala:21)\n\tat scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:57)\n\tat scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:66)\n\tat scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:35)\n\tat ml.combust.bundle.serializer.GraphSerializer.write(GraphSerializer.scala:20)\n\tat org.apache.spark.ml.bundle.ops.PipelineOp$$anon$1.store(PipelineOp.scala:21)\n\tat org.apache.spark.ml.bundle.ops.PipelineOp$$anon$1.store(PipelineOp.scala:14)\n\tat ml.combust.bundle.serializer.ModelSerializer$$anonfun$write$1.apply(ModelSerializer.scala:87)\n\tat ml.combust.bundle.serializer.ModelSerializer$$anonfun$write$1.apply(ModelSerializer.scala:83)\n\tat scala.util.Try$.apply(Try.scala:192)\n\tat ml.combust.bundle.serializer.ModelSerializer.write(ModelSerializer.scala:83)\n\tat ml.combust.bundle.serializer.NodeSerializer$$anonfun$write$1.apply(NodeSerializer.scala:85)\n\tat ml.combust.bundle.serializer.NodeSerializer$$anonfun$write$1.apply(NodeSerializer.scala:81)\n\tat scala.util.Try$.apply(Try.scala:192)\n\tat ml.combust.bundle.serializer.NodeSerializer.write(NodeSerializer.scala:81)\n\tat ml.combust.bundle.serializer.BundleSerializer$$anonfun$write$1.apply(BundleSerializer.scala:34)\n\tat ml.combust.bundle.serializer.BundleSerializer$$anonfun$write$1.apply(BundleSerializer.scala:29)\n\tat scala.util.Try$.apply(Try.scala:192)\n\tat ml.combust.bundle.serializer.BundleSerializer.write(BundleSerializer.scala:29)\n\tat ml.combust.bundle.BundleWriter.save(BundleWriter.scala:31)\n\tat ml.combust.mleap.spark.SimpleSparkSerializer$$anonfun$serializeToBundleWithFormat$2.apply(SimpleSparkSerializer.scala:26)\n\tat ml.combust.mleap.spark.SimpleSparkSerializer$$anonfun$serializeToBundleWithFormat$2.apply(SimpleSparkSerializer.scala:25)\n\tat resource.AbstractManagedResource$$anonfun$5.apply(AbstractManagedResource.scala:88)\n\tat scala.util.control.Exception$Catch$$anonfun$either$1.apply(Exception.scala:125)\n\tat scala.util.control.Exception$Catch$$anonfun$either$1.apply(Exception.scala:125)\n\tat scala.util.control.Exception$Catch.apply(Exception.scala:103)\n\tat scala.util.control.Exception$Catch.either(Exception.scala:125)\n\tat resource.AbstractManagedResource.acquireFor(AbstractManagedResource.scala:88)\n\tat resource.ManagedResourceOperations$class.apply(ManagedResourceOperations.scala:26)\n\tat resource.AbstractManagedResource.apply(AbstractManagedResource.scala:50)\n\tat resource.DeferredExtractableManagedResource$$anonfun$tried$1.apply(AbstractManagedResource.scala:33)\n\tat scala.util.Try$.apply(Try.scala:192)\n\tat resource.DeferredExtractableManagedResource.tried(AbstractManagedResource.scala:33)\n\tat ml.combust.mleap.spark.SimpleSparkSerializer.serializeToBundleWithFormat(SimpleSparkSerializer.scala:27)\n\tat ml.combust.mleap.spark.SimpleSparkSerializer.serializeToBundle(SimpleSparkSerializer.scala:17)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-101-8bfd658ce16e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m fittedPipeline.serializeToBundle(fname,\n\u001b[0;32m----> 4\u001b[0;31m                                 fittedPipeline.transform(df2))\n\u001b[0m",
      "\u001b[0;32m~/.venvs3/lrnpyspark/lib/python3.7/site-packages/mleap/pyspark/spark_support.py\u001b[0m in \u001b[0;36mserializeToBundle\u001b[0;34m(self, path, dataset)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mserializeToBundle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mserializer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSimpleSparkSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mserializer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserializeToBundle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.venvs3/lrnpyspark/lib/python3.7/site-packages/mleap/pyspark/spark_support.py\u001b[0m in \u001b[0;36mserializeToBundle\u001b[0;34m(self, transformer, path, dataset)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mserializeToBundle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserializeToBundle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeserializeFromBundle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.venvs3/lrnpyspark/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.venvs3/lrnpyspark/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.venvs3/lrnpyspark/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o911.serializeToBundle.\n: java.nio.file.FileAlreadyExistsException: root/VectorAssembler_ce2795015298.node/model.json\n\tat com.sun.nio.zipfs.ZipFileSystem.newOutputStream(ZipFileSystem.java:516)\n\tat com.sun.nio.zipfs.ZipPath.newOutputStream(ZipPath.java:790)\n\tat com.sun.nio.zipfs.ZipFileSystemProvider.newOutputStream(ZipFileSystemProvider.java:285)\n\tat java.nio.file.Files.newOutputStream(Files.java:216)\n\tat java.nio.file.Files.write(Files.java:3292)\n\tat ml.combust.bundle.serializer.JsonFormatModelSerializer.write(ModelSerializer.scala:48)\n\tat ml.combust.bundle.serializer.ModelSerializer$$anonfun$write$2$$anonfun$apply$1.apply$mcV$sp(ModelSerializer.scala:89)\n\tat ml.combust.bundle.serializer.ModelSerializer$$anonfun$write$2$$anonfun$apply$1.apply(ModelSerializer.scala:89)\n\tat ml.combust.bundle.serializer.ModelSerializer$$anonfun$write$2$$anonfun$apply$1.apply(ModelSerializer.scala:89)\n\tat scala.util.Try$.apply(Try.scala:192)\n\tat ml.combust.bundle.serializer.ModelSerializer$$anonfun$write$2.apply(ModelSerializer.scala:89)\n\tat ml.combust.bundle.serializer.ModelSerializer$$anonfun$write$2.apply(ModelSerializer.scala:89)\n\tat scala.util.Success.flatMap(Try.scala:231)\n\tat ml.combust.bundle.serializer.ModelSerializer.write(ModelSerializer.scala:88)\n\tat ml.combust.bundle.serializer.NodeSerializer$$anonfun$write$1.apply(NodeSerializer.scala:85)\n\tat ml.combust.bundle.serializer.NodeSerializer$$anonfun$write$1.apply(NodeSerializer.scala:81)\n\tat scala.util.Try$.apply(Try.scala:192)\n\tat ml.combust.bundle.serializer.NodeSerializer.write(NodeSerializer.scala:81)\n\tat ml.combust.bundle.serializer.GraphSerializer$$anonfun$writeNode$1.apply(GraphSerializer.scala:34)\n\tat ml.combust.bundle.serializer.GraphSerializer$$anonfun$writeNode$1.apply(GraphSerializer.scala:30)\n\tat scala.util.Try$.apply(Try.scala:192)\n\tat ml.combust.bundle.serializer.GraphSerializer.writeNode(GraphSerializer.scala:30)\n\tat ml.combust.bundle.serializer.GraphSerializer$$anonfun$write$2.apply(GraphSerializer.scala:21)\n\tat ml.combust.bundle.serializer.GraphSerializer$$anonfun$write$2.apply(GraphSerializer.scala:21)\n\tat scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:57)\n\tat scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:66)\n\tat scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:35)\n\tat ml.combust.bundle.serializer.GraphSerializer.write(GraphSerializer.scala:20)\n\tat org.apache.spark.ml.bundle.ops.PipelineOp$$anon$1.store(PipelineOp.scala:21)\n\tat org.apache.spark.ml.bundle.ops.PipelineOp$$anon$1.store(PipelineOp.scala:14)\n\tat ml.combust.bundle.serializer.ModelSerializer$$anonfun$write$1.apply(ModelSerializer.scala:87)\n\tat ml.combust.bundle.serializer.ModelSerializer$$anonfun$write$1.apply(ModelSerializer.scala:83)\n\tat scala.util.Try$.apply(Try.scala:192)\n\tat ml.combust.bundle.serializer.ModelSerializer.write(ModelSerializer.scala:83)\n\tat ml.combust.bundle.serializer.NodeSerializer$$anonfun$write$1.apply(NodeSerializer.scala:85)\n\tat ml.combust.bundle.serializer.NodeSerializer$$anonfun$write$1.apply(NodeSerializer.scala:81)\n\tat scala.util.Try$.apply(Try.scala:192)\n\tat ml.combust.bundle.serializer.NodeSerializer.write(NodeSerializer.scala:81)\n\tat ml.combust.bundle.serializer.BundleSerializer$$anonfun$write$1.apply(BundleSerializer.scala:34)\n\tat ml.combust.bundle.serializer.BundleSerializer$$anonfun$write$1.apply(BundleSerializer.scala:29)\n\tat scala.util.Try$.apply(Try.scala:192)\n\tat ml.combust.bundle.serializer.BundleSerializer.write(BundleSerializer.scala:29)\n\tat ml.combust.bundle.BundleWriter.save(BundleWriter.scala:31)\n\tat ml.combust.mleap.spark.SimpleSparkSerializer$$anonfun$serializeToBundleWithFormat$2.apply(SimpleSparkSerializer.scala:26)\n\tat ml.combust.mleap.spark.SimpleSparkSerializer$$anonfun$serializeToBundleWithFormat$2.apply(SimpleSparkSerializer.scala:25)\n\tat resource.AbstractManagedResource$$anonfun$5.apply(AbstractManagedResource.scala:88)\n\tat scala.util.control.Exception$Catch$$anonfun$either$1.apply(Exception.scala:125)\n\tat scala.util.control.Exception$Catch$$anonfun$either$1.apply(Exception.scala:125)\n\tat scala.util.control.Exception$Catch.apply(Exception.scala:103)\n\tat scala.util.control.Exception$Catch.either(Exception.scala:125)\n\tat resource.AbstractManagedResource.acquireFor(AbstractManagedResource.scala:88)\n\tat resource.ManagedResourceOperations$class.apply(ManagedResourceOperations.scala:26)\n\tat resource.AbstractManagedResource.apply(AbstractManagedResource.scala:50)\n\tat resource.DeferredExtractableManagedResource$$anonfun$tried$1.apply(AbstractManagedResource.scala:33)\n\tat scala.util.Try$.apply(Try.scala:192)\n\tat resource.DeferredExtractableManagedResource.tried(AbstractManagedResource.scala:33)\n\tat ml.combust.mleap.spark.SimpleSparkSerializer.serializeToBundleWithFormat(SimpleSparkSerializer.scala:27)\n\tat ml.combust.mleap.spark.SimpleSparkSerializer.serializeToBundle(SimpleSparkSerializer.scala:17)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "fname = uts.create_mleap_fname('pyspark.example.zip', '.')\n",
    "print(fname)\n",
    "fittedPipeline.serializeToBundle(fname,\n",
    "                                fittedPipeline.transform(df2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Databricks AirBnB example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df = spark.read.format(\"com.databricks.spark.avro\").load(\"file:////tmp/airbnb.avro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option('inferSchema','true')\n",
    "    .load(\"/Users/bartev/dev/gitpie/mleap-demo/data/airbnb.csv\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id',\n",
       " 'name',\n",
       " 'price',\n",
       " 'bedrooms',\n",
       " 'bathrooms',\n",
       " 'room_type',\n",
       " 'square_feet',\n",
       " 'host_is_superhost',\n",
       " 'state',\n",
       " 'cancellation_policy',\n",
       " 'security_deposit',\n",
       " 'cleaning_fee',\n",
       " 'extra_people',\n",
       " 'number_of_reviews',\n",
       " 'price_per_bedroom',\n",
       " 'review_scores_rating',\n",
       " 'instant_bookable']"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+-----+--------+---------+---------------+-----------+-----------------+---------+-------------------+----------------+------------+------------+-----------------+-----------------+--------------------+----------------+\n",
      "|     id|                name|price|bedrooms|bathrooms|      room_type|square_feet|host_is_superhost|    state|cancellation_policy|security_deposit|cleaning_fee|extra_people|number_of_reviews|price_per_bedroom|review_scores_rating|instant_bookable|\n",
      "+-------+--------------------+-----+--------+---------+---------------+-----------+-----------------+---------+-------------------+----------------+------------+------------+-----------------+-----------------+--------------------+----------------+\n",
      "|1949687|Delectable Victor...| 80.0|     1.0|      1.0|Entire home/apt|       null|              0.0|   London|           moderate|           100.0|        20.0|        10.0|                8|             80.0|                94.0|             0.0|\n",
      "|3863509|Fully Furnished 3...| 40.0|     1.0|      1.0|   Private room|       null|              0.0|   London|           flexible|             0.0|         0.0|         0.0|                5|             40.0|                55.0|             0.0|\n",
      "|1988980|Cozy Double Room ...| 35.0|     1.0|      1.0|   Private room|       null|              0.0|Greenwich|           flexible|             0.0|         5.0|        10.0|               32|             35.0|                89.0|             0.0|\n",
      "|2347198|Double Room In Ce...| 42.0|     1.0|      1.0|   Private room|       null|              0.0|   London|             strict|             0.0|         0.0|        15.0|               24|             42.0|                71.0|             0.0|\n",
      "| 144337|Fast WIFI Breakfa...|200.0|     1.0|      1.5|   Private room|      250.0|              0.0|   London|             strict|           300.0|         0.0|        20.0|               24|            200.0|                84.0|             0.0|\n",
      "|1372647|Double Room Queen...| 75.0|     1.0|      1.0|   Private room|       null|              0.0|   London|           flexible|             0.0|         0.0|        10.0|                3|             75.0|               100.0|             0.0|\n",
      "|2440394|Penthouse bedroom...| 70.0|     1.0|      1.0|   Private room|       null|              1.0|   London|           moderate|           100.0|        30.0|         0.0|                3|             70.0|               100.0|             0.0|\n",
      "|1949687|Delectable Victor...| 80.0|     1.0|      1.0|Entire home/apt|       null|              0.0|   London|           moderate|           100.0|        20.0|        10.0|                8|             80.0|                94.0|             0.0|\n",
      "|3863509|Fully Furnished 3...| 40.0|     1.0|      1.0|   Private room|       null|              0.0|   London|           flexible|             0.0|         0.0|         0.0|                5|             40.0|                55.0|             0.0|\n",
      "|1988980|Cozy Double Room ...| 35.0|     1.0|      1.0|   Private room|       null|              0.0|Greenwich|           flexible|             0.0|         5.0|        10.0|               32|             35.0|                89.0|             0.0|\n",
      "|2347198|Double Room In Ce...| 42.0|     1.0|      1.0|   Private room|       null|              0.0|   London|             strict|             0.0|         0.0|        15.0|               24|             42.0|                71.0|             0.0|\n",
      "| 144337|Fast WIFI Breakfa...|200.0|     1.0|      1.5|   Private room|      250.0|              0.0|   London|             strict|           300.0|         0.0|        20.0|               24|            200.0|                84.0|             0.0|\n",
      "|1372647|Double Room Queen...| 75.0|     1.0|      1.0|   Private room|       null|              0.0|   London|           flexible|             0.0|         0.0|        10.0|                3|             75.0|               100.0|             0.0|\n",
      "|2440394|Penthouse bedroom...| 70.0|     1.0|      1.0|   Private room|       null|              0.0|   London|           moderate|           100.0|        30.0|         0.0|                3|             70.0|               100.0|             0.0|\n",
      "|4754275|    Leafy town house|110.0|     1.0|      1.0|Entire home/apt|       null|              0.0|      VIC|           moderate|             0.0|        50.0|         0.0|                3|            110.0|                87.0|             0.0|\n",
      "|8154300|Bright, relaxed, ...| 91.0|     2.0|      1.0|Entire home/apt|       null|              0.0| Victoria|           moderate|           170.0|         0.0|         0.0|                2|             45.5|               100.0|             0.0|\n",
      "|8743955|Inner City Wareho...|191.0|     2.0|      2.5|Entire home/apt|       null|              0.0|      VIC|           flexible|             0.0|        30.0|         0.0|                2|             95.5|               100.0|             0.0|\n",
      "|1837433|Comfortable room ...| 60.0|     1.0|      1.0|   Private room|       null|              0.0|      VIC|           flexible|             0.0|        10.0|         0.0|               17|             60.0|                91.0|             0.0|\n",
      "|3372790|Home in prime Mel...|121.0|     2.0|      1.0|Entire home/apt|       null|              0.0|      VIC|           flexible|           200.0|        25.0|         0.0|                2|             60.5|                90.0|             0.0|\n",
      "|8247270|Triple+Ensuite+Wi...| 95.0|     1.0|      1.0|   Private room|       null|              0.0|      VIC|             strict|             0.0|        29.0|         0.0|                7|             95.0|               100.0|             1.0|\n",
      "+-------+--------------------+-----+--------+---------+---------------+-----------+-----------------+---------+-------------------+----------------+------------+------------+-----------------+-----------------+--------------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('id', 'int'),\n",
       " ('name', 'string'),\n",
       " ('price', 'double'),\n",
       " ('bedrooms', 'double'),\n",
       " ('bathrooms', 'double'),\n",
       " ('room_type', 'string'),\n",
       " ('square_feet', 'double'),\n",
       " ('host_is_superhost', 'double'),\n",
       " ('state', 'string'),\n",
       " ('cancellation_policy', 'string'),\n",
       " ('security_deposit', 'double'),\n",
       " ('cleaning_fee', 'double'),\n",
       " ('extra_people', 'double'),\n",
       " ('number_of_reviews', 'int'),\n",
       " ('price_per_bedroom', 'double'),\n",
       " ('review_scores_rating', 'double'),\n",
       " ('instant_bookable', 'double')]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetFiltered = (df.filter(\"price >= 50\")\n",
    "    .filter(\"price <= 750\")\n",
    "    .filter('bathrooms > 0.0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "389255\n",
      "321588\n"
     ]
    }
   ],
   "source": [
    "print(df.count())\n",
    "print(datasetFiltered.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id',\n",
       " 'name',\n",
       " 'price',\n",
       " 'bedrooms',\n",
       " 'bathrooms',\n",
       " 'room_type',\n",
       " 'square_feet',\n",
       " 'host_is_superhost',\n",
       " 'state',\n",
       " 'cancellation_policy',\n",
       " 'security_deposit',\n",
       " 'cleaning_fee',\n",
       " 'extra_people',\n",
       " 'number_of_reviews',\n",
       " 'price_per_bedroom',\n",
       " 'review_scores_rating',\n",
       " 'instant_bookable']"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasetFiltered.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "datasetFiltered.registerTempTable('df')\n",
    "\n",
    "q = \"\"\"\n",
    "select\n",
    "    id,\n",
    "    case when state in ('NY', 'CA', 'London', 'Berlin',\n",
    "        'TX', 'IL', 'OR', 'DC', 'WA')\n",
    "        then state\n",
    "        else 'Other'\n",
    "        end as state,\n",
    "    price,\n",
    "    bathrooms,\n",
    "    bedrooms,\n",
    "    room_type,\n",
    "    host_is_superhost,\n",
    "    cancellation_policy,\n",
    "    case when security_deposit is null\n",
    "        then 0.0\n",
    "        else security_deposit\n",
    "        end as security_deposit,\n",
    "    price_per_bedroom,\n",
    "    case when number_of_reviews is null\n",
    "        then 0.0\n",
    "        else number_of_reviews\n",
    "        end as number_of_reviews,\n",
    "    case when extra_people is null\n",
    "        then 0.0\n",
    "        else extra_people\n",
    "        end as extra_people,\n",
    "    instant_bookable,\n",
    "    case when cleaning_fee is null\n",
    "        then 0.0\n",
    "        else cleaning_fee\n",
    "        end as cleaning_fee,\n",
    "    case when review_scores_rating is null\n",
    "        then 0.0\n",
    "        else review_scores_rating\n",
    "        end as review_scores_rating,\n",
    "    case when square_feet is not null and square_feet > 100\n",
    "        then square_feet\n",
    "        when (square_feet is null or square_feet <=100)\n",
    "            and (bedrooms is null or bedrooms = 0)\n",
    "        then 350.0\n",
    "        else 380 * bedrooms        \n",
    "        end as square_feet,\n",
    "    case when bathrooms >= 2\n",
    "        then 1.0\n",
    "        else 0.0\n",
    "        end as n_bathrooms_more_than_two\n",
    "from df\n",
    "where bedrooms is not null\n",
    "        \"\"\"\n",
    "datasetImputed = spark.sql(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+-------------------+------------------+\n",
      "|summary|       square_feet|             price|          bedrooms|          bathrooms|      cleaning_fee|\n",
      "+-------+------------------+------------------+------------------+-------------------+------------------+\n",
      "|  count|            321588|            321588|            321588|             321588|            321588|\n",
      "|   mean| 546.7441757777032|131.54961006007687|1.3352426085550455|  1.199068373198005| 37.64188340360959|\n",
      "| stddev|363.39839582373594| 90.10912788720096|0.8466586601060732|0.48305900512627586|42.642377914845966|\n",
      "|    min|             104.0|              50.0|               0.0|                0.5|               0.0|\n",
      "|    max|           32292.0|             750.0|              10.0|                8.0|             700.0|\n",
      "+-------+------------------+------------------+------------------+-------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    datasetImputed.select(\n",
    "        \"square_feet\", \"price\", \"bedrooms\", \"bathrooms\", \"cleaning_fee\"\n",
    "    )\n",
    "    .describe()\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Look at some summary statistics of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+---------+---------+\n",
      "|        state|    n|avg_price|max_price|\n",
      "+-------------+-----+---------+---------+\n",
      "|           NY|48362|   146.75|    750.0|\n",
      "|           CA|44716|   158.76|    750.0|\n",
      "|Île-de-France|40732|   107.74|    750.0|\n",
      "|       London|17542|   117.72|    750.0|\n",
      "|          NSW|14416|   167.96|    750.0|\n",
      "|       Berlin|13098|    81.01|    650.0|\n",
      "|Noord-Holland| 8890|   128.56|    750.0|\n",
      "|          VIC| 8636|   144.49|    750.0|\n",
      "|North Holland| 7636|   134.60|    700.0|\n",
      "|           IL| 7544|   141.85|    750.0|\n",
      "|           ON| 7186|   129.05|    750.0|\n",
      "|           TX| 6702|   196.59|    750.0|\n",
      "|           WA| 5858|   132.48|    750.0|\n",
      "|    Catalonia| 5748|   106.39|    720.0|\n",
      "|           BC| 5522|   133.14|    750.0|\n",
      "|           DC| 5476|   136.56|    720.0|\n",
      "|       Québec| 5116|   104.98|    700.0|\n",
      "|    Catalunya| 4570|    99.36|    675.0|\n",
      "|       Veneto| 4486|   131.71|    700.0|\n",
      "|           OR| 4330|   114.02|    700.0|\n",
      "+-------------+-----+---------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# most popular states\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "select\n",
    "    state,\n",
    "    count(*) as n,\n",
    "    cast(avg(price) as decimal(12,2)) as avg_price,\n",
    "    max(price) as max_price\n",
    "from df\n",
    "group by state\n",
    "order by n desc\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>state</th>\n",
       "      <th>price</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>room_type</th>\n",
       "      <th>host_is_superhost</th>\n",
       "      <th>cancellation_policy</th>\n",
       "      <th>security_deposit</th>\n",
       "      <th>price_per_bedroom</th>\n",
       "      <th>number_of_reviews</th>\n",
       "      <th>extra_people</th>\n",
       "      <th>instant_bookable</th>\n",
       "      <th>cleaning_fee</th>\n",
       "      <th>review_scores_rating</th>\n",
       "      <th>square_feet</th>\n",
       "      <th>n_bathrooms_more_than_two</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1949687</td>\n",
       "      <td>London</td>\n",
       "      <td>80.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>Entire home/apt</td>\n",
       "      <td>0.000</td>\n",
       "      <td>moderate</td>\n",
       "      <td>100.000</td>\n",
       "      <td>80.000</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>20.000</td>\n",
       "      <td>94.000</td>\n",
       "      <td>380.000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>144337</td>\n",
       "      <td>London</td>\n",
       "      <td>200.000</td>\n",
       "      <td>1.500</td>\n",
       "      <td>1.000</td>\n",
       "      <td>Private room</td>\n",
       "      <td>0.000</td>\n",
       "      <td>strict</td>\n",
       "      <td>300.000</td>\n",
       "      <td>200.000</td>\n",
       "      <td>24.0</td>\n",
       "      <td>20.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>84.000</td>\n",
       "      <td>250.000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1372647</td>\n",
       "      <td>London</td>\n",
       "      <td>75.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>Private room</td>\n",
       "      <td>0.000</td>\n",
       "      <td>flexible</td>\n",
       "      <td>0.000</td>\n",
       "      <td>75.000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>380.000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2440394</td>\n",
       "      <td>London</td>\n",
       "      <td>70.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>Private room</td>\n",
       "      <td>1.000</td>\n",
       "      <td>moderate</td>\n",
       "      <td>100.000</td>\n",
       "      <td>70.000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>30.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>380.000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1949687</td>\n",
       "      <td>London</td>\n",
       "      <td>80.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>Entire home/apt</td>\n",
       "      <td>0.000</td>\n",
       "      <td>moderate</td>\n",
       "      <td>100.000</td>\n",
       "      <td>80.000</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>20.000</td>\n",
       "      <td>94.000</td>\n",
       "      <td>380.000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>144337</td>\n",
       "      <td>London</td>\n",
       "      <td>200.000</td>\n",
       "      <td>1.500</td>\n",
       "      <td>1.000</td>\n",
       "      <td>Private room</td>\n",
       "      <td>0.000</td>\n",
       "      <td>strict</td>\n",
       "      <td>300.000</td>\n",
       "      <td>200.000</td>\n",
       "      <td>24.0</td>\n",
       "      <td>20.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>84.000</td>\n",
       "      <td>250.000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1372647</td>\n",
       "      <td>London</td>\n",
       "      <td>75.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>Private room</td>\n",
       "      <td>0.000</td>\n",
       "      <td>flexible</td>\n",
       "      <td>0.000</td>\n",
       "      <td>75.000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>380.000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2440394</td>\n",
       "      <td>London</td>\n",
       "      <td>70.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>Private room</td>\n",
       "      <td>0.000</td>\n",
       "      <td>moderate</td>\n",
       "      <td>100.000</td>\n",
       "      <td>70.000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>30.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>380.000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4754275</td>\n",
       "      <td>Other</td>\n",
       "      <td>110.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>Entire home/apt</td>\n",
       "      <td>0.000</td>\n",
       "      <td>moderate</td>\n",
       "      <td>0.000</td>\n",
       "      <td>110.000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>50.000</td>\n",
       "      <td>87.000</td>\n",
       "      <td>380.000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8154300</td>\n",
       "      <td>Other</td>\n",
       "      <td>91.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>Entire home/apt</td>\n",
       "      <td>0.000</td>\n",
       "      <td>moderate</td>\n",
       "      <td>170.000</td>\n",
       "      <td>45.500</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>760.000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id   state   price  bathrooms  bedrooms        room_type  host_is_superhost cancellation_policy  security_deposit  price_per_bedroom number_of_reviews  extra_people  instant_bookable  cleaning_fee  review_scores_rating  square_feet n_bathrooms_more_than_two\n",
       "0  1949687  London  80.000      1.000     1.000  Entire home/apt              0.000            moderate           100.000             80.000               8.0        10.000             0.000        20.000                94.000      380.000                       0.0\n",
       "1   144337  London 200.000      1.500     1.000     Private room              0.000              strict           300.000            200.000              24.0        20.000             0.000         0.000                84.000      250.000                       0.0\n",
       "2  1372647  London  75.000      1.000     1.000     Private room              0.000            flexible             0.000             75.000               3.0        10.000             0.000         0.000               100.000      380.000                       0.0\n",
       "3  2440394  London  70.000      1.000     1.000     Private room              1.000            moderate           100.000             70.000               3.0         0.000             0.000        30.000               100.000      380.000                       0.0\n",
       "4  1949687  London  80.000      1.000     1.000  Entire home/apt              0.000            moderate           100.000             80.000               8.0        10.000             0.000        20.000                94.000      380.000                       0.0\n",
       "5   144337  London 200.000      1.500     1.000     Private room              0.000              strict           300.000            200.000              24.0        20.000             0.000         0.000                84.000      250.000                       0.0\n",
       "6  1372647  London  75.000      1.000     1.000     Private room              0.000            flexible             0.000             75.000               3.0        10.000             0.000         0.000               100.000      380.000                       0.0\n",
       "7  2440394  London  70.000      1.000     1.000     Private room              0.000            moderate           100.000             70.000               3.0         0.000             0.000        30.000               100.000      380.000                       0.0\n",
       "8  4754275   Other 110.000      1.000     1.000  Entire home/apt              0.000            moderate             0.000            110.000               3.0         0.000             0.000        50.000                87.000      380.000                       0.0\n",
       "9  8154300   Other  91.000      1.000     2.000  Entire home/apt              0.000            moderate           170.000             45.500               2.0         0.000             0.000         0.000               100.000      760.000                       0.0"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasetImputed.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Define continous and categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "continuous_features = [\n",
    "    \"bathrooms\",\n",
    "    \"bedrooms\",\n",
    "    \"security_deposit\",\n",
    "    \"cleaning_fee\",\n",
    "    \"extra_people\",\n",
    "    \"number_of_reviews\",\n",
    "    \"square_feet\",\n",
    "    \"review_scores_rating\",\n",
    "]\n",
    "categorical_features = [\n",
    "    \"state\",\n",
    "    \"room_type\",\n",
    "    \"host_is_superhost\",\n",
    "    \"cancellation_policy\",\n",
    "    \"instant_bookable\",\n",
    "]\n",
    "\n",
    "all_features = continuous_features + categorical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dataset_imputed = datasetImputed.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Split data into train and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "[training_dataset, validation_dataset] = dataset_imputed.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Continuous feature pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "continuous_feature_assembler = VectorAssembler(\n",
    "    inputCols=continuous_features, outputCol=\"unscaled_continuous_features\"\n",
    ")\n",
    "\n",
    "continuous_feature_scaler = StandardScaler(\n",
    "    inputCol=\"unscaled_continuous_features\",\n",
    "    outputCol=\"scaled_continuous_features\",\n",
    "    withStd=True,\n",
    "    withMean=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Categorical features pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import src.models.train_model as tm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%autoreload\n",
    "categorical_feature_indexers = tm.make_string_indexer_list(categorical_features)\n",
    "\n",
    "# ohe_input_cols = tm.get_output_col_names(categorical_feature_indexers)\n",
    "\n",
    "categorical_feature_ohe = tm.make_one_hot_encoder(categorical_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['state_Index',\n",
       " 'room_type_Index',\n",
       " 'host_is_superhost_Index',\n",
       " 'cancellation_policy_Index',\n",
       " 'instant_bookable_Index']"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_feature_ohe.getInputCols()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['state_OHE',\n",
       " 'room_type_OHE',\n",
       " 'host_is_superhost_OHE',\n",
       " 'cancellation_policy_OHE',\n",
       " 'instant_bookable_OHE']"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_feature_ohe.getOutputCols()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Assemble features and feature pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished constructing the pipeline\n"
     ]
    }
   ],
   "source": [
    "estimatorLr = (\n",
    "    [continuous_feature_assembler, continuous_feature_scaler]\n",
    "    + categorical_feature_indexers\n",
    "    + [categorical_feature_ohe]\n",
    ")\n",
    "\n",
    "featurePipeline = Pipeline(stages=estimatorLr)\n",
    "\n",
    "sparkFeaturePipelineModel = featurePipeline.fit(dataset_imputed)\n",
    "\n",
    "print(\"Finished constructing the pipeline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Train a Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complet: Training Linear Regression\n"
     ]
    }
   ],
   "source": [
    "linearRegression = LinearRegression(featuresCol='scaled_continuous_features',\n",
    "                                   labelCol='price',\n",
    "                                   predictionCol='price_prediction',\n",
    "                                   maxIter=10,\n",
    "                                   regParam=0.3,\n",
    "                                   elasticNetParam=0.8)\n",
    "\n",
    "pipeline_lr = [sparkFeaturePipelineModel, linearRegression]\n",
    "\n",
    "sparkPipelineEstimatorLr = Pipeline(stages=pipeline_lr)\n",
    "\n",
    "sparkPipelineLr = sparkPipelineEstimatorLr.fit(dataset_imputed)\n",
    "\n",
    "print('Complet: Training Linear Regression')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Train a Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete: Training Logistic Regression\n"
     ]
    }
   ],
   "source": [
    "logisticRegression = LogisticRegression(featuresCol='scaled_continuous_features',\n",
    "                                       labelCol='n_bathrooms_more_than_two',\n",
    "                                       predictionCol='n_bathrooms_more_than_two_prediction',\n",
    "                                       maxIter=10)\n",
    "\n",
    "pipeline_log_r = [sparkFeaturePipelineModel, logisticRegression]\n",
    "\n",
    "sparkPipelineEstimatorLogr = Pipeline(stages=pipeline_log_r)\n",
    "\n",
    "sparkPipelineLogr = sparkPipelineEstimatorLogr.fit(dataset_imputed)\n",
    "\n",
    "print('Complete: Training Logistic Regression')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Serialize the model ot Bundle.ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fname_lr = uts.create_mleap_fname('pyspark.lr.zip', '../models/')\n",
    "fname_logr = uts.create_mleap_fname('pyspark.logr.zip', '../models/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for mod, fname in [(sparkPipelineLr, fname_lr),\n",
    "                   (sparkPipelineLogr, fname_logr)]:\n",
    "    mod.serializeToBundle(fname, mod.transform(dataset_imputed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[36mlr-pipeline-model\u001b[m\u001b[m/ pyspark.logr.zip   pyspark.lr.zip\r\n"
     ]
    }
   ],
   "source": [
    "ls ../models/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Deserialize from Bundle.ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkPipelineLR_des = PipelineModel.deserializeFromBundle(fname_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.ml.pipeline.PipelineModel"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sparkPipelineLR_des)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
